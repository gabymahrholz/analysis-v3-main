[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis",
    "section": "",
    "text": "Overview\nThis course covers data skills R Markdown, data wrangling, and data visualisation. This book also introduces learners to the most common statistical analyses such as t-test, correlations, regressions, and ANOVAs. The main idea of this book is being reproducible in our data analysis approach.\nHow To Use This Book\nEach chapter in the book has the same structure:\n\n\nIndividual walkthrough: Complete this section at your own pace. Take your time to complete the guided activities. Plan for time pockets in your week to work through this section.\n\nPair coding: During the lab we are reserving time for you to work through activities with a peer. If you get stuck, try to solve the issue. A GTA and your tutor will be available to help you, too. Note: We have added a pair coding activity to every chapter in the book - even for chapters that are not associated with a lab. Feel free to get together with a peer to complete the tasks.\n\nTest your knowledge: The final section in each chapter is a short quiz that assesses your knowledge and skills covered in the chapter.\n\nTo support you to work through the book chapters continuously, we have scheduled each chapter so that it aligns as well as possible with the stats lectures and the lab content. We also made sure that no chapters are scheduled during busy assessment times (such as during the run-up to the research report deadline). You will have chapters in weeks 1-5 and then again weeks 9-11.\nIf you get stuck, seek help during your lab, GTA or PAL support sessions, and/or by posting on the Data Skills and R channel on Teams.\nStatement on use of AI\nChatGPT 4.0 was used to assist in the writing of these materials in the following ways:\n\nTo suggest multiple-choice questions in the “Test your knowledge and challenge yourself” section\nTo proof-read and check for typos\nTo suggest improvements to the text\n\nAny information provided by ChatGPT was verified, for example, where code was used, the syntax and output was checked to ensure it was correct and where theoretical or conceptual information was provided, only that which the author could verify from their pre-existing expertise was included.\nNote & Contact: This book is currently being updated which means that chapters are being published on a rolling basis. We regularly check and update for improvements. If you have any feedback or suggestions, please submit them to our Analysis R Book Improvement form. For people outwith University of Glasgow: You are welcome to share feedback by emailing Gaby Mahrholz.\nAcknowledgement of previous versions: This version of the book was adapted from a previous version written by Phil McAleer, Carolina E. Kuepper-Tetzel, & Helena M. Paterson\nR Version: This book has been written with R version 4.4.1 (2024-06-14 ucrt) (Race for Your Life) and RStudio version 2023.12.1+402 “Ocean Storm”."
  },
  {
    "objectID": "01-basics.html#intended-learning-outcomes",
    "href": "01-basics.html#intended-learning-outcomes",
    "title": "1  Projects and R Markdown",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\nRe-familiarise yourself with setting up projects\nRe-familiarise yourself with RMarkdown documents\nRecap and apply data wrangling procedures to analyse data"
  },
  {
    "objectID": "01-basics.html#individual-walkthrough",
    "href": "01-basics.html#individual-walkthrough",
    "title": "1  Projects and R Markdown",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "01-basics.html#r-and-r-studio",
    "href": "01-basics.html#r-and-r-studio",
    "title": "1  Projects and R Markdown",
    "section": "\n1.1 R and R Studio",
    "text": "1.1 R and R Studio\nRemember, R is a programming language that you will write code in and RStudio is an Integrated Development Environment (IDE) which makes working with R easier as it’s more user friendly. You need both components for this course.\nIf this is not ringing any bells yet, have a quick browse through the materials from year 1 to refresh your memory.\n\n1.1.1 R server\nUse the server only if you are unable to install R and RStudio on your computer (e.g., if you are using a Chromebook) or if you encounter issues while installing R on your own machine. Otherwise, you should install R and RStudio directly on your own computer. R and RStudio are already installed on the R server.\nYou will find the link to the server on Moodle.\n\n1.1.2 Installing R and RStudio on your computer\nThe RSetGo book provides detailed instructions on how to install R and RStudio on your computer. It also includes links to walkthroughs for installing R on different types of computers and operating systems.\nIf you had R and RStudio installed on your computer last year, we recommend updating to the latest versions. In fact, it’s a good practice to update them at the start of each academic year. Detailed guidance can be found in Appendix B.\nOnce you have installed or updated R and RStudio, return to this chapter.\n\n1.1.3 Settings for Reproducibility\nBy now, you should be aware that the Psychology department at the University of Glasgow places a strong emphasis on reproducibility, open science, and raising awareness about questionable research practices (QRPs) and how to avoid them. Therefore, it’s important that you work in a reproducible manner so that others (and your future self) can understand and check your work. This also makes it easier for you to reuse your work in the future.\nAlways start with a clear workspace. If your Global Environment contains anything from a previous session, you can’t be certain whether your current code is working as intended or if it’s using objects created earlier.\nTo ensure a clean and reproducible workflow, there are a few settings you should adjust immediately after installing or updating RStudio. In Tools &gt; Global Options… General tab\n\nUncheck the box labelled Restore .RData into workspace at startup to make sure no data from a previous session is loaded into the environment\nset Save workspace to .RData on exit to Never to prevent your workspace from being saved when you exit RStudio.\n\n\n\nReproducibility settings in Global Options\n\n\n\n\n\n\n\nTip for keeping taps on parentheses\n\n\n\n\n\nR has included rainbow parentheses to help with keeping count on the brackets.\nTo enable the feature, go to Tools &gt; Global Options… Code tab &gt; Display tab and tick the last checkbox “Use rainbow parentheses”\n\n\nEnable Rainbow parenthesis\n\n\n\n\n\n1.1.4 RStudio panes\nRStudio has four main panes each in a quadrant of your screen:\n\nSource pane\nEnvironment pane\nConsole pane\nOutput pane\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAre you ready for a quick quiz to see what you remember about the RStudio panes from last year? Click on Quiz to see the questions.\n\n\n\n\n\n\nQuiz\n\n\n\n\n\nWhat is their purpose?\nThe Source pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Environment pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Console pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Output pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nWhere are these panes located by default?\n\nThe Source pane is located? \nbottom left\nbottom right\ntop right\ntop left\n\nThe Environment pane is located? \ntop left\nbottom left\nbottom right\ntop right\n\nThe Console pane is located? \nbottom right\ntop left\ntop right\nbottom left\n\nThe Output pane is located? \ntop left\nbottom left\ntop right\nbottom right\n\n\n\n\n\n\n\nIf you were not quite sure about one/any of the panes, check out the materials from Level 1. If you want to know more about them, there is the RStudio guide on posit"
  },
  {
    "objectID": "01-basics.html#sec-project",
    "href": "01-basics.html#sec-project",
    "title": "1  Projects and R Markdown",
    "section": "\n1.2 Activity 1: Creating a new project",
    "text": "1.2 Activity 1: Creating a new project\nIt’s important to create a new RStudio project whenever you start a new project. This practice makes it easier to work in multiple contexts, such as when analysing different datasets simultaneously. Each RStudio project has its own folder location, workspace, and working directories, which keeps all your data and RMarkdown documents organised in one place.\nLast year, you learnt how to create projects on the server, so you already know the steps. If cannot quite recall how that was done, go back to the Level 1 materials.\nOn your own computer, open RStudio, and complete the following steps in this order:\n\nClick on File &gt; New Project…\n\nThen, click on “New Directory”\nThen, click on “New Project”\nName the directory something meaningful (e.g., “2A_chapter1”), and save it in a location that makes sense, for example, a dedicated folder you have for your level 2 Psychology labs - you can either select a folder you have already in place or create a new one (e.g., I named my new folder “Level 2 labs”)\nClick “Create Project”. RStudio will restart itself and open with this new project directory as the working directory. If you accidentally close it, you can open it by double-clicking on the project icon in your folder\nYou can also check in your folder structure that everything was created as intended\n\n\n\nCreating a new project\n\n\n\n\n\n\n\nWhy is the Colour scheme in the gif different to my version?\n\n\n\n\n\nIn case anyone is wondering why my colour scheme in the gif above looks different to yours, I’ve set mine to “Pastel On Dark” in Tools &gt; Global Options… &gt; Appearances. And my computer lives in “dark mode”.\n\n\n\n\n\n\n\n\n\nDon’t nest projects\n\n\n\nDon’t ever save a new project inside another project directory. This can cause some hard-to-resolve problems."
  },
  {
    "objectID": "01-basics.html#sec-rmd",
    "href": "01-basics.html#sec-rmd",
    "title": "1  Projects and R Markdown",
    "section": "\n1.3 Activity 2: Create a new R Markdown file",
    "text": "1.3 Activity 2: Create a new R Markdown file\n\nOpen a new R Markdown document: click File &gt; New File &gt; R Markdown or click on the little page icon with a green plus sign (top left).\nGive it a meaningful Title (e.g., Level 2 chapter 1) - you can also change the title later. Feel free to add your name or GUID in the Author field author name. Keep the Default Output Format as HTML.\nOnce the .Rmd opened, you need to save the file.\nTo save it, click File &gt; Save As… or click on the little disc icon. Name it something meaningful (e.g., “chapter_01.Rmd”, “01_intro.Rmd”). Make sure there are no spaces in the name - R is not very fond of spaces… This file will automatically be saved in your project folder (i.e., your working directory) so you should now see this file appear in your file viewer pane.\n\n\n\nCreating a new .Rmd file\n\nRemember, an R Markdown document or .Rmd has “white space” (i.e., the markdown for formatted text) and “grey parts” (i.e., code chunks) in the default colour scheme (see Figure 1.1). R Markdown is a powerful tool for creating dynamic documents because it allows you to integrate code and regular text seamlessly. You can then knit your .Rmd using the knitr package to create a final document as either a webpage (HTML), a PDF, or a Word document (.docx). We’ll only knit to HTML documents in this course.\n\n\nR markdown anatomy (image from https://intro2r.com/r-markdown-anatomy.html)\n\n\n1.3.1 Markdown\nThe markdown space in an .Rmd is ideal for writing notes that explain your code and document your thought process. Use this space to clarify what your code is doing, why certain decisions were made, and any insights or conclusions you have drawn along the way. These notes are invaluable when revisiting your work later, helping you (or others) understand the rationale behind key decisions, such as setting inclusion/exclusion criteria or interpreting the results of assumption tests. Effectively documenting your work in the markdown space enhances both the clarity and reproducibility of your analysis.\nThe markdown space offers a variety of formatting options to help you organise and present your notes effectively. Here are a few of them that can enhance your documentation:\nHeading levels\nThere is a variety of heading levels to make use of, using the # symbol.\n\n\nYou would incorporate this into your text as:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n#### Heading level 4\n##### Heading level 5\n###### Heading level 6\n\n\nAnd it will be displayed in your knitted html file as:\n\n\n\n\n\n\n\n\n\n\nERROR: My heading levels don’t render properly when knitting\n\n\n\n\n\nYou need a space between the # and the first letter. If the space is missing, the heading will be displayed in the HTML file as …\n#Heading 1\n\n\n\nUnordered and ordered lists\nYou can also include unordered lists and ordered lists. Click on the tabs below to see how they are incorporated\n\n\nunordered lists\nordered lists\nordered lists magic\n\n\n\nYou can add bullet points using either *, - or + and they will turn into:\n\nbullet point (created with *)\nbullet point (created with -)\nbullet point (created with +)\n\nor use bullet points of different levels using 1 tab key press or 2 spaces (for sub-item 1) or 2 tabs/4 spaces (for sub-sub-item 1):\n\nbullet point item 1\n\nsub-item 1\n\nsub-sub-item 1\nsub-sub-item 2\n\n\n\n\nbullet point item 2\n\n\n\n\n\n\n\nERROR: My bullet points don’t render properly when knitting\n\n\n\n\n\nYou need an empty row before your bullet points start. If I delete the empty row before the bullet points, they will be displayed in the HTML as …\nText without the empty row: * bullet point created with * - bullet point created with - + bullet point created with +\n\n\n\n\n\nStart the line with 1., 2., etc. When you want to include sub-items, either use the tab key twice or add 4 spaces. Same goes for the sub-sub-item: include either 2 tabs (or 4 manual spaces) from the last item or 4 tabs/ 8 spaces from the start of the line.\n\nlist item 1\nlist item 2\n\nsub-item 1 (with 4 spaces) A. sub-sub-item 1 (with an additional 4 spaces from the last indent)\n\n\n\n\n\n\n\n\n\nMy list items don’t render properly when knitting\n\n\n\n\n\nIf you don’t leave enough spaces, the list won’t be recognised, and your output looks like this:\n\nlist item 3\n\n\nsub-item 1 (with only 2 spaces) A. sub-sub-item 1 (with an additional 2 spaces from the last indent)\n\n\n\n\n\n\nThe great thing though is that you don’t need to know your alphabet or number sequences. R markdown will fix that for you\nIf I type into my .Rmd…\n\n…it will be rendered in the knitted HTML output as…\n\nlist item 3\nlist item 1\n\nsub-item labelled “a)”\nsub-item labelled “i)”\n\nsub-item labelled “C)”\nsub-item labelled “Z)”\n\n\n\n\nlist item 7\n\n\n\n\n\n\n\nERROR: The labels of the sub-items are not what I thought they would be. You said they are fixing themselves…\n\n\n\n\n\nYes, they do but you need to label your sub-item lists accordingly. The first label you list in each level is set as the baseline. If they are labelled 1) instead of i) or A., the output will show as follows, but the automatic-item-fixing still works:\n\nlist item 7\n\nlist item “1)” with 4 spaces\n\nlist item “1)” with 8 spaces\nthis is an item labelled “6)” (magically corrected to “2.”)\n\n\n\n\n\n\n\n\n\n\n\nEmphasis\nInclude emphasis to draw attention to keywords in your text:\n\n\nR markdown syntax\nDisplayed in the knitted HTML file\n\n\n\n**bold text**\nbold text\n\n\n*italic text*\nitalic text\n\n\n***bold and italic***\nbold and italic\n\n\n\nOther examples can be found in the R Markdown Cheat Sheet\n\n1.3.2 Code chunks\nEverything you write inside the code chunks will be interpreted as code and executed by R. Code chunks start with ``` followed by an {r} which specifies the coding language R, some space for code, and ends with ```. If you accidentally delete one of those backticks, your code won’t run and/or your text parts will be interpreted as part of the code chunks or vice versa. This should be evident from the colour change - more white than expected typically indicates missing starting backticks, whilst too much grey/not enough white suggests missing ending backticks. But no need to fret if that happens - just add the missing backticks manually.\nYou can insert a new code chunk in several ways:\n\nClick the Insert a new code chunk button in the RStudio Toolbar (green icon at the top right corner of the Source pane).\nSelect Code &gt; Insert Chunk from the menu.\nUsing the shortcut Ctrl + Alt + I for Windows or Cmd + Option + I on MacOSX.\nType ```{r} and ``` manually\n\n\n\n\n\nFigure 1.1: Default .Rmd with highlighting - names in pink and knitr display options in purple\n\n\n\nWithin the curly brackets of a code chunk, you can specify a name for the code chunk (see pink highlighting in Figure 1.1). The chunk name is not necessarily required; however, it is good practice to give each chunk a unique name to support more advanced knitting approaches. It also makes it easier to reference and manage chunks.\nWithin the curly brackets, you can also place rules and arguments (see purple highlighting in Figure 1.1) to control how your code is executed and what is displayed in your final HTML output. The most common knitr display options include:\n\n\n\n\n\n\n\n\nCode\nDoes code run\nDoes code show\nDo results show\n\n\n\neval=FALSE\nNO\nYES\nNO\n\n\necho=TRUE (default)\nYES\nYES\nYES\n\n\necho=FALSE\nYES\nNO\nYES\n\n\nresults=‘hide’\nYES\nYES\nNO\n\n\ninclude=FALSE\nYES\nNO\nNO\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe table above will be incredibly important for the data skills homework II. When solving error mode items you will need to pay attention to the first one eval = FALSE.\n\n\nOne last thing: In your newly created .Rmd file, delete everything below line 12 (keep the set-up code chunk) and save your .Rmd by clicking on the disc symbol.\n\n\nDelete everything below line 12\n\n\n\n\n\n\n\nYour Turn\n\n\n\nThat was quite a long section about what Markdown can do. I promise, we’ll practice that more later. For the minute, we want you to create a new level 2 heading on line 12 and give it a meaningful heading title (something like “Loading packages and reading in data” or “Chapter 1”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn line 12, you should have typed ## Loading packages and reading in data (or whatever meaningful title you chose). This will create level 2 heading once we knit the .Rmd."
  },
  {
    "objectID": "01-basics.html#sec-download_data_ch1",
    "href": "01-basics.html#sec-download_data_ch1",
    "title": "1  Projects and R Markdown",
    "section": "\n1.4 Activity 3: Download the data",
    "text": "1.4 Activity 3: Download the data\nThe data for chapters 1-3. Download it here: data_ch1.zip. There are 2 csv files contained in a zip folder. One is the data file we are going to use today prp_data_reduced.csv and the other is an Excel file prp_codebook that explains the variables in the data.\nThe first step is to unzip the zip folder so that the files are placed within the same folder as your project.\n\nPlace the zip folder within your 2A_chapter1 folder\nRight mouse click –&gt; Extract All...\n\nCheck the folder location is the one to extract the files to\nCheck the extracted files are placed next to the project icon\nFiles and project should be visible in the Output pane in RStudio\n\n\n\n\n\n\n\nFor screenshots click here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnzipping a zip folder\n\n\n\n\n\n\nThe paper by Pownall et al. was a registered report published in 2023, and the original data can be found on OSF (https://osf.io/5qshg/).\nCitation\n\nPownall, M., Pennington, C. R., Norris, E., Juanchich, M., Smailes, D., Russell, S., Gooch, D., Evans, T. R., Persson, S., Mak, M. H. C., Tzavella, L., Monk, R., Gough, T., Benwell, C. S. Y., Elsherif, M., Farran, E., Gallagher-Mitchell, T., Kendrick, L. T., Bahnmueller, J., . . . Clark, K. (2023). Evaluating the Pedagogical Effectiveness of Study Preregistration in the Undergraduate Dissertation. Advances in Methods and Practices in Psychological Science, 6(4). https://doi.org/10.1177/25152459231202724\n\nAbstract\n\nResearch shows that questionable research practices (QRPs) are present in undergraduate final-year dissertation projects. One entry-level Open Science practice proposed to mitigate QRPs is “study preregistration,” through which researchers outline their research questions, design, method, and analysis plans before data collection and/or analysis. In this study, we aimed to empirically test the effectiveness of preregistration as a pedagogic tool in undergraduate dissertations using a quasi-experimental design. A total of 89 UK psychology students were recruited, including students who preregistered their empirical quantitative dissertation (n = 52; experimental group) and students who did not (n = 37; control group). Attitudes toward statistics, acceptance of QRPs, and perceived understanding of Open Science were measured both before and after dissertation completion. Exploratory measures included capability, opportunity, and motivation to engage with preregistration, measured at Time 1 only. This study was conducted as a Registered Report; Stage 1 protocol: https://osf.io/9hjbw (date of in-principle acceptance: September 21, 2021). Study preregistration did not significantly affect attitudes toward statistics or acceptance of QRPs. However, students who preregistered reported greater perceived understanding of Open Science concepts from Time 1 to Time 2 compared with students who did not preregister. Exploratory analyses indicated that students who preregistered reported significantly greater capability, opportunity, and motivation to preregister. Qualitative responses revealed that preregistration was perceived to improve clarity and organization of the dissertation, prevent QRPs, and promote rigor. Disadvantages and barriers included time, perceived rigidity, and need for training. These results contribute to discussions surrounding embedding Open Science principles into research training.\n\nChanges made to the dataset\nWe made some changes to the dataset for the purpose of increasing difficulty for data wrangling (Chapter 2 and Chapter 3) and data visualisation (Chapter 4 and Chapter 5). This will ensure some “teachable moments”. The changes are as follows:\n\nWe removed some of the variables to make the data more manageable for teaching purposes.\nWe recoded some values from numeric responses to labels (e.g., understanding).\nWe added the word “years” to one of the Age entries.\nWe tidied a messy column Ethnicity but introduced a similar but easier-to-solve “messiness pattern” when recoding the understanding data.\nThe scores in the original file were already corrected from reverse-coded responses. We reversed that process to present raw data here."
  },
  {
    "objectID": "01-basics.html#activity-4-loading-packages-and-reading-in-data",
    "href": "01-basics.html#activity-4-loading-packages-and-reading-in-data",
    "title": "1  Projects and R Markdown",
    "section": "\n1.5 Activity 4: Loading packages and reading in data",
    "text": "1.5 Activity 4: Loading packages and reading in data\nThe first step is to load in the packages we need and read in the data. Today, we’ll only be using tidyverse, and read_csv() will help us store the data from prp_data_reduced.csv in an object called data_prp.\nCopy the code into a code chunk in your .Rmd file and run it. You can either click the green error to run the entire code chunk, or use the shortcut Ctrl + Enter (Windows) or Cmd + Enter (Mac) to run a line of code/ pipe from the Rmd.\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 89 Columns: 91\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): Code, Age, Ethnicity, Opptional_mod_1_TEXT, Research_exp_1_TEXT, U...\ndbl (74): Gender, Secondyeargrade, Opptional_mod, Research_exp, Plan_prereg,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "01-basics.html#sec-familiarise",
    "href": "01-basics.html#sec-familiarise",
    "title": "1  Projects and R Markdown",
    "section": "\n1.6 Activity 5: Familiarise yourself with the data",
    "text": "1.6 Activity 5: Familiarise yourself with the data\n\nLook at the Codebook to get a feel of the variables in the dataset and how they have been measured. Note that some of the columns were deleted in the dataset you have been given.\nYou’ll notice that some questionnaire data was collected at 2 different time points (i.e., SATS28, QRPs, Understanding_OS)\nsome of the data was only collected at one time point (i.e., supervisor judgements, OS_behav items, and Included_prereg variables are t2-only variables)\n\n\n1.6.1 First glimpse at the data\nBefore you start wrangling your data, it is important to understand what kind of data you’re working with and what the format of your dataframe looks like.\nAs you may have noticed, read_csv() provides a message listing the data types in your dataset and how many columns are of each type. Plus, it shows a few examples columns for each data type.\nTo obtain more detailed information about your data, you have several options. Click on the individual tabs to see the different options available. Test them out in your own .Rmd file and use whichever method you prefer (but do it).\n\n\n\n\n\n\nWarning\n\n\n\nSome of the output is a bit long because we do have quite a few variables in the data file.\n\n\n\n\nvisual inspection 1\nglimpse()\nspec()\nvisual inspection 2\n\n\n\nIn the Global Environment, click the blue arrow icon next to the object name data_prp. This action will expand the object, revealing details about its columns. The $ symbol is commonly used in Base R to access a specific column within your dataframe.\n\n\nVisual inspection of the data\n\nCon: When you have quite a few variables, not all of them are shown.\n\n\nUse glimpse() if you want a more detailed overview you can see on your screen. The output will display rows and column numbers, and some examples of the first couple of observations for each variable.\n\nglimpse(data_prp)\n\nRows: 89\nColumns: 91\n$ Code                                  &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", …\n$ Gender                                &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,…\n$ Age                                   &lt;chr&gt; \"22\", \"20\", \"22\", \"26\", \"22\", \"2…\n$ Ethnicity                             &lt;chr&gt; \"White European\", \"White British…\n$ Secondyeargrade                       &lt;dbl&gt; 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1,…\n$ Opptional_mod                         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ Opptional_mod_1_TEXT                  &lt;chr&gt; \"Research methods in first year\"…\n$ Research_exp                          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Research_exp_1_TEXT                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Plan_prereg                           &lt;dbl&gt; 1, 3, 1, 2, 1, 1, 3, 3, 2, 2, 2,…\n$ SATS28_1_Affect_Time1                 &lt;dbl&gt; 4, 5, 5, 6, 2, 1, 6, 3, 2, 5, 2,…\n$ SATS28_2_Affect_Time1                 &lt;dbl&gt; 5, 6, 3, 3, 6, 1, 2, 2, 7, 3, 4,…\n$ SATS28_3_Affect_Time1                 &lt;dbl&gt; 3, 2, 5, 2, 6, 7, 2, 6, 6, 5, 2,…\n$ SATS28_4_Affect_Time1                 &lt;dbl&gt; 4, 5, 2, 2, 6, 6, 5, 5, 5, 5, 2,…\n$ SATS28_5_Affect_Time1                 &lt;dbl&gt; 5, 5, 5, 6, 1, 1, 5, 1, 2, 5, 2,…\n$ SATS28_6_Affect_Time1                 &lt;dbl&gt; 5, 6, 2, 5, 6, 7, 4, 5, 5, 3, 5,…\n$ SATS28_7_CognitiveCompetence_Time1    &lt;dbl&gt; 4, 2, 2, 5, 6, 7, 2, 5, 5, 2, 2,…\n$ SATS28_8_CognitiveCompetence_Time1    &lt;dbl&gt; 2, 2, 2, 1, 6, 7, 2, 5, 3, 2, 3,…\n$ SATS28_9_CognitiveCompetence_Time1    &lt;dbl&gt; 2, 2, 2, 3, 3, 7, 2, 6, 3, 3, 1,…\n$ SATS28_10_CognitiveCompetence_Time1   &lt;dbl&gt; 6, 7, 6, 6, 4, 2, 6, 4, 5, 6, 5,…\n$ SATS28_11_CognitiveCompetence_Time1   &lt;dbl&gt; 4, 3, 5, 5, 3, 1, 6, 2, 5, 6, 5,…\n$ SATS28_12_CognitiveCompetence_Time1   &lt;dbl&gt; 3, 5, 3, 5, 5, 7, 3, 4, 7, 2, 3,…\n$ SATS28_13_Value_Time1                 &lt;dbl&gt; 1, 1, 2, 1, 3, 7, 1, 2, 1, 2, 4,…\n$ SATS28_14_Value_Time1                 &lt;dbl&gt; 7, 7, 6, 6, 5, 1, 6, 5, 7, 6, 2,…\n$ SATS28_15_Value_Time1                 &lt;dbl&gt; 7, 7, 6, 6, 3, 5, 6, 6, 6, 5, 5,…\n$ SATS28_16_Value_Time1                 &lt;dbl&gt; 2, 1, 3, 2, 6, 5, 3, 7, 2, 2, 2,…\n$ SATS28_17_Value_Time1                 &lt;dbl&gt; 1, 1, 3, 3, 7, 7, 2, 7, 2, 2, 5,…\n$ SATS28_18_Value_Time1                 &lt;dbl&gt; 3, 6, 5, 3, 1, 1, 5, 1, 5, 2, 2,…\n$ SATS28_19_Value_Time1                 &lt;dbl&gt; 3, 3, 3, 3, 7, 7, 4, 5, 3, 5, 6,…\n$ SATS28_20_Value_Time1                 &lt;dbl&gt; 2, 1, 4, 2, 7, 7, 2, 4, 2, 2, 7,…\n$ SATS28_21_Value_Time1                 &lt;dbl&gt; 2, 1, 3, 2, 6, 7, 2, 5, 1, 3, 5,…\n$ SATS28_22_Difficulty_Time1            &lt;dbl&gt; 3, 2, 5, 3, 2, 1, 4, 2, 2, 5, 3,…\n$ SATS28_23_Difficulty_Time1            &lt;dbl&gt; 5, 6, 5, 6, 6, 7, 4, 6, 7, 5, 6,…\n$ SATS28_24_Difficulty_Time1            &lt;dbl&gt; 2, 2, 2, 3, 1, 4, 4, 2, 2, 2, 2,…\n$ SATS28_25_Difficulty_Time1            &lt;dbl&gt; 6, 7, 5, 5, 6, 7, 5, 6, 5, 5, 5,…\n$ SATS28_26_Difficulty_Time1            &lt;dbl&gt; 4, 2, 2, 2, 6, 7, 4, 5, 3, 5, 3,…\n$ SATS28_27_Difficulty_Time1            &lt;dbl&gt; 4, 5, 5, 3, 6, 7, 4, 3, 5, 3, 6,…\n$ SATS28_28_Difficulty_Time1            &lt;dbl&gt; 1, 7, 5, 5, 6, 6, 5, 4, 4, 4, 2,…\n$ QRPs_1_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 6, 2, 7, 6, 7,…\n$ QRPs_2_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 5,…\n$ QRPs_3_Time1                          &lt;dbl&gt; 5, 2, 6, 2, 6, 4, 6, 3, 7, 3, 3,…\n$ QRPs_4_Time1                          &lt;dbl&gt; 7, 7, 6, 6, 7, 4, 6, 7, 7, 7, 6,…\n$ QRPs_5_Time1                          &lt;dbl&gt; 3, 3, 7, 7, 2, 7, 4, 6, 7, 3, 2,…\n$ QRPs_6_Time1                          &lt;dbl&gt; 4, 7, 6, 5, 7, 4, 4, 5, 7, 6, 5,…\n$ QRPs_7_Time1                          &lt;dbl&gt; 5, 7, 7, 7, 7, 4, 5, 6, 7, 7, 5,…\n$ QRPs_8_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7,…\n$ QRPs_9_Time1                          &lt;dbl&gt; 6, 7, 7, 4, 7, 7, 3, 7, 6, 6, 2,…\n$ QRPs_10_Time1                         &lt;dbl&gt; 7, 6, 5, 2, 5, 4, 2, 6, 7, 7, 2,…\n$ QRPs_11_Time1                         &lt;dbl&gt; 7, 7, 7, 4, 7, 7, 4, 6, 7, 7, 5,…\n$ QRPs_12NotQRP_Time1                   &lt;dbl&gt; 2, 2, 1, 4, 1, 4, 2, 4, 2, 2, 1,…\n$ QRPs_13NotQRP_Time1                   &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 2, 4, 1, 1, 1,…\n$ QRPs_14NotQRP_Time1                   &lt;dbl&gt; 1, 4, 3, 4, 1, 4, 2, 3, 3, 4, 3,…\n$ QRPs_15NotQRP_Time1                   &lt;dbl&gt; 2, 4, 2, 2, 1, 4, 2, 1, 4, 4, 2,…\n$ Understanding_OS_1_Time1              &lt;chr&gt; \"2\", \"2\", \"6\", \"2\", \"6\", \"Not at…\n$ Understanding_OS_2_Time1              &lt;chr&gt; \"2\", \"Not at all confident\", \"2\"…\n$ Understanding_OS_3_Time1              &lt;chr&gt; \"2\", \"Not at all confident\", \"3\"…\n$ Understanding_OS_4_Time1              &lt;chr&gt; \"6\", \"Not at all confident\", \"6\"…\n$ Understanding_OS_5_Time1              &lt;chr&gt; \"Entirely confident\", \"6\", \"6\", …\n$ Understanding_OS_6_Time1              &lt;chr&gt; \"Entirely confident\", \"Entirely …\n$ Understanding_OS_7_Time1              &lt;chr&gt; \"6\", \"Not at all confident\", \"2\"…\n$ Understanding_OS_8_Time1              &lt;chr&gt; \"6\", \"3\", \"5\", \"3\", \"5\", \"Not at…\n$ Understanding_OS_9_Time1              &lt;chr&gt; \"Entirely confident\", \"6\", \"5\", …\n$ Understanding_OS_10_Time1             &lt;chr&gt; \"Entirely confident\", \"6\", \"5\", …\n$ Understanding_OS_11_Time1             &lt;chr&gt; \"Entirely confident\", \"2\", \"4\", …\n$ Understanding_OS_12_Time1             &lt;chr&gt; \"Entirely confident\", \"2\", \"5\", …\n$ Pre_reg_group                         &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,…\n$ Other_OS_behav_2                      &lt;dbl&gt; 1, NA, NA, NA, 1, NA, NA, 1, NA,…\n$ Other_OS_behav_4                      &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, N…\n$ Other_OS_behav_5                      &lt;dbl&gt; NA, NA, NA, NA, 1, 1, NA, NA, NA…\n$ Closely_follow                        &lt;dbl&gt; 2, 2, 2, NA, 3, 3, 3, NA, NA, 2,…\n$ SATS28_Affect_Time2_mean              &lt;dbl&gt; 3.500000, 3.166667, 4.833333, 4.…\n$ SATS28_CognitiveCompetence_Time2_mean &lt;dbl&gt; 4.166667, 4.666667, 6.166667, 5.…\n$ SATS28_Value_Time2_mean               &lt;dbl&gt; 3.000000, 6.222222, 6.000000, 4.…\n$ SATS28_Difficulty_Time2_mean          &lt;dbl&gt; 2.857143, 2.857143, 4.000000, 2.…\n$ QRPs_Acceptance_Time2_mean            &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.…\n$ Time2_Understanding_OS                &lt;dbl&gt; 5.583333, 3.333333, 5.416667, 4.…\n$ Supervisor_1                          &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 6, 7, 5, 6,…\n$ Supervisor_2                          &lt;dbl&gt; 5, 6, 7, 4, 6, 2, 7, 5, 6, 5, 5,…\n$ Supervisor_3                          &lt;dbl&gt; 6, 7, 7, 1, 7, 1, 7, 5, 6, 6, 7,…\n$ Supervisor_4                          &lt;dbl&gt; 6, 7, 7, 1, 7, 1, 7, 6, 7, 6, 6,…\n$ Supervisor_5                          &lt;dbl&gt; 5, 7, 7, 4, 7, 3, 7, 7, 6, 6, 6,…\n$ Supervisor_6                          &lt;dbl&gt; 5, 7, 7, 4, 6, 3, 7, 6, 7, 6, 6,…\n$ Supervisor_7                          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Supervisor_8                          &lt;dbl&gt; 5, 5, 7, 1, 7, 1, 7, 5, 7, 5, 6,…\n$ Supervisor_9                          &lt;dbl&gt; 6, 7, 7, 4, 7, 3, 7, 5, 7, 6, 7,…\n$ Supervisor_10                         &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 6, 7, 6, 6,…\n$ Supervisor_11                         &lt;dbl&gt; NA, 7, 7, NA, 7, 1, 7, 5, 7, 6, …\n$ Supervisor_12                         &lt;dbl&gt; 4, 5, 7, 1, 4, 1, 7, 3, 6, 6, 5,…\n$ Supervisor_13                         &lt;dbl&gt; 4, 2, 5, 1, 2, 1, 6, 3, 5, 6, 5,…\n$ Supervisor_14                         &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 5, 7, 6, 6,…\n$ Supervisor_15_R                       &lt;dbl&gt; 1, 1, 1, 4, 1, 7, 1, 2, 1, 2, 1,…\n\n\n\n\nYou can also use spec() as suggested in the message above and then it shows you a list of the data type in every single column. But it doesn’t show you the number of rows and columns.\n\nspec(data_prp)\n\ncols(\n  Code = col_character(),\n  Gender = col_double(),\n  Age = col_character(),\n  Ethnicity = col_character(),\n  Secondyeargrade = col_double(),\n  Opptional_mod = col_double(),\n  Opptional_mod_1_TEXT = col_character(),\n  Research_exp = col_double(),\n  Research_exp_1_TEXT = col_character(),\n  Plan_prereg = col_double(),\n  SATS28_1_Affect_Time1 = col_double(),\n  SATS28_2_Affect_Time1 = col_double(),\n  SATS28_3_Affect_Time1 = col_double(),\n  SATS28_4_Affect_Time1 = col_double(),\n  SATS28_5_Affect_Time1 = col_double(),\n  SATS28_6_Affect_Time1 = col_double(),\n  SATS28_7_CognitiveCompetence_Time1 = col_double(),\n  SATS28_8_CognitiveCompetence_Time1 = col_double(),\n  SATS28_9_CognitiveCompetence_Time1 = col_double(),\n  SATS28_10_CognitiveCompetence_Time1 = col_double(),\n  SATS28_11_CognitiveCompetence_Time1 = col_double(),\n  SATS28_12_CognitiveCompetence_Time1 = col_double(),\n  SATS28_13_Value_Time1 = col_double(),\n  SATS28_14_Value_Time1 = col_double(),\n  SATS28_15_Value_Time1 = col_double(),\n  SATS28_16_Value_Time1 = col_double(),\n  SATS28_17_Value_Time1 = col_double(),\n  SATS28_18_Value_Time1 = col_double(),\n  SATS28_19_Value_Time1 = col_double(),\n  SATS28_20_Value_Time1 = col_double(),\n  SATS28_21_Value_Time1 = col_double(),\n  SATS28_22_Difficulty_Time1 = col_double(),\n  SATS28_23_Difficulty_Time1 = col_double(),\n  SATS28_24_Difficulty_Time1 = col_double(),\n  SATS28_25_Difficulty_Time1 = col_double(),\n  SATS28_26_Difficulty_Time1 = col_double(),\n  SATS28_27_Difficulty_Time1 = col_double(),\n  SATS28_28_Difficulty_Time1 = col_double(),\n  QRPs_1_Time1 = col_double(),\n  QRPs_2_Time1 = col_double(),\n  QRPs_3_Time1 = col_double(),\n  QRPs_4_Time1 = col_double(),\n  QRPs_5_Time1 = col_double(),\n  QRPs_6_Time1 = col_double(),\n  QRPs_7_Time1 = col_double(),\n  QRPs_8_Time1 = col_double(),\n  QRPs_9_Time1 = col_double(),\n  QRPs_10_Time1 = col_double(),\n  QRPs_11_Time1 = col_double(),\n  QRPs_12NotQRP_Time1 = col_double(),\n  QRPs_13NotQRP_Time1 = col_double(),\n  QRPs_14NotQRP_Time1 = col_double(),\n  QRPs_15NotQRP_Time1 = col_double(),\n  Understanding_OS_1_Time1 = col_character(),\n  Understanding_OS_2_Time1 = col_character(),\n  Understanding_OS_3_Time1 = col_character(),\n  Understanding_OS_4_Time1 = col_character(),\n  Understanding_OS_5_Time1 = col_character(),\n  Understanding_OS_6_Time1 = col_character(),\n  Understanding_OS_7_Time1 = col_character(),\n  Understanding_OS_8_Time1 = col_character(),\n  Understanding_OS_9_Time1 = col_character(),\n  Understanding_OS_10_Time1 = col_character(),\n  Understanding_OS_11_Time1 = col_character(),\n  Understanding_OS_12_Time1 = col_character(),\n  Pre_reg_group = col_double(),\n  Other_OS_behav_2 = col_double(),\n  Other_OS_behav_4 = col_double(),\n  Other_OS_behav_5 = col_double(),\n  Closely_follow = col_double(),\n  SATS28_Affect_Time2_mean = col_double(),\n  SATS28_CognitiveCompetence_Time2_mean = col_double(),\n  SATS28_Value_Time2_mean = col_double(),\n  SATS28_Difficulty_Time2_mean = col_double(),\n  QRPs_Acceptance_Time2_mean = col_double(),\n  Time2_Understanding_OS = col_double(),\n  Supervisor_1 = col_double(),\n  Supervisor_2 = col_double(),\n  Supervisor_3 = col_double(),\n  Supervisor_4 = col_double(),\n  Supervisor_5 = col_double(),\n  Supervisor_6 = col_double(),\n  Supervisor_7 = col_double(),\n  Supervisor_8 = col_double(),\n  Supervisor_9 = col_double(),\n  Supervisor_10 = col_double(),\n  Supervisor_11 = col_double(),\n  Supervisor_12 = col_double(),\n  Supervisor_13 = col_double(),\n  Supervisor_14 = col_double(),\n  Supervisor_15_R = col_double()\n)\n\n\n\n\nIn the Global Environment, click on the object name data_prp. This action will open the data in a new tab. Hovering over the column headings with your mouse will also reveal their data type. However, it seems to be a fairly tedious process when you have loads of columns.\n\n\n\n\n\n\nHang on, where is the rest of my data? Why do I only see 50 columns?\n\n\n\n\n\nOne common source of confusion is not seeing all your columns when you open up a data object as a tab. This is because RStudio shows you a maximum of 50 columns at a time. If you have more than 50 columns, navigate with the arrows to see the remaining columns.\n\n\nShowing 50 columns at a time\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nNow that you have tested out all the options in your own .Rmd file, you can probably answer the following questions:\n\nHow many observations? \n\nHow many variables? \n\nHow many columns are col_character or chr data type? \n\nHow many columns are col_double or dbl data type? \n\n\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe visual inspections shows you the number of observations and variables. glimpse() also gives you that information but calls them rows and columns respectively.\nThe data type information actually comes from the output when using the read_csv() function. Did you notice the information on Column specification (see screenshot below)?\n\n\nmessage from read_csv() when reading in the data\n\nWhilst spec() is quite useful for data type information per individual column, it doesn’t give you the total count of each data type. So it doesn’t really help with answering the questions here - unless you want to count manually from its extremely long output.\n\n\n\nIn your .Rmd, include a new heading level 2 called “Information about the data” (or something equally meaningful) and jot down some notes about data_prp. You could include the citation and/or the abstract, and whatever information you think you should note about this dataset (e.g., any observations from looking at the codebook?). You could also include some notes on the functions used so far and what they do. Try to incorporate some bold, italic or bold and italic emphasis and perhaps a bullet point or two.\n\n\n\n\n\n\nPossible solution\n\n\n\n\n\n## Information about the data\nThe data is from Pownall et al. (2023), and I can find the paper here: https://doi.org/10.1177/25152459231202724.\nI’ve noticed in the prp codebook that the SATS-28 questionnaire has quite a few *reverse-coded items*, and the supervisor support questionnaire also has a reverse-coded item.\nSo far, I think I prefer **glimpse()** to show me some more detail about the data. Specs() is too text-heavy for me which makes it hard to read.\nThings to keep in mind:\n\n**don’t forget to load in tidyverse first!!!**\nalways read in the data with **read_csv**, ***never ever use read.csv***!!!\n\n\n\nThe output rendered in a knitted html file\n\n\n\n\n\n\n\n1.6.2 Data types\nEach variable has a data type, such as numeric (numbers), character (text), and logical (TRUE/FALSE values), or a special class of factor. As you have just seen, our data_prp only has character and numeric columns (so far).\nNumeric data can be double (dbl) or integer (int). Doubles can have decimal places (e.g., 1.1). Integers are the whole numbers (e.g., 1, 2, -1) and are displayed with the suffix L (e.g., 1L). This is not overly important but might leave you less puzzled the next time you see an L after a number.\nCharacters (also called “strings”) is anything written between quotation marks. This is usually text, but in special circumstances, a number can be a character if it placed within quotation marks. This can happen when you are recoding variables. It might not be too obvious at the time, but you won’t be able to calculate anything if the number is a character\n\n\nExample data types\nnumeric computation\ncharacter computation\n\n\n\n\ntypeof(1)\ntypeof(1L)\ntypeof(\"1\")\ntypeof(\"text\")\n\n[1] \"double\"\n[1] \"integer\"\n[1] \"character\"\n[1] \"character\"\n\n\n\n\nNo problems here…\n\n1+1\n\n[1] 2\n\n\n\n\nWhen the data type is incorrect, you won’t be able to compute anything, despite your numbers being shown as numeric values in the dataframe. The error message tells you exactly what’s wrong with it, i.e., that you have non-numeric arguments.\n\n\"1\"+\"1\" # ERROR\n\nError in \"1\" + \"1\": non-numeric argument to binary operator\n\n\n\n\n\nLogical data (also sometimes called “Boolean” values) are one of two values: TRUE or FALSE (written in uppercase). They become really important when we use filter() or mutate() with conditional statements such as case_when(). More about those in Chapter 3.\nSome commonly used logical operators:\n\n\noperator\ndescription\n\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n%in%\nTRUE if any element is in the following vector\n\n\n\nA factor is a specific type of integer or character that lets you assign the order of the categories. This becomes useful when you want to display certain categories in “the correct order” either in a dataframe (see arrange) or when plotting (see Chapter 4/ Chapter 5).\n\n1.6.3 Variable types\nYou’ve already encountered them in Level 1 but let’s refresh. Variables can be classified as continuous (numbers) or categorical (labels).\nCategorical variables are properties you can count. They can be nominal, where the categories don’t have an order (e.g., gender) or ordinal (e.g., Likert scales either with numeric values 1-7 or with character labels such as “agree”, “neither agree nor disagree”, “disagree”). Categorical data may also be factors rather than characters.\nContinuous variables are properties you can measure and calculate sums/ means/ etc. They may be rounded to the nearest whole number, but it should make sense to have a value between them. Continuous variables always have a numeric data type (i.e. integer or double).\n\n\n\n\n\n\nWhy is this important you may ask?\n\n\n\nKnowing your variable and data types will help later on when deciding on an appropriate plot (see Chapter 4 and Chapter 5) or which inferential test to run (Chapter 6 to Chapter 13).\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAs we’ve seen earlier, data_prp only had character and numeric variables which hardly tests your understanding to see if you can identify a variety of data types and variable types. So, for this little quiz, we’ve spiced it up a bit. We’ve selected a few columns, shortened some of the column names, and modified some of the data types. Here you can see the first few rows of the new object data_quiz. You can find the code with explanations at the end of this section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nAge\nGender\nEthnicity\nSecondyeargrade\nQRP_item\nQRPs_mean\nUnderstanding_item\nQRP_item &gt; 4\n\n\n\nTr10\n22\n2\nWhite European\n60-69% (2:1 grade)\n5\n5.636364\n2\nTRUE\n\n\nBi07\n20\n2\nWhite British\n50-59% (2:2 grade)\n2\n5.454546\n2\nFALSE\n\n\nSK03\n22\n2\nWhite British\n≥ 70% (1st class grade)\n6\n6.272727\n6\nTRUE\n\n\nSM95\n26\n2\nWhite British\n60-69% (2:1 grade)\n2\n5.000000\n2\nFALSE\n\n\nSt01\n22\n2\nWhite British\n60-69% (2:1 grade)\n6\n5.545454\n6\nTRUE\n\n\n\n\n\n\n\nglimpse(data_quiz)\n\nRows: 89\nColumns: 9\n$ Code               &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", \"St01\", \"St10\", \"Wa…\n$ Age                &lt;chr&gt; \"22\", \"20\", \"22\", \"26\", \"22\", \"20\", \"21\", \"21\", \"22…\n$ Gender             &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Ethnicity          &lt;chr&gt; \"White European\", \"White British\", \"White British\",…\n$ Secondyeargrade    &lt;fct&gt; 60-69% (2:1 grade), 50-59% (2:2 grade), ≥ 70% (1st …\n$ QRP_item           &lt;dbl&gt; 5, 2, 6, 2, 6, 4, 6, 3, 7, 3, 3, 4, 4, 4, 4, 6, 3, …\n$ QRPs_mean          &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.000000, 5.545455, 6…\n$ Understanding_item &lt;chr&gt; \"2\", \"2\", \"6\", \"2\", \"6\", \"Not at all confident\", \"4…\n$ `QRP_item &gt; 4`     &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE,…\n\n\nSelect from the dropdown menu the variable type and their data types for each of the columns.\n\n\n\n\n\n\n\nColumn\nVariable type\nData type\n\n\n\nAge\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nGender\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nEthinicity\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nSecondyeargrade\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRP_item\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRPs_mean\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nUnderstanding_item\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRP_item &gt; 4\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\n\n\n\n\n\n\n\n\n\nRevealing the mystery code that created data_quiz\n\n\n\n\n\nThe code might look a bit complex for the minute despite the line-by-line explanations below. Come back to it after completing chapter 2.\n\ndata_quiz &lt;- data_prp %&gt;% \n  select(Code, Age, Gender, Ethnicity, Secondyeargrade, QRP_item = QRPs_3_Time1, QRPs_mean = QRPs_Acceptance_Time2_mean, Understanding_item = Understanding_OS_1_Time1) %&gt;% \n  mutate(Gender = factor(Gender),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         `QRP_item &gt; 4` = case_when(\n           QRP_item &gt; 4 ~ TRUE, \n           .default = FALSE))\n\nLets go through this line by line:\n\n\nline 1: creates a new object called data_quiz and it is based on the already existing data object data_prp\n\n\nline 2: we are selecting a few variables of interest, such as Code, Age etc. Some of those variables were renamed in the process according to the structure new_name = old_name, for example QRP item 3 at time point 1 got renamed as QRP_item.\n\n\nline 3: The function mutate() is used to create a new column called Gender that turns the existing column Gender from a numeric value into a factor. R simply overwrites the existing column of the same name. If we had named the new column Gender_factor, we would have been able to retain the original Gender column and Gender_factor would have been added as the last column.\n\nline 4-6: See how the line starts with an indent which indicates we are still within the mutate() function. You can also see this by counting brackets - in line 3 there are 2 opening brackets but only 1 closes.\n\nSimilar to Gender, we are replacing the “old” Secondyeargrade with the new Secondyeargrade column that is now a factor.\nTurning our variable Secondyeargrade into a factor, spot the difference between this attempt and the one we used for Gender? Here we are using a lot more arguments in that factor function, namely levels and labels. Levels describes the unique values we have for that column, and in labels we want to define how these levels will be shown in the data object. If you don’t add the levels and labels argument, the labels will be the labels (as you can see in the Gender column in which we kept the numbers).\n\n\n\nline 7: Doesn’t start with a function name and has an indent, which means we are still within the mutate() function - count the opening and closing brackets to confirm.\n\nHere, we are creating a new column called QRP_item &gt; 4. Notice the two backticks we have to use to make this weird column name work? This is because it has spaces (and we did mention that R doesn’t like spaces). So the backticks help R to group it as a unit/ a single name.\nNext we have a case_when() function which helps executing conditional statements. We are using it to check whether a statement is TRUE or FALSE. Here, we ask whether the QRP item (column QRP_item) is larger than 4 (midpoint of the scale) using the Boolean operator &gt;. If the statement is TRUE, the label TRUE should appear in column QRP_item &gt; 4. Otherwise, if the value is equal to 4 or smaller, the label should read FALSE. We will come back to conditional statements in Chapter 2. But long story short, this Boolean expression created the only logical data type in data_quiz.\n\n\n\n\n\n\nAnd with this, we are done with the individual walkthrough. Well done :)"
  },
  {
    "objectID": "01-basics.html#pair-coding",
    "href": "01-basics.html#pair-coding",
    "title": "1  Projects and R Markdown",
    "section": "Pair-coding",
    "text": "Pair-coding\nThe data we will be using in the upcoming lab activities is a randomised controlled trials experiment by Binfet et al. (2021) that was conducted in Canada.\nCitation\n\nBinfet, J. T., Green, F. L. L., & Draper, Z. A. (2021). The Importance of Client–Canine Contact in Canine-Assisted Interventions: A Randomized Controlled Trial. Anthrozoös, 35(1), 1–22. https://doi.org/10.1080/08927936.2021.1944558\n\nAbstract\n\nResearchers have claimed that canine-assisted interventions (CAIs) contribute significantly to bolstering participants’ wellbeing, yet the mechanisms within interactions have received little empirical attention. The aim of this study was to assess the impact of client–canine contact on wellbeing outcomes in a sample of 284 undergraduate college students (77% female; 21% male, 2% non-binary). Participants self-selected to participate and were randomly assigned to one of two canine interaction treatment conditions (touch or no touch) or to a handler-only condition with no therapy dog present. To assess self-reports of wellbeing, measures of flourishing, positive and negative affect, social connectedness, happiness, integration into the campus community, stress, homesickness, and loneliness were administered. Exploratory analyses were conducted to assess whether these wellbeing measures could be considered as measuring a unidimensional construct. This included both reliability analysis and exploratory factor analysis. Based on the results of these analyses we created a composite measure using participant scores on a latent factor. We then conducted the tests of the four hypotheses using these factor scores. Results indicate that participants across all conditions experienced enhanced wellbeing on several measures; however, only those in the direct contact condition reported significant improvements on all measures of wellbeing. Additionally, direct interactions with therapy dogs through touch elicited greater wellbeing benefits than did no touch/indirect interactions or interactions with only a dog handler. Similarly, analyses using scores on the wellbeing factor indicated significant improvement in wellbeing across all conditions (handler-only, d = 0.18, p = 0.041; indirect, d = 0.38, p &lt; 0.001; direct, d = 0.78, p &lt; 0.001), with more benefit when a dog was present (d = 0.20, p &lt; 0.001), and the most benefit coming from physical contact with the dog (d = 0.13, p = 0.002). The findings hold implications for post-secondary wellbeing programs as well as the organization and delivery of CAIs.\n\nHowever, we accessed the data via Ciaran Evans’ github (https://github.com/ciaran-evans/dog-data-analysis). Evans et al. (2023) published a paper that reused the Binfet data for teaching statistics and research methods. If anyone is interested, the accompanying paper is:\n\nEvans, C., Cipolli, W., Draper, Z. A., & Binfet, J. T. (2023). Repurposing a Peer-Reviewed Publication to Engage Students in Statistics: An Illustration of Study Design, Data Collection, and Analysis. Journal of Statistics and Data Science Education, 31(3), 236–247. https://doi.org/10.1080/26939169.2023.2238018\n\nThere are a few changes that Evans and we made to the data:\n\nEvans removed the demographics ethnicity and gender to make the study data available while protecting participant privacy. Which means we’ll have limited demographic variables available, but we will make do with what we’ve got.\nWe modified some of the responses in the raw data csv - for example, we took out impossible response values and replaced them with NA.\nWe replaced some of the numbers with labels to increase the difficulty in the dataset for Chapter 2 and Chapter 3.\n\nTask 1: Create a project folder for the lab activities\nSince we will be working with the same data throughout semester 1, create a separate project for the lab data. Name it something useful, like lab_data or dogs_in_the_lab. Make sure you are not placing it within the project you have already created today. If you need guidance, see Section 1.2 above.\nTask 2: Create a new .Rmd file\n… and name it something useful. If you need help, have a look at Section 1.3.\nTask 3: Download the data\nDownload the data here: data_pair_ch1. The zip folder contains the raw data file with responses to individual questions, a cleaned version of the same data in long format and wide format, and the codebook describing the variables in the raw data file and the long format.\nUnzip the folder and place the data files in the same folder as your project.\nTask 4: Familiarise yourself with the data\nOpen the data files, look at the codebook, and perhaps skim over the original Binfet article (methods in particular) to see what kind of measures they used.\nRead in the raw data file as dog_data_raw and the cleaned-up data (long format) as dog_data_long. See if you can answer the following questions.\n\nlibrary(tidyverse)\n\ndog_data_raw &lt;- read_csv(\"dog_data_raw.csv\")\ndog_data_long &lt;- read_csv(\"dog_data_clean_long.csv\")\n\n\n\nRows: 284 Columns: 136\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (41): GroupAssignment, L2_1, L2_2, L2_3, L2_4, L2_5, L2_6, L2_7, L2_8, L...\ndbl (95): RID, Age_Yrs, Year_of_Study, Live_Pets, Consumer_BARK, S1_1, HO1_1...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 568 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): GroupAssignment, Year_of_Study, Live_Pets, Stage\ndbl (12): RID, Age_Yrs, Consumer_BARK, Flourishing, PANAS_PA, PANAS_NA, SHS,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nHow many participants took part in the study? \n\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nYou can see this from dog_data_raw. Each participant ID is on a single row meaning the number of observations is the number of participants.\nIf you look at dog_data_long, there are 568 observations. Each participant answered the questionnaires pre and post intervention, resulting in 2 rows per participant ID. This means you’d have to divide the number of observations by 2 to get to the number of participants.\n\n\n\n\nHow many different questionnaires did the participants answer? \n\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nThe Binfet paper (e.g., Methods section and/or abstract) and the codebook show it’s 9 questionnaires - Flourishing scale (variable Flourishing), the UCLS Loneliness scale Version 3 (Loneliness), Positive and Negative affect scale (PANAS_PA and PANAS_NA), the Subjective Happiness scale (SHS), the Social connectedness scale (SCS), and 3 scales with 1 question each, i.e., perception of stress levels (Stress), self-reported level of homesickness (Homesick), and integration into the campus community (Engagement).\nHowever, if you thought PANAS_PA and PANAS_NA are a single questionnaire, 8 was also acceptable as an answer here."
  },
  {
    "objectID": "01-basics.html#test-your-knowledge",
    "href": "01-basics.html#test-your-knowledge",
    "title": "1  Projects and R Markdown",
    "section": "Test your knowledge",
    "text": "Test your knowledge\nAre you ready for some knowledge check questions to test your understanding of the chapter? We also have some faulty codes. See if you can spot what’s wrong with them.\nKnowledge check\nQuestion 1\nOne of the key first steps when we open RStudio is to:\n\nput on some music as we will be here a whileopen an existing project or create a new onemake a coffeecheck out the news\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nOpening an existing project (e.g., when coming back to the same dataset) or creating a new project (e.g., for a new task or new dataset) ensures that subsequent .Rmd files, any output, figures, etc are saved within the same folder on your computer (i.e., the working directory). If the.Rmd files or data is not in the same folder as “the project icon”, things can get messy and code might not run.\n\n\n\nQuestion 2\nWhen using the default environment colour settings for RStudio, what colour would the background of a code chunk be in R Markdown? \nred\nwhite\ngreen\ngrey\nWhen using the default environment colour settings for RStudio, what colour would the background of normal text be in R Markdown? \nred\nwhite\ngreen\ngrey\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nAssuming you have not changed any of the settings in RStudio, code chunks will tend to have a grey background and normal text will tend to have a white background. This is a good way to check that you have closed and opened code chunks correctly.\n\n\n\nQuestion 3\nCode chunks start and end with:\n\nthree single quotesthree backticksthree double quotesthree single asterisks\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nCode chunks always take the same general format of three backticks followed by curly parentheses and a lower case r inside the parentheses ({r}). People often mistake these backticks for single quotes but that will not work. If you have set your code chunk correctly using backticks, the background colour should change to grey from white.\n\n\n\nQuestion 4\nWhat is the correct way to include a code chunk in RMarkdown that will be executed but neither the code nor its output will be shown in the final HTML document? \n{r, echo=FALSE}\n{r, eval=FALSE}\n{r, include=FALSE}\n{r, results=‘hide’}\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nCheck the table of knitr display options in Section 1.3.2.\n\n{r, echo=FALSE} also executes the code and does not show the code, but it does display the result in the knitted html file. (matches 2/3 criteria)\n{r, eval=FALSE} does not show the results but does not execute the code and it does show it in the knitted file. (matches 1/3 criteria)\n{r, results=“hide”} executes the code and does not show results, however, it does include the code in the knitted html document. (matches 2/3 criteria)\n\n\n\n\nError mode\nSome of these codes have mistakes in them, other code chunks are not quite producing what was aimed for. Your task is to spot anything faulty, explain why the things happened, and perhaps try to fix them.\nQuestion 5\nYou want to read in data with the read_csv() function. You have just stated R, created a new .Rmd file, and typed the following code into your code chunk.\n\ndata &lt;- read_csv(\"data.csv\")\n\nHowever, R gives you an error message: could not find function \"read_csv\". What could be the reason?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\n“Could not find function” is an indication that you have forgotten to load in tidyverse. Because read_csv() is a function in the tidyverse collection, R cannot find it.\nFIX: Add library(tidyverse) prior to reading in the data and run the code chunk again.\n\n\n\nQuestion 6\nYou want to read in data with the read_csv() function. This time, you are certain you have loaded in tidyverse first. The code is as follows:\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data.csv\")\n\nThe error message shows 'data.csv' does not exist in current working directory. You check your folder and it looks like this:\n\nWhy is there an error message?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nR is looking for a csv file that is called data which is currently not in the working directory. We may assume it’s in the data folder. Perhaps that happened when unzipping the zip file. So instead of placing the csv file on the same level as the project icon, it was unzipped into a folder named data.\nFIX - option 1: Take the data.csv out of the data folder and place it next to the project icon and the .Rmd file.\nFIX - option 2: Modify your R code to tell R that the data is in a separate folder called data, e.g., …\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/data.csv\")\n\n\n\n\nQuestion 7\nYou want to load tidyverse into the library. The code is as follows:\n\nlibrary(tidyverse)\n\nThe error message says: Error in library(tidyverse) : there is no package called ‘tidyverse’\nWhy is there an error message and how can we fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nIf R says there is no package called tidyverse, means you haven’t installed the package yet. This could be an error message you receive either after switching computers or a fresh install of R and RStudio.\nFIX: Type install.packages(\"tidyverse\") into your Console.\n\n\n\nQuestion 8\nYou knitted your .Rmd into a html but the output is not as expected. You see the following:\n\nWhy did the file not knit properly?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThere is a backtick missing in the code chunk. If you check your .Rmd file, you can see that the code chunk does not show up in grey which means it’s one of the 3 backticks at the beginning of the chunk.\n\nFIX: Add a single backtick manually where it’s missing."
  },
  {
    "objectID": "02-wrangling.html#intended-learning-outcomes",
    "href": "02-wrangling.html#intended-learning-outcomes",
    "title": "2  Data wrangling I",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nIn the next two chapters, we will build on the data wrangling skills from level 1. We will revisit all the functions you have already encountered (and might have forgotten over the summer break) and introduce 2 or 3 new functions. These two chapters will provide an opportunity to revise and apply the functions to a novel dataset.\nBy the end of this chapter, you should be able to:\n\napply familiar data wrangling functions to novel datasets\nread and interpret error messages\nrealise there are several ways of getting to the results\nexport data objects as csv files\n\nThe main purpose of this chapter and Chapter 3 is to wrangle your data into shape for data visualisation (Chapter 4 and Chapter 5). For the two chapters, we will:\n\ncalculate demographics\ntidy 3 different questionnaires with varying degrees of complexity\nsolve an error mode problem\njoin all data objects together"
  },
  {
    "objectID": "02-wrangling.html#individual-walkthrough",
    "href": "02-wrangling.html#individual-walkthrough",
    "title": "2  Data wrangling I",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough\nBefore we start, we need to set up some things."
  },
  {
    "objectID": "02-wrangling.html#activity-1-setup",
    "href": "02-wrangling.html#activity-1-setup",
    "title": "2  Data wrangling I",
    "section": "\n2.1 Activity 1: Setup",
    "text": "2.1 Activity 1: Setup\n\nWe will be working on the dataset by Pownall et al. (2023) again, which means we can still use the project we created last week. The data files will already be there, so no need to download them again.\nTo open the project in RStudio, go to the folder in which you stored the project and the data last time, and double click on the project icon.\n\nCreate a new .Rmd file for chapter 2 and save it to your project folder. Name it something meaningful (e.g., “chapter_02.Rmd”, “02_data_wrangling.Rmd”). See Section 1.3 if you need some guidance.\nIn your newly created .Rmd file, delete everything below line 12 (after the set-up code chunk)."
  },
  {
    "objectID": "02-wrangling.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "href": "02-wrangling.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "title": "2  Data wrangling I",
    "section": "\n2.2 Activity 2: Load in the libraries and read in the data",
    "text": "2.2 Activity 2: Load in the libraries and read in the data\nWe will use tidyverse today, and we want to create a data object data_prp that stores the data from the file prp_data_reduced.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(???)\ndata_prp &lt;- read_csv(\"???\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n\nIf you need a quick reminder what the dataset was about, have a look at the abstract in Section 1.4. We also addressed the changes we made to the dataset there.\nAnd remember to have a quick glimpse() at your data."
  },
  {
    "objectID": "02-wrangling.html#activity-3-calculating-demographics",
    "href": "02-wrangling.html#activity-3-calculating-demographics",
    "title": "2  Data wrangling I",
    "section": "\n2.3 Activity 3: Calculating demographics",
    "text": "2.3 Activity 3: Calculating demographics\nLet’s start with some simple data-wrangling steps to compute demographics for our original dataset, data_prp. First, we want to determine how many participants took part in the study by Pownall et al. (2023) and compute the mean age and the standard deviation of age for the sample.\n\n2.3.1 … for the full sample using summarise()\n\nThe summarise() function is part of the “Wickham Six” alongside group_by(), select(), filter(), mutate(), and arrange(). You used them plenty of times last year.\nWithin summarise(), we can use the n() function, which calculates the number of rows in the dataset. Since each row corresponds to a unique participant, this gives us the total number of participants.\nTo calculate the mean age and the standard deviation of age, we need to use the functions mean() and sd() on the column Age respectively.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age), # mean age\n            sd_age = sd(Age)) # standard deviation of age\n\nWarning: There were 2 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `mean_age = mean(Age)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\nNA\nNA\n\n\n\n\n\nR did not give us an error message per se, but the output is not quite as expected either. There are NA values in the mean_age and sd_age columns. Looking at the warning message and at Age, can you explain what happened?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe warning message says: argument is not numeric or logical: returning NA If we look at the Age column more closely, we can see that it’s a character data type.\n\n\n\nFixing Age\n\nMight be wise to look at the unique answers in column Age to determine what is wrong. We can do that with the function distinct().\n\nage_distinct &lt;- data_prp %&gt;% \n  distinct(Age)\n\nage_distinct\n\n\n\n\n\n\n\nShow the unique values of Age.\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n22\n\n\n20\n\n\n26\n\n\n21\n\n\n29\n\n\n23\n\n\n39\n\n\nNA\n\n\n24\n\n\n43\n\n\n31\n\n\n25 years\n\n\n\n\n\n\n\n\n\n\n\nOne cell has the string “years” added to their number 25, which has converted the entire column into a character column.\nWe can easily fix this by extracting only the numbers from the column and converting it into a numeric data type. The parse_number() function, which is part of the tidyverse package, handles both steps in one go (so there’s no need to load additional packages).\nWe will combine this with the mutate() function to create a new column called Age (containing those numeric values), effectively replacing the old Age column (which had the character values).\n\n\n\nparse_number() illustration by Allison Horst (see https://allisonhorst.com/r-packages-functions)\n\n\n\n\ndata_prp &lt;- data_prp %&gt;% \n  mutate(Age = parse_number(Age))\n\ntypeof(data_prp$Age) # fixed\n\n[1] \"double\"\n\n\nComputing summary stats\nExcellent. Now that the numbers are in a numeric format, let’s try calculating the demographics for the total sample again.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age), # mean age\n            sd_age = sd(Age)) # standard deviation of age\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\nNA\nNA\n\n\n\n\n\nEven though there’s no error or warning, the table still shows NA values for mean_age and sd_age. So, what could possibly be wrong now?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nDid you notice that the Age column in age_distinct contains some missing values (NA)? To be honest, it’s easier to spot this issue in the actual R output than in the printed HTML page.\n\n\n\nComputing summary stats - third attempt\nTo ensure R ignores missing values during calculations, we need to add the extra argument na.rm = TRUE to the mean() and sd() functions.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age, na.rm = TRUE), # mean age\n            sd_age = sd(Age, na.rm = TRUE)) # standard deviation of age\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\n21.88506\n3.485603\n\n\n\n\n\nFinally, we’ve got it! 🥳 Third time’s the charm!\n\n2.3.2 … per gender using summarise() and group_by()\n\nNow we want to compute the summary statistics for each gender. The code inside the summarise() function remains unchanged; we just need to use the group_by() function beforehand to tell R that we want to compute the summary statistics for each group separately. It’s also a good practice to use ungroup() afterwards, so you are not taking groupings forward unintentionally.\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% # split data up into groups (here Gender)\n  summarise(n = n(), # participant number \n            mean_age = mean(Age, na.rm = TRUE), # mean age \n            sd_age = sd(Age, na.rm = TRUE)) %&gt;%  # standard deviation of age\n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\nmean_age\nsd_age\n\n\n\n1\n17\n23.31250\n5.770254\n\n\n2\n69\n21.57353\n2.738973\n\n\n3\n3\n21.33333\n1.154700\n\n\n\n\n\n\n\n2.3.3 Adding percentages\nSometimes, it may be useful to calculate percentages, such as for the gender split. You can do this by adding a line within the summarise() function to perform the calculation. All we need to do is take the number of female, male, and non-binary participants (stored in the n column of demo_by_gender), divide it by the total number of participants (stored in the n column of demo_total), and multiply by 100. Let’s add percentage to the summarise() function of demo_by_gender. Make sure that the code for percentages is placed after the value for n has been computed.\nAccessing the value of n for the different gender categories is straightforward because we can refer back to it directly. However, since the total number of participants is stored in a different data object, we need to use a base R function to access it – specifically the $ operator. To do this, you simply type the name of the data object (in this case, demo_total), followed by the $ symbol (with no spaces), and then the name of the column you want to retrieve (in this case, n). The general pattern is data$column.\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(n = n(), \n            # n from the line above divided by n from demo_total *100\n            percentage = n/demo_total$n *100, \n            mean_age = mean(Age, na.rm = TRUE), \n            sd_age = sd(Age, na.rm = TRUE)) %&gt;% \n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\npercentage\nmean_age\nsd_age\n\n\n\n1\n17\n19.101124\n23.31250\n5.770254\n\n\n2\n69\n77.528090\n21.57353\n2.738973\n\n\n3\n3\n3.370786\n21.33333\n1.154700\n\n\n\n\n\n\n\n\n\n\n\n\nTip for decimal places - use round()\n\n\n\n\n\nNot super important, because you could round the values by yourself when writing up your reports, but if you wanted to tidy up the decimal places in the output, you can do that using the round() function. You would need to “wrap” it around your computations and specify how many decimal places you want to display (for example mean(Age) would turn into round(mean(Age), 1)). It may look odd for percentage, just make sure the number that specifies the decimal places is placed within the round function. The default value is 0 (meaning no decimal spaces).\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(n = n(), \n            percentage = round(n/demo_total$n *100, 2), # percentage with 2 decimal places\n            mean_age = round(mean(Age, na.rm = TRUE), 1), # mean Age with 1 decimal place\n            sd_age = round(sd(Age, na.rm = TRUE), 3)) %&gt;% # sd Age with 3 decimal places\n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\npercentage\nmean_age\nsd_age\n\n\n\n1\n17\n19.10\n23.3\n5.770\n\n\n2\n69\n77.53\n21.6\n2.739\n\n\n3\n3\n3.37\n21.3\n1.155"
  },
  {
    "objectID": "02-wrangling.html#sec-ch2_act4",
    "href": "02-wrangling.html#sec-ch2_act4",
    "title": "2  Data wrangling I",
    "section": "\n2.4 Activity 4: Questionable Research Practices (QRPs)",
    "text": "2.4 Activity 4: Questionable Research Practices (QRPs)\nThe main goal is to compute the mean QRP score per participant for time point 1.\nLooking at the QRP data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nSo, we just have to compute an average score for items 1 to 11 as items 12 to 15 are distractor items. Seems quite straightforward.\nThe downside is that individual items are each in a separate column, i.e., in wide format, and everything would be easier if the items were arranged in long format.\nLet’s tackle this problem in steps. Best would be to create a separate data object for that. If we wanted to compute this within data_prp, it would turn into a nightmare.\n\n\nStep 1: Select the relevant columns Code, and QRPs_1_Time1 to QRPs_1_Time1 and store them in an object called qrp_t1\n\n\nStep 2: Pivot the data from wide format to long format using pivot_longer() so we can calculate the average score more easily (in step 3)\n\nStep 3: Calculate the average QRP score (QRPs_Acceptance_Time1_mean) per participant using group_by() and summarise()\n\n\n\nqrp_t1 &lt;- data_prp %&gt;% \n  #Step 1\n  select(Code, QRPs_1_Time1:QRPs_11_Time1) %&gt;%\n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(Code) %&gt;% # grouping py participant id\n  summarise(QRPs_Acceptance_Time1_mean = mean(Scores)) %&gt;% # calculating the average Score\n  ungroup() # just make it a habit\n\n\n\n\n\n\n\nExplain the individual functions\n\n\n\n\n\n\n\nselect ()\npivot_longer()\ngroup_by() and summarise()\n\n\n\nThe select function allows to include or exclude certain variables (columns). Here we want to focus on the participant ID column (i.e., Code) and the QRP items at time point 1. We can either list them all individually, i.e., Code, QRPs_1_Time1, QRPs_2_Time1, QRPs_3_Time1, and so forth (you get the gist), but that would take forever to type.\nA shortcut is to use the colon operator :. It allows us to select all columns that fall within the range of first_column_name to last_column_name. We can apply this here since the QRP items (1 to 11) are sequentially listed in data_prp.\n\nqrp_step1 &lt;- data_prp %&gt;% \n  select(Code, QRPs_1_Time1:QRPs_11_Time1)\n\n# show first 5 rows of qrp_step1\nhead(qrp_step1, n = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nQRPs_1_Time1\nQRPs_2_Time1\nQRPs_3_Time1\nQRPs_4_Time1\nQRPs_5_Time1\nQRPs_6_Time1\nQRPs_7_Time1\nQRPs_8_Time1\nQRPs_9_Time1\nQRPs_10_Time1\nQRPs_11_Time1\n\n\n\nTr10\n7\n7\n5\n7\n3\n4\n5\n7\n6\n7\n7\n\n\nBi07\n7\n7\n2\n7\n3\n7\n7\n7\n7\n6\n7\n\n\nSK03\n7\n7\n6\n6\n7\n6\n7\n7\n7\n5\n7\n\n\nSM95\n7\n7\n2\n6\n7\n5\n7\n7\n4\n2\n4\n\n\nSt01\n7\n7\n6\n7\n2\n7\n7\n7\n7\n5\n7\n\n\n\n\n\n\nHow many rows/observations and columns/variables do we have in qrp_step1?\n\nrows/observations: \n\ncolumns/variables: \n\n\n\n\nAs you can see, the table we got from Step 1 is in wide format. To get it into wide format, we need to define:\n\nthe columns that need to be reshuffled from wide into long format (col argument). Here we selected “everything except the Code column”, as indicated by -Code [minus Code]. However, QRPs_1_Time1:QRPs_11_Time1 would also work and give you the exact same result.\nthe names_to argument. R is creating a new column in which all the column names from the columns you selected in col will be stored in. Here we are naming this column “Items” but you could pick something equally sensible if you like.\nthe values_to argument. R creates this second column to store all responses the participants gave to the individual questions, i.e., all the numbers in this case. We named it “Scores” here, but you could have called it something different, like “Responses”\n\n\nqrp_step2 &lt;- qrp_step1 %&gt;% \n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Scores\")\n\n# show first 15 rows of qrp_step2\nhead(qrp_step2, n = 15)\n\n\n\n\nCode\nItems\nScores\n\n\n\nTr10\nQRPs_1_Time1\n7\n\n\nTr10\nQRPs_2_Time1\n7\n\n\nTr10\nQRPs_3_Time1\n5\n\n\nTr10\nQRPs_4_Time1\n7\n\n\nTr10\nQRPs_5_Time1\n3\n\n\nTr10\nQRPs_6_Time1\n4\n\n\nTr10\nQRPs_7_Time1\n5\n\n\nTr10\nQRPs_8_Time1\n7\n\n\nTr10\nQRPs_9_Time1\n6\n\n\nTr10\nQRPs_10_Time1\n7\n\n\nTr10\nQRPs_11_Time1\n7\n\n\nBi07\nQRPs_1_Time1\n7\n\n\nBi07\nQRPs_2_Time1\n7\n\n\nBi07\nQRPs_3_Time1\n2\n\n\nBi07\nQRPs_4_Time1\n7\n\n\n\n\n\n\nNow, have a look at qrp_step2. In total, we now have  rows/observations,  per participant, and  columns/variables.\n\n\nThis follows exactly the same sequence we used when calculating descriptive statistics by gender. The only difference is that we are now grouping the data by the participant’s Code instead of Gender.\nsummarise() works exactly the same way: summarise(new_column_name = function_to_calculate_something(column_name_of_numeric_values))\nThe function_to_calculate_something can be mean(), sd() or sum() for mean scores, standard deviations, or summed-up scores respectively. You could also use min() or max() if you wanted to determine the lowest or the highest score for each participant.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou could rename the columns whilst selecting them. The pattern would be select(new_name = old_name). For example, if we wanted to select variable Code and rename it as Participant_ID, we could do that.\n\nrenaming_col &lt;- data_prp %&gt;% \n  select(Participant_ID = Code)\n\nhead(renaming_col, n = 5)\n\n\n\n\nParticipant_ID\n\n\n\nTr10\n\n\nBi07\n\n\nSK03\n\n\nSM95\n\n\nSt01"
  },
  {
    "objectID": "02-wrangling.html#activity-5-knitting",
    "href": "02-wrangling.html#activity-5-knitting",
    "title": "2  Data wrangling I",
    "section": "\n2.5 Activity 5: Knitting",
    "text": "2.5 Activity 5: Knitting\nOnce you’ve completed your R Markdown file, the final step is to “knit” it, which converts the .Rmd file into a HTML file. Knitting combines your code, text, and output (like tables and plots) into a single cohesive document. This is a really good way to check your code is working.\nTo knit the file, click the Knit button at the top of your RStudio window. The document will be generated and, depending on your setting, automatically opened in the viewer in the Output pane or an external browser window.\nIf any errors occur during knitting, RStudio will show you an error message with details to help you troubleshoot.\nIf you want to intentionally keep any errors we tackled today to keep a reference on how you solved them, you could add error=TRUE or eval=FALSE to the code chunk that isn’t running."
  },
  {
    "objectID": "02-wrangling.html#activity-6-export-a-data-object-as-a-csv",
    "href": "02-wrangling.html#activity-6-export-a-data-object-as-a-csv",
    "title": "2  Data wrangling I",
    "section": "\n2.6 Activity 6: Export a data object as a csv",
    "text": "2.6 Activity 6: Export a data object as a csv\nTo avoid having to repeat the same steps in the next chapter, it’s a good idea to save the data objects you’ve created today as csv files. You can do this by using the write_csv() function from the readr package. The csv files will appear in your project folder.\nThe basic syntax is:\n\nwrite_csv(data_object, \"filename.csv\")\n\nNow, let’s export the objects data_prp and qrp_t1.\n\nwrite_csv(data_prp, \"data_prp_for_ch3.csv\")\n\nHere we named the file data_prp_for_ch3.csv, so we wouldn’t override the original data csv file prp_data_reduced.csv. However, feel free to choose a name that makes sense to you.\n\n\n\n\n\n\nYour Turn\n\n\n\nExport qrp_t1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwrite_csv(qrp_t1, \"qrp_t1.csv\")\n\n\n\n\n\n\nCheck that your csv files have appeared in your project folder, and you’re all set!\nThat’s it for Chapter 2: Individual Walkthrough."
  },
  {
    "objectID": "02-wrangling.html#pair-coding",
    "href": "02-wrangling.html#pair-coding",
    "title": "2  Data wrangling I",
    "section": "Pair-coding",
    "text": "Pair-coding\nWe will continue working with the data from Binfet et al. (2021), focusing on the randomised controlled trial of therapy dog interventions. Today, our goal is to calculate an average Flourishing score for each participant at time point 1 (pre-intervention) using the raw data file dog_data_raw. Currently, the data looks like this:\n\n\n\n\n\nRID\nF1_1\nF1_2\nF1_3\nF1_4\nF1_5\nF1_6\nF1_7\nF1_8\n\n\n\n1\n6\n7\n5\n5\n7\n7\n6\n6\n\n\n2\n5\n7\n6\n5\n5\n5\n5\n4\n\n\n3\n5\n5\n5\n6\n6\n6\n5\n5\n\n\n4\n7\n6\n7\n7\n7\n6\n7\n4\n\n\n5\n5\n5\n4\n6\n7\n7\n7\n6\n\n\n\n\n\n\nHowever, we want the data to look like this:\n\n\n\n\n\nRID\nFlourishing_pre\n\n\n\n1\n6.125\n\n\n2\n5.250\n\n\n3\n5.375\n\n\n4\n6.375\n\n\n5\n5.875\n\n\n\n\n\n\nTask 1: Open the R project you created last week\nIf you haven’t created an R project for the lab yet, please do so now. If you already have one set up, go ahead and open it.\nTask 2: Open your .Rmd file from last week\nSince we haven’t used it much yet, feel free to continue using the .Rmd file you created last week in Task 2.\nTask 3: Load in the library and read in the data\nThe data should be in your project folder. If you didn’t download it last week, or if you’d like a fresh copy, you can download the data again here: data_pair_ch1.\nWe will be using the tidyverse package today, and the data file we need to read in is dog_data_raw.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(???)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"???\")\n\n\n\n\nTask 4: Calculating the mean for Flourishing_pre\n\n\n\nStep 1: Select all relevant columns, including participant ID and all 8 items from the Flourishing questionnaire completed before the intervention. Store this data in an object called data_flourishing.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nLook at the codebook. Try to determine:\n\nThe variable name of the column where the participant ID is stored.\nThe items related to the Flourishing scale at the pre-intervention stage.\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\nFrom the codebook, we know that:\n\nThe participant ID column is called RID.\nThe Flourishing items at the pre-intervention stage start with F1_.\n\n\n\n\n\n\n\n\n\nStep 2: Pivot the data from wide format to long format so we can calculate the average score more easily (in step 3).\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nWhich pivot function should you use? We have pivot_wider() and pivot_longer() to choose from.\nWe also need 3 arguments in that function:\n\nThe columns you want to select (e.g., all the Flourishing items),\nThe name of the column where the current column headings will be stored (e.g., “Questionnaire”),\nThe name of the column that should store all the values (e.g., “Responses”).\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\")\n\n\n\n\n\n\n\n\n\nStep 3: Calculate the average Flourishing score per participant and name this column Flourishing_pre to match the table above.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nBefore summarising the mean, you may need to group the data.\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\n  group_by(???) %&gt;% \n  summarise(Flourishing_pre = ???(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(tidyverse)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"dog_data_raw.csv\")\n\n# Task 4: Tidying \ndata_flourishing &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, F1_1:F1_8) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Questionnaire\", values_to = \"Responses\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(Flourishing_pre = mean(Response)) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "02-wrangling.html#test-your-knowledge-and-challenge-yourself",
    "href": "02-wrangling.html#test-your-knowledge-and-challenge-yourself",
    "title": "2  Data wrangling I",
    "section": "Test your knowledge and challenge yourself",
    "text": "Test your knowledge and challenge yourself\nKnowledge check\nQuestion 1\nWhich function of the Wickham Six would you use to include or exclude certain variables (columns)? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 2\nWhich function of the Wickham Six would you use to create new columns or modify existing columns in a dataframe? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 3\nWhich function of the Wickham Six would you use to organise data into groups based on one or more columns? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 4\nWhich function of the Wickham Six would you use to sort the rows of a dataframe based on the values in one or more columns? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\nQuestion 5\nWhich function of the Wickham Six would NOT modify the original dataframe? \nselect()\nfilter()\nmutate()\narrange()\ngroup_by()\nsummarise()\n\n\n\n\n\n\nExplain these answers\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\nselect()\nInclude or exclude certain variables/columns\n\n\nfilter()\nInclude or exclude certain observations/rows\n\n\nmutate()\nCreates new columns or modifies existing ones\n\n\narrange()\nChanges the order of the rows\n\n\ngroup_by()\nSplit data into groups based on one or more variables\n\n\nsummarise()\nCreates a new dataframe returning one row for each combination of grouping variables\n\n\n\nTechnically, the first five functions operate on the existing data object, making adjustments like sorting the data (e.g., with arrange()), reducing the number of rows (e.g., with filter()), reducing the number of columns (e.g., with select()), or adding new columns (e.g., with mutate()). In contrast, summarise() fundamentally alters the structure of the original dataframe by generating a completely new dataframe that contains only summary statistics, rather than retaining the original rows and columns.\n\n\n\nError mode\nSome of the code chunks contain mistakes and result in errors, while others do not produce the expected results. Your task is to identify any issues, explain why they occurred, and, if possible, fix them.\nWe will use a few built-in datasets, such as billboard and starwars, to help you replicate the errors in your own R environment. You can view the data either by typing the dataset name directly into your console or by storing the data as a separate object in your Global Environment.\n\nbillboard\n\nstarwars_data = starwars\n\nQuestion 6\nCurrently, the weekly song rankings for Billboard Top 100 in 2000 are in wide format, with each week in a separate column. The following code is supposed to transpose the wide-format billboard data into long format:\n\nlong_data &lt;- billboard %&gt;% \n  pivot_longer(names_to = \"weeks\", values_to = \"rank\")\n\nHowever, an error occurs during execution: `cols` must select at least one column. What does this error message mean and how do you fix it?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe error message indicates that the cols argument is missing in the function. This means the function doesn’t know which columns to transpose from wide format to long format.\nFIX: Add cols = wk1:wk76 to the function to select columns from wk1 to wk76. Alternatively, cols = starts_with(\"wk\") would also work since all columns start with the letter combination “wk”.\n\nlong_data &lt;- billboard %&gt;% \n  pivot_longer(cols = wk1:wk76, names_to = \"weeks\", values_to = \"rank\")\n# OR\nlong_data &lt;- billboard %&gt;% \n  pivot_longer(cols = starts_with(\"wk\"), names_to = \"weeks\", values_to = \"rank\")\n\n\n\n\nQuestion 7\nThe following code is intended to calculate the mean height of all the characters in the built-in starwars dataset, grouped by their gender.\n\nsummary_data &lt;- starwars %&gt;%\n  group_by(gender) %&gt;%\n  summarise(mean_height = height)\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\nThe code runs, but it’s giving us some weird warning and the output is also not as expected. What steps should we take to fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe aggregation function mean() is missing from within summarise(). Without it, the function does not perform any aggregation and returns all rows with only the columns for gender and height.\nFIX: Wrap the mean() function around the variable you want to aggregate, here height.\n\nsummary_data &lt;- starwars %&gt;%\n  group_by(gender) %&gt;%\n  summarise(mean_height = mean(height))\n\n\n\n\nQuestion 8\nFollowing up on Question 7, we now have summary_data that looks approximately correct - it has the expected rows and column numbers, however, the cell values are “weird”.\n\nsummary_data\n\n\n\n\ngender\nmean_height\n\n\n\nfeminine\nNA\n\n\nmasculine\nNA\n\n\nNA\n175\n\n\n\n\n\n\nCan you explain what is happening here? And how can we modify the code to fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nLook at the original starwars data. You will notice that some of the characters with feminine and masculine gender entries have missing height values. However, all four characters without a specified gender have provided their height.\nFIX: We need to add na.rm = TRUE to the mean() function to ensure that R ignores missing values before aggregating the data.\n\nsummary_data &lt;- starwars %&gt;%\n  group_by(gender) %&gt;%\n  summarise(mean_height = mean(height, na.rm = TRUE))\n\nsummary_data\n\n\n\n\ngender\nmean_height\n\n\n\nfeminine\n166.5333\n\n\nmasculine\n176.5323\n\n\nNA\n175.0000\n\n\n\n\n\n\n\n\n\nChallenge Yourself\nIf you want to challenge yourself and further apply the skills from Chapter 2, you can wrangle the data from dog_data_raw for additional questionnaires from either the pre- and/or post-intervention stages:\n\nCalculate the mean score for flourishing_post for each participant.\nCalculate the mean score for the PANAS (Positive and/or Negative Affect) per participant\nCalculate the mean score for happiness (SHS) per participant\n\nThe 3 steps are equivalent for those questionnaires - select, pivot, group_by and summarise; you just have to “replace” the questionnaire items involved.\n\n\n\n\n\n\nSolution for Challenge yourself\n\n\n\n\n\nFlourishing post-intervention\n\n## flourishing_post\nflourishing_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"F2\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(Flourishing_post = mean(Response)) %&gt;% \n  ungroup()\n\nThe PANAS could be solved more concisely with the skills we learn in Chapter 3, but for now, you would have solved it this way:\n\n# PANAS - positive affect pre\nPANAS_PA_pre &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN1_3, PN1_5, PN1_7, PN1_8, PN1_10) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_PA_pre = mean(Scores)) %&gt;% \n  ungroup()\n\n# PANAS - positive affect post\nPANAS_PA_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN2_3, PN2_5, PN2_7, PN2_8, PN2_10) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_PA_post = mean(Scores)) %&gt;% \n  ungroup()\n\n# PANAS - negative affect pre\nPANAS_NA_pre &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN1_1, PN1_2, PN1_4, PN1_6, PN1_9) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_NA_pre = mean(Scores)) %&gt;% \n  ungroup()\n\n# PANAS - negative affect post\nPANAS_NA_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, PN2_1, PN2_2, PN2_4, PN2_6, PN2_9) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(PANAS_NA_post = mean(Scores)) %&gt;% \n  ungroup()\n\nHappiness scale\n\n# happiness_pre\nhappiness_pre &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, HA1_1, HA1_2, HA1_3) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Item\", values_to = \"Score\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(SHS_pre = mean(Score)) %&gt;% \n  ungroup()\n\n#happiness_post\nhappiness_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, HA2_1, HA2_2, HA2_3) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Item\", values_to = \"Score\") %&gt;% \n  # Step 3\n  group_by(RID) %&gt;% \n  summarise(SHS_post = mean(Score)) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#intended-learning-outcomes",
    "href": "03-wrangling2.html#intended-learning-outcomes",
    "title": "3  Data wrangling II",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\napply familiar data wrangling functions to novel datasets\nread and interpret error messages\nrealise there are several ways of getting to the results\n\nIn this chapter, we will continue from where we left off in Chapter 2. We will compute average scores for two of the questionnaires, address an error mode problem, and finally, join all data objects together. This will finalise our data for the upcoming data visualization sections (Chapter 4 and Chapter 5)."
  },
  {
    "objectID": "03-wrangling2.html#individual-walkthrough",
    "href": "03-wrangling2.html#individual-walkthrough",
    "title": "3  Data wrangling II",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "03-wrangling2.html#activity-1-setup",
    "href": "03-wrangling2.html#activity-1-setup",
    "title": "3  Data wrangling II",
    "section": "\n3.1 Activity 1: Setup",
    "text": "3.1 Activity 1: Setup\n\nGo to the project folder we have been using in the last two weeks and double-click on the project icon to open the project in RStudio\nEither Create a new .Rmd file for chapter 3 and save it to your project folder or continue the one from last week. See Section 1.3 if you need some guidance."
  },
  {
    "objectID": "03-wrangling2.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "href": "03-wrangling2.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "title": "3  Data wrangling II",
    "section": "\n3.2 Activity 2: Load in the libraries and read in the data",
    "text": "3.2 Activity 2: Load in the libraries and read in the data\nWe will use tidyverse today, as well as the two csv files we created at the end of the last chapter: data_prp_for_ch3.csv and qrp_t1.csv. If you need to download them for whatever reason, click on the following links: data_prp_for_ch3.csv and qrp_t1.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(???)\ndata_prp &lt;- read_csv(\"???\")\nqrp_t1 &lt;- read_csv(\"???\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\nqrp_t1 &lt;- read_csv(\"qrp_t1.csv\")\n\n\n\n\nIf you need a quick reminder what the dataset was about, have a look at the abstract in Section 1.4. We also addressed the changes we made to the dataset there.\nAnd remember to have a quick glimpse() at your data."
  },
  {
    "objectID": "03-wrangling2.html#activity-3-confidence-in-understanding-open-science-practices",
    "href": "03-wrangling2.html#activity-3-confidence-in-understanding-open-science-practices",
    "title": "3  Data wrangling II",
    "section": "\n3.3 Activity 3: Confidence in understanding Open Science practices",
    "text": "3.3 Activity 3: Confidence in understanding Open Science practices\nThe main goal is to compute the mean Understanding score per participant.\nThe mean Understanding score for time point 2 is already calculated in the (column Time2_Understanding_OS), but we have to compute it for time point 1.\nLooking at the Understanding data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nThe steps are quite similar to those for QRP, but we need to add an extra step: converting the character labels into numbers.\nAgain, let’s do this step by step:\n\n\nStep 1: select the relevant columns Code, and every Understanding column from time point 1 (e.g., from Understanding_OS_1_Time1 to Understanding_OS_12_Time1) and store them in an object called understanding_t1\n\n\nStep 2: pivot the data from wide format to long format using pivot_longer() so we can recode the labels into values (step 3) and calculate the average score (in step 4) more easily\n\nStep 3: Recode the values “Not at all confident” as 1 and “Entirely confident” as 7. All other values are already numbers. We can use functions mutate() in combination with case_match() for that\n\nStep 4: calculate the average QRP score (QRPs_Acceptance_Time1_mean) per participant using group_by() and summarise()\n\nSteps 1 and 2: select and pivot\nHow about you try the first 2 steps yourself using the code from Chapter 2 Activity 4 (Section 2.4) as a template?\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  select(???) %&gt;% # Step 1\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\") # Step 2\n\n\n\n\n\n\n\nSolution for steps 1 and 2\n\n\n\n\n\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, Understanding_OS_1_Time1:Understanding_OS_12_Time1) %&gt;% \n  # Step 2 - I picked different column labels this time for some variety\n  pivot_longer(cols = Understanding_OS_1_Time1:Understanding_OS_12_Time1, names_to = \"Understanding_Qs\", values_to = \"Responses\") \n\n\n\n\nStep 3: recoding the values\nOK, we now want to recode the values in the Responses column (or whatever name you picked for your column that has some of the numbers in it) so that “Not at all confident” = 1 and “Entirely confident” = 7. We want to keep all other values as they are (2-6 look already quite “numeric”).\nLet’s create a new column Responses_corrected that stores the new values with mutate(). Then we can combine that with the case_match() function.\n\nThe first argument in case_match() is the column name of the variable you want to recode.\nThen you can start recoding the values in the way of CurrentValue ~ NewValue (~ is a tilde). Make sure you use the ~ and not =.\nThe .default argument tells R what to do with values that are neither “Not at all confident” nor “Entirely confident”. Here, we want to replace them with the original value of the Responses column. In other datasets, you may want to set the default to NA for missing values, a character string or a number, and case_match() is happy to oblige.\n\n\nunderstanding_t1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = Responses # all other values taken from column Responses\n  ))\n\nError in `mutate()`:\nℹ In argument: `Responses_corrected = case_match(...)`.\nCaused by error in `case_match()`:\n! Can't combine `..1 (right)` &lt;double&gt; and `.default` &lt;character&gt;.\n\n\n\n\n\n\n\n\nError!!! Can you explain what is happening here?\n\n\n\n\n\nHave a look at the error message. It’s pretty helpful this time. It says Can't combine ..1 (right) &lt;double&gt; and .default &lt;character&gt;. It means that the replacement values are expected to be data type character since the original column type was type character.\n\n\n\nSo how do we fix this? Actually, there are several ways this could be done. Click on the tabs below to check out 3 possible solutions.\n\n\nFix option 1\nFix option 2\nFix option 3\n\n\n\nOne option is to modify the .default argument Responses so that the values are copied over from the original column but as a number rather than a character value. The function as.numeric() does the conversion.\n\nunderstanding_t1_step3_v1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  ))\n\n\n\nChange the numeric values on the right side of the ~ to character. Then in a second step, we would need to turn the character column into a numeric type. Again, we have several options to do so. We could either use the parse_number() function we encountered earlier during the demographics wrangling or the as.numeric() function.\n\nV1: Responses_corrected = parse_number(Responses_corrected)\n\nV2: Responses_corrected = as.numeric(Responses_corrected)\n\n\nJust pay attention that you are still working within the mutate() function.\n\nunderstanding_t1_step3_v2 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ \"1\",\n                                          \"Entirely confident\" ~ \"7\",\n                                          .default = Responses # all other values taken from column Responses (character)\n  ),\n  Responses_corrected = parse_number(Responses_corrected)) # turning Responses_corrected into a numeric column\n\n\n\nIf you recode all the labels into numbers (e.g., “2” into 2, “3” into 3, etc.) from the start, you won’t need to perform any additional conversions later.\n\nunderstanding_t1_step3_v2 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_recoded = case_match(Responses, # column of the values to recode\n                                        \"Not at all confident\" ~ 1, # recode all of them\n                                        \"2\" ~ 2,\n                                        \"3\" ~ 3,\n                                        \"4\" ~ 4,\n                                        \"5\" ~ 5,\n                                        \"6\" ~ 6,\n                                        \"Entirely confident\" ~ 7))\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nChoose the option that works best for you to modify the code above that didn’t work. You should now be able to calculate the mean Understanding Score per participant. Store the average scores in a variable called Time1_Understanding_OS. If you need help, refer to the hint below or use Chapter 2 Activity 4 (Section 2.4) as guidance.\n\n\n\n\n\n\nOne solution for Steps 3 and 4\n\n\n\n\n\n\nunderstanding_t1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  )) %&gt;% \n  # Step 4: calculating averages per participant\n  group_by(Code) %&gt;%\n  summarise(Time1_Understanding_OS = mean(Responses_corrected)) %&gt;%\n  ungroup()\n\n\n\n\n\n\nOf course, this could have been written up as a single pipe.\n\n\n\n\n\n\nSingle pipe of activity 3\n\n\n\n\n\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, Understanding_OS_1_Time1:Understanding_OS_12_Time1) %&gt;% \n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Understanding_Qs\", values_to = \"Responses\") %&gt;% \n  # Step 3\n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  )) %&gt;% \n  # Step 4\n  group_by(Code) %&gt;%\n  summarise(Time1_Understanding_OS = mean(Responses_corrected)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#activity-4-survey-of-attitudes-toward-statistics-sats-28",
    "href": "03-wrangling2.html#activity-4-survey-of-attitudes-toward-statistics-sats-28",
    "title": "3  Data wrangling II",
    "section": "\n3.4 Activity 4: Survey of Attitudes Toward Statistics (SATS-28)",
    "text": "3.4 Activity 4: Survey of Attitudes Toward Statistics (SATS-28)\nThe main goal is to compute the mean SATS-28 score for each of the 4 subscales per participant for time point 1.\nLooking at the SATS data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\nAdditionally, we are looking to compute the means for the 4 different subscales of the SAT-28 which are , , , and .\n\nThis scenario is slightly more tricky than the previous ones due to the reverse-coding and the 4 subscales. So, let’s tackle this step by step again:\n\n\nStep 1: select the relevant columns Code, and every SATS28 column from time point 1 (e.g., from SATS28_1_Affect_Time1 to SATS28_28_Difficulty_Time1) and store them in an object called sats_t1\n\n\nStep 2: pivot the data from wide format to long format using pivot_longer() so we can recode the labels into values (step 3) and calculate the average score (in step 4) more easily\n\nStep 3: We need to know which items belong to which subscale - fortunately, we have that information in the variable name and can use the separate() function to access it.\n\nStep 4: We need to know which items are reverse-coded and then reverse-code them - unfortunately, the info is only in the codebook and we need to find a work-around. case_when() can help identify and re-score the reverse-coded items.\n\nStep 5: calculate the average SATS score per participant and subscale using group_by() and summarise()\n\n\nStep 6: use pivot_wider() to spread out the dataframe into wide format and rename() to tidy up the datanames\nSteps 1 and 2: select and pivot\nThe selecting and pivoting are exactly the same way as we already practised in the other 2 questionniares. Apply them here to this questionnaire.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  select(???) %&gt;% # Step 1\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\") # Step 2\n\n\n\n\n\n\n\nSolution for steps 1 and 2\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  select(Code, SATS28_1_Affect_Time1:SATS28_28_Difficulty_Time1) %&gt;% # Step 1\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Response\") # Step 2\n\n\n\n\n\n\n\nStep 3: separate Subscale information\nIf you look at the Items column more closely, you can see that there is information on the Questionnaire, the Item_number, the Subscale, and the Timepoint the data was collected at.\nWe can separate the information into separate columns using the separate() function. The function’s first argument is the column to separate, then define into which columns you want the original column to split up, and lastly, define the separator sep (here an underscore). For our example, we would write:\n\nV1: separate(Items, into = c(\"SATS\", \"Item_number\", \"Subscale\", \"Time\"), sep = \"_\")\n\n\nHowever, we don’t need all of those columns, so we could just drop the ones we are not interested in by replacing them with NA.\n\nV2: separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\")\n\n\nWe might also add an extra argument of convert = TRUE to have numeric columns (i.e., Item_number) converted to numeric as opposed to keeping them as characters. Saves us typing a few quotation marks later in Step 4.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  # Step 3\n  separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\", convert = TRUE)\n\nStep 4: identifying reverse-coded items and then correct them\nWe can use case_when() within the mutate() function here to create a new column FW_RV that stores information on whether the item is a reverse-coded item or not.\ncase_when() works similarly to case_match(), however case_match() only allows you to “recode” values (i.e., replace one value with another), whereas case_when() is more flexible. It allows you to use conditional statements on the left side of the tilde which is useful when you want to change only some of the data based on specific conditions.\nLooking at the codebook, it seems that items 2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28 are reverse-coded. The rest are forward-coded.\nWe want to tell R now, that\n\n\nif the Item_number is any of those numbers listed above, R should write “Reverse” into the new column FW_RV we are creating. Since we have a few possible matches for Item_number, we need the Boolean expression %in% rather than ==.\n\nif Item_number is none of those numbers, then we would like the word “Forward” in the FW_RV column to appear. We can achieve that by specifying a .default argument again, but this time we want a “word” rather than a value from another column.\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(FW_RV = case_when(\n    Item_number %in% c(2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28) ~ \"Reverse\",\n    .default = \"Forward\"\n  ))\n\nMoving on to correcting the scores. Once again, we can use case_when () within the mutate() function to create another conditional statement. This time, the condition is:\n\n\nif FW_RV column has a value of “Reverse” then we would like to turn all 1 into 7, 2 into 6, etc.\n\nif FW_RV column has a value of “Forward” then we would like to keep the score from the Response column\n\nThere is a quick way and a not-so-quick way to achieve the actual reverse-coding.\n\nOption 1 (quick): The easiest way to reverse-code scores is by taking the maximum value of the scale, adding 1 unit and then subtracting the original value. For example, on a 5-point Likert scale, it would be 6 minus the original rating; for a 7-point Likert scale, 8 minus the original rating, etc. (see tab Option 1).\nOption 2 (not so quick): This includes the use of 2 conditional statements (see tab Option 2).\n\nUse the one you find more intuitive.\n\n\nOption 1\nOption 2\n\n\n\nHere we are using the Boolean expression to determine if there is a string “Reverse” in the FW_RV column. And if that conditional statement is TRUE then the value in the new column we are creating Scores_corrected should be calculated as 8 minus the value from the Response column. If it’s not (i.e., the .default argument), then the values of the Response column should be kept.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(Scores_corrected = case_when(\n    FW_RV == \"Reverse\" ~ 8-Response,\n    .default = Response\n  ))\n\n\n\nAs stated above, the longer version would include 2 conditional statements. The first condition checks if the value in FW_RV is “Reverse”. The second condition checks if the value in Response is equal to a specific number. If both conditions are met, then the value on the right side of the tilde should be placed in the newly created Scores_corrected_v2 column.\nFor example, line 3 would read: if the FW_RV value is “Reverse” AND the value in the Response column is 1, then place a value of 7 into Scores_corrected_v2.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(Scores_corrected_v2 = case_when(\n    FW_RV == \"Reverse\" & Response == 1 ~ 7,\n    FW_RV == \"Reverse\" & Response == 2 ~ 6,\n    FW_RV == \"Reverse\" & Response == 3 ~ 5,\n    # no need to recode 4 as 4\n    FW_RV == \"Reverse\" & Response == 5 ~ 3,\n    FW_RV == \"Reverse\" & Response == 6 ~ 2,\n    FW_RV == \"Reverse\" & Response == 7 ~ 1,\n    .default = Response\n  ))\n\nAs you can see now in sats_t1, both columns Scores_corrected and Scores_corrected_v2 are identical.\n\n\n\nOne way of checking whether our reverse-coding worked is to look at the distinct values of the original Response column and Scores_corrected. We would also need to keep information of the FW_RV column.\nTo see the pattern better, we want to use arrange() to sort the values in a more meaningful way. Remember from last year, the default order is ascending, and you would need to add the function desc() on your variable to sort values in descending order.\n\ncheck_coding &lt;- sats_t1 %&gt;% \n  distinct(FW_RV, Response, Scores_corrected) %&gt;% \n  arrange(desc(FW_RV), Response)\n\n\n\n\n\n\n\nShow check_coding output\n\n\n\n\n\n\ncheck_coding\n\n\n\n\nFW_RV\nResponse\nScores_corrected\n\n\n\nReverse\n1\n7\n\n\nReverse\n2\n6\n\n\nReverse\n3\n5\n\n\nReverse\n4\n4\n\n\nReverse\n5\n3\n\n\nReverse\n6\n2\n\n\nReverse\n7\n1\n\n\nForward\n1\n1\n\n\nForward\n2\n2\n\n\nForward\n3\n3\n\n\nForward\n4\n4\n\n\nForward\n5\n5\n\n\nForward\n6\n6\n\n\nForward\n7\n7\n\n\n\n\n\n\n\n\n\nStep 5\nNow that we know everything worked out as intended, we can calculate the mean scores of each subscale for each participant in sats_t1.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  group_by(???, ???) %&gt;% \n  summarise(mean_score = ???(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  group_by(Code, Subscale) %&gt;% \n  summarise(mean_score = mean(Scores_corrected)) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'Code'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nStep 6\nOne final step is to turn the data back into wide format so that each subscale has its own column. That would make joining the data objects easier. The first argument in pivot_wider() is names_from and you should specify the column here that you want as your new column headings. The second argument is values_from and you need to specify the column that you want to get the cell values from\nWe should also rename the column names to match more with the column names in the codebook. Conveniently, we can use a function called rename() that works exactly like select() (i.e., pattern new_name = old_name) but keeps all the other column names the same (rather than reducing the number of columns)\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  pivot_wider(names_from = Subscale, values_from = mean_score) %&gt;% \n  rename(SATS28_Affect_Time1_mean = Affect,\n         SATS28_CognitiveCompetence_Time1_mean = CognitiveCompetence,\n         SATS28_Value_Time1_mean = Value,\n         SATS28_Difficulty_Time1_mean = Difficulty)\n\n\n\n\n\n\n\nShow final sats_t1 output\n\n\n\n\n\n\nhead(sats_t1, n = 5)\n\n\n\n\n\n\n\n\n\n\n\nCode\nSATS28_Affect_Time1_mean\nSATS28_CognitiveCompetence_Time1_mean\nSATS28_Difficulty_Time1_mean\nSATS28_Value_Time1_mean\n\n\n\nAD03\n2.333333\n3.833333\n3.428571\n5.555556\n\n\nAD05\n3.500000\n5.000000\n2.142857\n4.777778\n\n\nAb01\n5.166667\n5.666667\n4.142857\n5.444444\n\n\nAl05\n2.166667\n2.666667\n2.857143\n3.777778\n\n\nAm05\n4.166667\n5.666667\n5.571429\n4.888889\n\n\n\n\n\n\n\n\n\nAgain, this could have been written up as a single pipe.\n\n\n\n\n\n\nSingle pipe of activity 4\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, SATS28_1_Affect_Time1:SATS28_28_Difficulty_Time1) %&gt;% \n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Response\") %&gt;% \n  # Step 3\n  separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\", convert = TRUE) %&gt;% \n  # step 4\n  mutate(FW_RV = case_when(\n    Item_number %in% c(2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28) ~ \"Reverse\",\n    .default = \"Forward\"\n  ),\n    Scores_corrected = case_when(\n      FW_RV == \"Reverse\" ~ 8-Response,\n      .default = Response\n  )) %&gt;% \n  # step 5\n  group_by(Code, Subscale) %&gt;% \n  summarise(mean_score = mean(Scores_corrected)) %&gt;% \n  ungroup() %&gt;% \n  # step 6\n  pivot_wider(names_from = Subscale, values_from = mean_score) %&gt;% \n  rename(SATS28_Affect_Time1_mean = Affect,\n         SATS28_CognitiveCompetence_Time1_mean = CognitiveCompetence,\n         SATS28_Value_Time1_mean = Value,\n         SATS28_Difficulty_Time1_mean = Difficulty)"
  },
  {
    "objectID": "03-wrangling2.html#activity-5-error-mode-perceptions-of-supervisory-support",
    "href": "03-wrangling2.html#activity-5-error-mode-perceptions-of-supervisory-support",
    "title": "3  Data wrangling II",
    "section": "\n3.5 Activity 5 (Error Mode): Perceptions of supervisory support",
    "text": "3.5 Activity 5 (Error Mode): Perceptions of supervisory support\nThe main goal is to compute the mean score for perceived supervisory support per participant.\nLooking at the supervisory support data, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nI have outlined my steps as follows:\n\n\nStep 1: reverse-code the single column first because that’s less hassle than having to do that with conditional statements (Supervisor_15_R). mutate() is my friend.\n\nStep 2: I want to filter out everyone who failed the attention check in Supervisor_7. I can do this with a Boolean expression within the filter() function. The correct response was “completely disagree” which is 1.\n\nStep 3: select their id from time point 2 and all the columns that start with the word “super”, apart from Supervisor_7 and the original Supervisor_15_R column\n\nStep 4: pivot into long format so I can calculate the averages better\n\nStep 5: calculate the average scores per participant\n\nI’ve started coding but there are some errors in my code. Help me find and fix all of them. Try to go through the code line by line and read the error messages.\n\nsuper &lt;- data_ppr %&gt;% \n  mutate(Supervisor_15 = 9-supervisor_15_R) %&gt;% \n  filter(Supervisor_7 = 1) %&gt;% \n  select(Code, starts_with(\"Super\"), -Supervisor_7, -Supervisor_15_R) \npivot_wider(cols = -Code, names_to = \"Item\", values_to = \"Response\") %&gt;% \n  group_by(Time2_Code) %&gt;% \n  summarise(Mean_Supervisor_Support = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\nHow many mistakes am I supposed to find?\n\n\n\n\n\nThere are 8 mistakes in the code.\n\n\n\n\n\n\n\n\n\nReveal solution\n\n\n\n\n\nDid you spot all 8 mistakes? Let’s go through them line by line.\n\nsuper &lt;- data_prp %&gt;% # spelling mistake in data object\n  mutate(Supervisor_15 = 8-Supervisor_15_R) %&gt;% # semantic error: 8 minus response for a 7-point scale and supervisor_15_R needs a capital S\n  filter(Supervisor_7 == 1) %&gt;% # needs a Boolean expression == instead of =\n  select(Code, starts_with(\"Super\"), -Supervisor_7, -Supervisor_15_R) %&gt;% # no pipe at the end, the rest is actually legit\n  pivot_longer(cols = -Code, names_to = \"Item\", values_to = \"Response\") %&gt;% # pivot_longer instead of pivot_wider\n  group_by(Code) %&gt;% # Code rather than Time2_Code - the reduced dataset does not contain Time2_Code\n  summarise(Mean_Supervisor_Support = mean(Response, na.rm = TRUE)) %&gt;% # Score_corrected doesn't exist; needs to be Response\n  ungroup()\n\n\nNote that the semantic error in line 2 will not give you an error message.\nWere you thrown off by the starts_with(\"Super\") expression in line 4? starts_with() and ends_with() are great alternatives to selecting columns via : But, using select(Code, Supervisor_1:Supervisor_6, Supervisor_8:Supervisor_14) would have given us the same result. [I admit, that one was a bit mean]"
  },
  {
    "objectID": "03-wrangling2.html#activity-6-join-everything-together-with-_join",
    "href": "03-wrangling2.html#activity-6-join-everything-together-with-_join",
    "title": "3  Data wrangling II",
    "section": "\n3.6 Activity 6: Join everything together with ???_join()\n",
    "text": "3.6 Activity 6: Join everything together with ???_join()\n\nTime to join all of the relevant data files together so we have a single dataframe ready for the next chapter on data visualisation. There are 4 options of joining data together, namely inner_join(), left_join(), right_join(), and full_join(). Each of these functions differs in terms of what information is retained from the two data objects being joined. Here is a quick overview:\n\n\n\n\n\n\nInfo on mutating joins\n\n\n\nYou have 4 types of join functions you could make use of. Click on the panels to know more\n\n\ninner_join()\nleft_join()\nright_join()\nfull_join()\n\n\n\ninner_join() returns only the rows where the values in the column specified in the by = statement match in both tables.\n\n\ninner_join(): gif by Garrick Aden-Buie\n\n\n\nleft_join() retains the complete first (left) table and adds values from the second (right) table that have matching values in the column specified in the by = statement. Rows in the left table with no match in the right table will have missing values (NA) in the new columns.\n\n\nleft_join(): gif by Garrick Aden-Buie\n\n\n\nright_join() retains the complete second (right) table and adds values from the first (left) table that have matching values in the column specified in the by = statement. Rows in the right table with no match in the left table will have missing values (NA) in the new columns.\n\n\nright_join(): gif by Garrick Aden-Buie\n\n\n\nfull_join() returns all rows and all columns from both tables. NA values fill unmatched rows.\n\n\nfull_join(): gif by Garrick Aden-Buie\n\n\n\n\n\n\nFrom our data_prp, we need to select demographics data and all summarised questionnaire data from time point 2. Next, we will join this with all other aggregated datasets from time point 1 which are currently stored in separate data objects in the Global Environment.\nYou have already encountered inner_join last year, but for now, we want to retain all data from all the data objects, so we will use full_join instead. Keep in mind, you can only join two data objects at a time, so there will be quite a bit of piping and joining in the upcoming code chunk.\nPS: Since I (Gaby) am a bit particular, I want my columns to be arranged meaningfully. Hence I’m using select to sort them in a better way.\n\ndata_prp_final &lt;- data_prp %&gt;% \n  select(Code:Plan_prereg, Other_OS_behav_2:Time2_Understanding_OS) %&gt;% \n  full_join(qrp_t1) %&gt;% \n  full_join(understanding_t1) %&gt;% \n  full_join(sats_t1) %&gt;% \n  full_join(super) %&gt;% \n  select(Code:Plan_prereg, Pre_reg_group, SATS28_Affect_Time1_mean, SATS28_CognitiveCompetence_Time1_mean, SATS28_Value_Time1_mean, SATS28_Difficulty_Time1_mean, QRPs_Acceptance_Time1_mean, Time1_Understanding_OS, Other_OS_behav_2:Time2_Understanding_OS, Mean_Supervisor_Support)\n\nAnd this is basically the dataset we need for Chapter 4 and Chapter 5."
  },
  {
    "objectID": "03-wrangling2.html#activity-7-knit-and-export",
    "href": "03-wrangling2.html#activity-7-knit-and-export",
    "title": "3  Data wrangling II",
    "section": "\n3.7 Activity 7: Knit and export",
    "text": "3.7 Activity 7: Knit and export\nKnit the .Rmd file to ensure everything runs as expected. Once it does, export the data object data_prp_final as a csv for use in the Chapter 4. Name it something meaningful, something like data_prp_for_ch4.csv.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwrite_csv(data_prp_final, \"data_prp_for_ch4.csv\")"
  },
  {
    "objectID": "03-wrangling2.html#pair-coding",
    "href": "03-wrangling2.html#pair-coding",
    "title": "3  Data wrangling II",
    "section": "Pair-coding",
    "text": "Pair-coding\nWe will be working with the data from Binfet et al. (2021) again - the randomised controlled trial data about therapy dog intervention. Today we want to calculate an average Loneliness score for each participant measured at time point 1 (pre-intervention) from the raw data file dog_data_raw. Currently, it looks like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRID\nL1_1\nL1_2\nL1_3\nL1_4\nL1_5\nL1_6\nL1_7\nL1_8\nL1_9\nL1_10\nL1_11\nL1_12\nL1_13\nL1_14\nL1_15\nL1_16\nL1_17\nL1_18\nL1_19\nL1_20\n\n\n\n1\n3\n3\n4\n3\n2\n3\n1\n2\n3\n4\n3\n1\n3\n1\n2\n3\n2\n3\n2\n4\n\n\n2\n3\n2\n3\n3\n4\n3\n2\n2\n4\n3\n2\n2\n1\n2\n4\n3\n3\n2\n4\n3\n\n\n3\n3\n3\n2\n3\n3\n4\n2\n3\n3\n3\n2\n2\n2\n2\n3\n3\n4\n3\n3\n3\n\n\n4\n4\n2\n2\n3\n4\n4\n1\n3\n3\n4\n2\n1\n2\n2\n4\n4\n3\n3\n4\n3\n\n\n5\n2\n3\n3\n3\n4\n3\n2\n2\n3\n2\n4\n4\n4\n3\n2\n2\n3\n4\n3\n2\n\n\n\n\n\n\nBut we want to have the data looking like this:\n\n\n\n\n\nRID\nLoneliness_pre\n\n\n\n1\n2.25\n\n\n2\n1.90\n\n\n3\n2.25\n\n\n4\n1.75\n\n\n5\n2.85\n\n\n\n\n\n\nThis task is a bit more challenging compared to last week’s lab activity since the loneliness scale contains some reverse-coded items.\nTask 1: Open the R project for the lab\nTask 2: Open your .Rmd file from last week or create a new .Rmd file\nYou could continue the .Rmd file you used last week, or create a new .Rmd. If you need some guidance, have a look at Section 1.3.\nTask 3: Load in the library and read in the data\nThe data should already be in your project folder. If you want a fresh copy, you can download the data again here: data_pair_ch1.\nWe are using the package tidyverse today, and the datafile we should read in is dog_data_raw.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(???)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"???\")\n\n\n\n\nTask 4: Calculating the mean for Loneliness_pre\n\n\n\nStep 1: select all relevant columns, such as participant ID and all the 20 items of the Loneliness questionnaire that participants completed before the intervention. Store them in an object called data_loneliness\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nLook at the codebook. Try to figure out\n\nthe variable name of the column in which the participant id is stored, and\nwhich items relate to the Loneliness scale at Stage “pre”\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\nthe participant id column is called RID\n\nThe Loneliness items at pre-intervention stage start with L1_\n\n\n\n\n\n\n\n\n\n\nStep 2: pivot the data from wide format to long format so we can reverse-score and calculate the average score more easily (in step 3)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\npivot_\nWe also need 3 arguments in that function:\n\nthe columns we want to select (e.g., all the loneliness items),\nthe name of the column in which the current column headings will be stored (e.g., “Qs”), and\nthe name of the column that should store all the values (e.g., “Responses”).\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\")\n\n\n\n\n\n\n\n\n\nStep 3: Reverse-scoring\n\nIdentify the reverse-coded items of the loneliness scale and reverse-score them.\n\n\n\n\n\n\nHint\n\n\n\n\n\nWe need to figure out:\n\nwhich are the items of the loneliness scale we need to reverse-score\nwhat is the measuring scale of loneliness so we can determine the new values\nwhich function to use to create a new column that has the corrected scores in it\nwhich one of the case_ functions will get us there\n\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\nThe items to be reverse-coded items can be found in the codebook: L1_1, L1_5, L1_6, L1_9, L1_10, L1_15, L1_16, L1_19, L1_20\nthe loneliness scale ranges from 1 to 4, so we need to replace 1 with 4, 2 with 3, 3 with 2, and 4 with 1\nthe function to create a new column mutate()\n\nit’s a conditional statement rather than “just” replacing values, hence we need case_when()\n\n\n\n  mutate(Score_corrected = case_when(\n    ??? ~ ???,\n    .default = ???\n    ))\n\n\n\n\n\n\n\n\n\nStep 4: calculate the average Loneliness score per participant. To match with the table above, we want to call this column Loneliness_pre\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ngrouping and summarising\n\n\n\n\n\n\nMore concrete hint\n\n\n\n\n\n\n  group_by(???) %&gt;% \n  summarise(Loneliness_pre = ???(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# loading tidyverse into the library\nlibrary(tidyverse)\n\n# reading in `dog_data_raw.csv`\ndog_data_raw &lt;- read_csv(\"dog_data_raw.csv\")\n\n# Task 4: Tidying \nloneliness_tidy &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"L1\")) %&gt;% # select(RID, L1_1:L1_20) also works\n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Qs\", values_to = \"Response\") %&gt;% \n  # Step 3\n  mutate(Score_corrected = case_when(\n    Qs %in% c(\"L1_1\", \"L1_5\", \"L1_6\", \"L1_9\", \"L1_10\", \"L1_15\", \"L1_16\", \"L1_19\", \"L1_20\") ~ 5-Response,\n    .default = Response\n    )) %&gt;% \n  # Step 4\n  group_by(RID) %&gt;% \n  summarise(Loneliness_pre = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#test-your-knowledge-and-challenge-yourself",
    "href": "03-wrangling2.html#test-your-knowledge-and-challenge-yourself",
    "title": "3  Data wrangling II",
    "section": "Test your knowledge and challenge yourself",
    "text": "Test your knowledge and challenge yourself\nKnowledge check\ninclude a few mcq items here\nChallenge Yourself\nIf you want to challenge yourself and apply the skills from chapter 3 further, you could wrangle the data from dog_data_raw for one of the other questionnaires. There are plenty of options to choose from:\n\n\n\n\n\n\nDifficulty level: easy\n\n\n\n\n\n\nrecode column Live_Pets so the values read yes and no rather than 1 and 2\nrecode Year_of_Study so they have the labels from the codebook rather than the numbers\nreverse-code the Homesickness scale for _pre and _post\n\nrenaming the columns of the other one-item scales as Stress_pre, Stress_post, Engagement_pre and Engagement_post\n\n\nAny of these tasks should be doable in one step. No need to select or pivot anything. You could just modify dog_data_raw.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nFor the recoding tasks, you need to work out which function to use to recode one value as another - just plain replacing, no conditional statements\nThe reverse-coding might sound daunting to do in one step, but it is only a single value that needs to be recoded. Take some inspiration from Activity 5 (error mode).\nFor the renaming tasks, check how you would change column names without reducing the number of columns overall\n\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - easy\n\n\n\n\n\n\n## Live_Pets\ndog_data_raw &lt;- dog_data_raw %&gt;%\n  mutate(Live_Pets = case_match(Live_Pets,\n                                1 ~ \"yes\",\n                                2 ~ \"no\"))\n\n\n## Year of Study\ndog_data_raw &lt;- dog_data_raw %&gt;%\n  mutate(Year_of_Study = case_match(Year_of_Study,\n                                    1 ~ \"First\",\n                                    2 ~ \"Second\",\n                                    3 ~ \"Third\",\n                                    4 ~ \"Fourth\",\n                                    5 ~ \"Fifth or above\"))\n\n\n## Reverse-coding of homesickness pre and post. It's a 5-point scale, hence you'd calculate 6-the original response column\ndog_data_raw &lt;- dog_data_raw %&gt;% \n  mutate(Homesick_pre = 6-HO1_1,\n         Homesick_post = 6-HO2_1)\n\n\n## Renaming of Stress and Engagement\ndog_data_raw &lt;- dog_data_raw %&gt;% \n  rename(Stress_pre = S1_1, Stress_post = S2_1, Engagement_pre = HO1_2, Engagement_post = HO2_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty level: medium\n\n\n\n\n\n\nreverse-code the Social connectedness scale (pre-intervention) and compute a mean score per participant\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis task would take 4 steps to complete. These are the exact same steps we applied to Loneliness_pre in the lab activity. You would just need to figure out which items are related to the Social connectedness scale (pre-intervention) and which ones of those are reverse-coded. The codebook has all the answers.\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - medium\n\n\n\n\n\n\n## SCS pre\nscs_pre &lt;- dog_data_raw %&gt;% \n  select(RID, starts_with(\"SC1\")) %&gt;% \n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  mutate(Score_corrected = case_when(\n    Names %in% c(\"SC1_3\", \"SC1_6\", \"SC1_7\", \"SC1_9\", \"SC1_11\", \"SC1_13\", \"SC1_15\", \"SC1_17\", \"SC1_18\", \"SC1_20\") ~ 7-Response,\n    .default = Response\n    )) %&gt;% \n  group_by(RID) %&gt;% \n  summarise(SCS_pre = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty level: hard\n\n\n\n\n\n\nreverse-code the Loneliness scale (post-intervention) and compute a mean score per participant\nreverse-code the Social connectedness scale (post-intervention) and compute a mean score per participant\n\nBoth activities are similar to Activity 3 from the individual walkthrough and would take about 5 steps to complete. Start by mapping out the steps.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\nStep 1: Select all relevant columns, such as participant ID and all the items that belong to the questionnaire that participants completed after the intervention\n\nStep 2: Pivot the data from wide format to long format so we can reverse-score and calculate the average score more easily\n\nStep 3: Recode the initial responses so that the new column has numbers instead of labels\n\nStep 4: Reverse-score the items that are labelled as “Reverse” in the codebook and then reverse-score them\n\nStep 5: Group by and summarise to calculate the mean Score\n\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - hard\n\n\n\n\n\n\n## loneliness post\nlonely_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"L2\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  # Step 3\n  mutate(Score = case_match(Response,\n                            \"never\" ~ 1,\n                            \"rarely\" ~ 2,\n                            \"sometimes\" ~ 3,\n                            \"often\" ~ 4,\n                            .default = NA\n  ),\n  # Step 4 - we are still in the same mutate function (count the brackets)\n        Score_corrected = case_when(\n          Names %in% c(\"L2_1\", \"L2_5\", \"L2_6\", \"L2_9\", \"L2_10\", \"L2_15\", \"L2_16\", \"L2_19\", \"L2_20\") ~ 5-Score,\n          .default = Score\n  )) %&gt;% \n  # Step 5\n  group_by(RID) %&gt;% \n  summarise(Loneliness_post = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n## SCS post\nscs_post &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"SC2\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Names\", values_to = \"Response\") %&gt;% \n  # Step 3\n  mutate(Response = case_match(Response,\n                               \"strongly disagree\" ~ \"1\",\n                               \"strongly agree\" ~ \"6\",\n                               .default = Response),\n         Response = parse_number(Response),\n  # Step 4 - we are still in the same mutate function (count the brackets)\n         Score_corrected = case_when(\n           Names %in% c(\"SC2_3\", \"SC2_6\", \"SC2_7\", \"SC2_9\", \"SC2_11\", \"SC2_13\", \"SC2_15\", \"SC2_17\", \"SC2_18\", \"SC2_20\") ~ 7-Response,\n           .default = Response\n         )) %&gt;% \n  # Step 5\n  group_by(RID) %&gt;% \n  summarise(SCS_post = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifficulty level: extra hard\n\n\n\n\n\n\nPANAS: positive and negative affect of pre- and post-intervention in a single pipe rather than in 4 different data objects (see last week’s)\n\nThis task would take about 7 steps to get it from\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRID\nPN1_1\nPN1_2\nPN1_3\nPN1_4\nPN1_5\nPN1_6\nPN1_7\nPN1_8\nPN1_9\nPN1_10\nPN2_1\nPN2_2\nPN2_3\nPN2_4\nPN2_5\nPN2_6\nPN2_7\nPN2_8\nPN2_9\nPN2_10\n\n\n\n1\n1\n1\n1\n1\n4\n1\n4\n3\n1\n4\n2\n1\n3\n1\n4\n1\n4\n4\n1\n4\n\n\n2\n1\n2\n3\n2\n1\n3\n3\n4\n1\n4\n1\n1\n2\n1\n3\n1\n3\n4\n1\n4\n\n\n3\n1\n1\n3\n1\n2\n4\n4\n3\n1\n2\n2\n2\n3\n1\n3\n2\n4\n3\n1\n2\n\n\n4\n1\n1\n5\n1\n4\n3\n5\n5\n3\n2\n1\n1\n5\n1\n4\n3\n4\n4\n2\n2\n\n\n5\n2\n3\n5\n2\n3\n2\n3\n4\n2\n2\n1\n2\n5\n2\n3\n2\n4\n5\n1\n3\n\n\n\n\n\n\nto\n\n\n\n\n\nRID\nStage\nPANAS_NA\nPANAS_PA\n\n\n\n1\npost\n1.2\n3.8\n\n\n1\npre\n1.0\n3.2\n\n\n2\npost\n1.0\n3.2\n\n\n2\npre\n1.8\n3.0\n\n\n3\npost\n1.6\n3.0\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nStart by mapping out the steps\n\n\nStep 1: select all relevant columns, such as participant ID and all the items that belong to PANAs scale (pos, neg, pre, and post)\n\nStep 2: pivot the data from wide format to long format. You want to do that for ALL columns that are not the participant id. The data object should have 3 columns and 5680 observations, i.e. each participant has 20 rows.\n\nStep 3: All of the items will have the structure PN1_1. Use separate to split the information across 2 columns. First column has information about the Stage, second column should turn into an Item_number and it should convert into a numeric column in the process to save you typing quotation marks in Step 5.\n\n\nStep 4: recode the Stage column you just created so that everything that starts with PN1 relates to “pre” and PN2 as post.\n\nStep 5: identify the subscales positive affect (PA) and negative affect (NA) by item number and recode them. This requires a conditional statement.\n\nStep 6: group by and summarise to calculate the mean Score\n\nStep 7: pivot, so that you have the 2 PANAS subscales presented in separate columns (see table above). You might need an extra step if the columns aren’t labelled exactly as shown in the table above.\n\n\n\n\n\n\n\n\n\n\nSolution for Challenge yourself - extra hard\n\n\n\n\n\n\nPANAS &lt;- dog_data_raw %&gt;% \n  # Step 1\n  select(RID, starts_with(\"PN\")) %&gt;% \n  # Step 2\n  pivot_longer(cols = -RID, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  separate(Items, into = c(\"Stage\", \"Item_number\"), sep = \"_\", convert = TRUE) %&gt;% \n  # Step 4 recode Stage\n  mutate(Stage = case_match(Stage,\n                            \"PN1\" ~ \"pre\",\n                            \"PN2\" ~ \"post\")) %&gt;% \n  # Step 5 identify subscales by item number\n  mutate(Subscales = case_when(\n    Item_number %in% c(3, 5, 7, 8, 10) ~ \"PANAS_PA\",\n    .default = \"PANAS_NA\"\n  )) %&gt;% \n  # Step 6 \n  group_by(RID, Stage, Subscales) %&gt;% \n  summarise(Score = mean(Scores)) %&gt;% \n  ungroup() %&gt;% \n  # Step 7 - to make the data look like the data in `dog_data_clean_long.csv`\n  pivot_wider(names_from = Subscales, values_from = Score)"
  },
  {
    "objectID": "04-dataviz.html#intended-learning-outcomes",
    "href": "04-dataviz.html#intended-learning-outcomes",
    "title": "4  Data viz I",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\nexplain the layered grammar of graphics\nchoose an appropriate plot for categorical variables\ncreate a basic version of an appropriate plot\napply extra layers to change the appearance of the plot\n\nIt is time to think about which plot is the appropriate plot for your data. Different types of variables require different types of plots, so this comes back to how many variables are you aiming to plot and what kind of data type are they. Today, we will focus on plots for categorical data. Next week, we’ll cover plots for continuous variables and discover which plots are appropriate if you have a mix of continuous and categorical variables."
  },
  {
    "objectID": "04-dataviz.html#individual-walkthrough",
    "href": "04-dataviz.html#individual-walkthrough",
    "title": "4  Data viz I",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "04-dataviz.html#building-plots",
    "href": "04-dataviz.html#building-plots",
    "title": "4  Data viz I",
    "section": "\n4.1 Building plots",
    "text": "4.1 Building plots\nWe are using the package ggplot2 to create data visualisations. It’s part of the tidyverse package. Actually, most people call th package ggplot but it’s official name is ggplot2.\n\n\nggplot2 uses a layered grammar of graphics, in which plots are built up in a series of layers. You would start with a base layer (opening ggplot), adding data and aesthetics, and selecting the geometries for plot.\nThese first 3 layers will give you the most simple version of a complete plot, but you could add other layers to make the plots more accessible (and pretty) by using scales, facets, coordinates, labels and themes.\n\n\n\n\ngg layers (Presentation by Ryan Safner)\n\n\n\nTo give you a brief overview of the layering system, let’s use the package palmerpenguins (https://allisonhorst.github.io/palmerpenguins/). It contains data about bill length and depth, flipper length, and body mass, etc.\n\nhead(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nLet’s build a basic scatterplot to show the relationship between flipper_length and body_mass. We will customise plots further later on in the individual plots. This is just a quick overview of the different layers.\n\nLayer 1 creates a plot base to built up upon.\nLayer 2 adds the data and some aesthetics\n\ndata is first argument\naesthetics are added via the mapping argument. There you define your variables to be added (such as x, or x and y) and allows you specify overall properties like the colour of grouping variables etc.\n\n\nLayer 3 adds the geometries or geom_? for short. This tells ggplot in which style we want to plot the data points. Remember to add these layers with a + rather than a pipe %&gt;%. You can add multiple geoms if you wish, e.g., building a violin-boxplot\nLayer 4 adds the scale_? functions which can help you customise the aesthetics, such as changing colour. You can do much more with scales, but we’ll get to that later.\nLayer 5 introduces facets, such as facet_wrap() which allows you to add another dimension to the data output by showing the relationship you are interested in for each level of a categorical variable.\nLayer 6 - coordinates: coord_cartesian() controls the limits for the x- and y-axes (arguments xlim and ylim). Changing those allows you to zoom in or out of your plot.\nLayer 7 helps you to modify axes labels.\nLayer 8 controls the general style of a ggplot (e.g., background colour, size of text, borders, etc.). R comes with a few pre-defined ones (like theme_classic, theme_bw, theme_minimal, theme_light).\n\nClick on the tabs below to see how each layer contributes to refining the plot.\n\n\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\n\n\n\n\nggplot()\n\n\n\n\n\n\n\nWe don’t see much here. It’s basically an empty plot layer.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm))\n\n\n\n\n\n\n\nYou won’t see any data points yet, because we haven’t specified how we want to display the data points. But we mapped in the aesthetics, that we want to plot variable body mass on the x-axis and flipper length on the y-axis. This also adds the axes titles and the values and break points of the axes.\n\n\n\n\n\n\nTip\n\n\n\nYou won’t need to add data = or mapping = if you keep those arguments in exactly that order. Likewise, the first column name you enter within the aes() function will always be interpreted as x, and the second as y, so you could omit them if you wish.\n\nggplot(penguins, aes(body_mass_g, flipper_length_mm))\n\nwill give you the same output as the code above.\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHere we are telling ggplot that we want a scatterplot added. There is a warning displayed showing that rows were removed because of missing values.\nThe argument colour adds colour to the points according to a grouping variable (in this case sex). If you want all of the points to be black (i.e. only represent 2 rather than 3 dimensions of the data), leave the colour argument out.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  # changes colour palette\n  scale_colour_brewer(palette = \"Dark2\") + \n  # add breaks from 2500 to 6500 in increasing steps of 500\n  scale_x_continuous(breaks = seq(from = 2500, to = 6500, by = 500)) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nThe scale_? functions allow us to change the colour palette of the plot or the axes breaks etc. You could change the name of the axis in scale_x_continuous() as well or leave it for Layer 7.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  # split main plot up into different subplots by species \n  facet_wrap(~ species) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHere we are faceting this plot out for the individual species.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  # limits the range of the y axis\n  coord_cartesian(ylim = c(0, 250)) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nChanging the limits of the y axis to zoom in or out of the plot. If you wanted to the same for the x axis, you would add an argument xlim to the coord_cartesian() function.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  labs(x = \"Body Mass (in g)\", # labels the x axis\n       y = \"Flipper length (in mm)\", # labels the y axis\n       colour = \"Sex\") # labels the grouping variable in the legend\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou can change the axes labels via the labs() function or include that step when modifying the scales (i.e. in the scale_x_continuous() function).\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  labs(x = \"Body Mass (in g)\", \n       y = \"Flipper length (in mm)\",\n       colour = \"Sex\") +\n  # add a theme\n  theme_classic()\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\ntheme_classic() is applied to change the overall appearance of the plot.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to stick to the first 3 layers to get your base plot. Everything else is optional meaning you don’t have to use all 8 layers in a plot. And layers 4-8 can be added in a random order whereas layers 1-3 are fixed."
  },
  {
    "objectID": "04-dataviz.html#activity-1-set-up-and-data-for-today",
    "href": "04-dataviz.html#activity-1-set-up-and-data-for-today",
    "title": "4  Data viz I",
    "section": "\n4.2 Activity 1: Set-up and data for today",
    "text": "4.2 Activity 1: Set-up and data for today\n\nWe are still working with the data by Pownall et al. (2023), so open your project.\nHowever, we could do with a fresh Rmd: Create a new .Rmd file* and save it to your project folder. Name it something meaningful (e.g., “chapter_04.Rmd”, “04_data_viz.Rmd”). See Section 1.3 if you need some guidance. Delete everything below line 12 (keep the set-up code chunk)\nWe aggregated the data in Chapter 2 and Chapter 3. If you want a fresh copy, download the data here: data_prp_for_ch4.csv. Make sure to place the csv file in the project folder.\nIf you need a reminder about the data and variables, have a look at the codebook and/or Section 1.4."
  },
  {
    "objectID": "04-dataviz.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "href": "04-dataviz.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "title": "4  Data viz I",
    "section": "\n4.3 Activity 2: Load in libraries, read in data, and adjust data types",
    "text": "4.3 Activity 2: Load in libraries, read in data, and adjust data types\nWe need the package tidyverse today, and the data data_prp_for_ch4.csv.\n\n## packages \n???\n\n## data\ndata_prp_viz &lt;- read_csv(???)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp_viz &lt;- read_csv(\"data_prp_for_ch4.csv\")\n\n\n\n\nAs we said in Section 1.6, it is always recommended to glimpse at the data to see how many variables and observations there are in the dataset and what kind of data type they are.\n\n\n\n\n\n\nglimpse output\n\n\n\n\n\n\nglimpse(data_prp_viz)\n\nRows: 89\nColumns: 28\n$ Code                                  &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", …\n$ Gender                                &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,…\n$ Age                                   &lt;dbl&gt; 22, 20, 22, 26, 22, 20, 21, 21, …\n$ Ethnicity                             &lt;chr&gt; \"White European\", \"White British…\n$ Secondyeargrade                       &lt;dbl&gt; 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1,…\n$ Opptional_mod                         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ Opptional_mod_1_TEXT                  &lt;chr&gt; \"Research methods in first year\"…\n$ Research_exp                          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Research_exp_1_TEXT                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Plan_prereg                           &lt;dbl&gt; 1, 3, 1, 2, 1, 1, 3, 3, 2, 2, 2,…\n$ Pre_reg_group                         &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,…\n$ SATS28_Affect_Time1_mean              &lt;dbl&gt; 4.000000, 3.833333, 5.000000, 5.…\n$ SATS28_CognitiveCompetence_Time1_mean &lt;dbl&gt; 5.166667, 5.166667, 5.666667, 4.…\n$ SATS28_Value_Time1_mean               &lt;dbl&gt; 6.000000, 6.666667, 5.222222, 5.…\n$ SATS28_Difficulty_Time1_mean          &lt;dbl&gt; 3.571429, 2.428571, 3.571429, 3.…\n$ QRPs_Acceptance_Time1_mean            &lt;dbl&gt; 5.909091, 6.090909, 6.545455, 5.…\n$ Time1_Understanding_OS                &lt;dbl&gt; 5.500000, 3.166667, 4.500000, 3.…\n$ Other_OS_behav_2                      &lt;dbl&gt; 1, NA, NA, NA, 1, NA, NA, 1, NA,…\n$ Other_OS_behav_4                      &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, N…\n$ Other_OS_behav_5                      &lt;dbl&gt; NA, NA, NA, NA, 1, 1, NA, NA, NA…\n$ Closely_follow                        &lt;dbl&gt; 2, 2, 2, NA, 3, 3, 3, NA, NA, 2,…\n$ SATS28_Affect_Time2_mean              &lt;dbl&gt; 3.500000, 3.166667, 4.833333, 4.…\n$ SATS28_CognitiveCompetence_Time2_mean &lt;dbl&gt; 4.166667, 4.666667, 6.166667, 5.…\n$ SATS28_Value_Time2_mean               &lt;dbl&gt; 3.000000, 6.222222, 6.000000, 4.…\n$ SATS28_Difficulty_Time2_mean          &lt;dbl&gt; 2.857143, 2.857143, 4.000000, 2.…\n$ QRPs_Acceptance_Time2_mean            &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.…\n$ Time2_Understanding_OS                &lt;dbl&gt; 5.583333, 3.333333, 5.416667, 4.…\n$ Mean_Supervisor_Support               &lt;dbl&gt; 5.230769, 6.285714, 6.857143, 2.…\n\n\n\n\n\nWe can see that some of the categorical data in data_prp_viz was read in as numeric variables which makes them continuous. This is going to haunt us big time when building the plots. We would be better off addressing these changes in the dataset before we start plotting (and potentially getting frustrated with R and data viz in general)\nLet’s turn a few of the categorical variables into factors.\n\ndata_prp_viz &lt;- data_prp_viz %&gt;% \n  mutate(Gender = factor(Gender,\n                         levels = c(2, 1, 3),\n                         labels = c(\"females\", \"males\", \"non-binary\")),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         Plan_prereg = factor(Plan_prereg,\n                              levels = c(1, 3, 2),\n                              labels = c(\"Yes\", \"Unsure\", \"No\")),\n         Closely_follow = factor(Closely_follow,\n                                 levels = c(2, 3),\n                                 labels = c(\"Followed it somewhat\", \"Followed it exactly\")),\n         Research_exp = factor(Research_exp),\n         Pre_reg_group = factor(Pre_reg_group))"
  },
  {
    "objectID": "04-dataviz.html#activity-3-barchart-geom_bar",
    "href": "04-dataviz.html#activity-3-barchart-geom_bar",
    "title": "4  Data viz I",
    "section": "\n4.4 Activity 3: Barchart (geom_bar())",
    "text": "4.4 Activity 3: Barchart (geom_bar())\nA barchart is the way to go when you have a single categorical variable you want to plot.\nLet’s say we want to count some demographics. To keep it simple, we want to show gender counts. We would use a barplot for it. This is done with geom_bar() in your third layer, and because the counting is done in the background, the aes only requires an x value (i.e. the name of your variable).\n\nggplot(data_prp_viz, aes(x = Gender)) +\n  geom_bar() \n\n\n\nFigure 4.1: Default barchart\n\n\n\nThis is the base plot done. You can customise it by adding different layers. For example, the labels aren’t super clear or it could do with a dash of colour. Click on the tabs below to see examples of what could be added and practice on your base plot in your own Rmd.\n\n\nColour\nAxes labels & margins\nLegend\nThemes\n\n\n\nWe can change the colour by adding a fill argument in the aes(). If we want to modify these colours further, we would add a scale_fill_? argument. If you have specific colours in mind, you would use scale_fill_manual() or if you want to stick with pre-defined ones, like viridis, use scale_fill_viridis_d()\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  # customise colour\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\nThe x-axis label is fine, but the categories need to be relabeled. You can achieve that with the scale_x_discrete() function and the argument labels =. But make sure you order them in the order of the labels in the dataframe.\nThere is also this gap between bottom of the chart and the bars which seems a bit weird. We can remove that with an expansion() function.\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  # changing group labels on the breaks of the x axis\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) + \n  scale_y_continuous(\n    # changing name of the y axis\n    name = \"Count\",\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05))\n  )\n\n\n\n\n\n\n\n\n\nThe legend does not add any information because the labels are already provided on the x axis. We can remove the legend by adding the argument guide = \"none\" into the scale_fill function.\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d(\n    # remove the legend\n    guide = \"none\") +\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) +\n  scale_y_continuous(\n    name = \"Count\",\n    expand = expansion(mult = c(0, 0.05))\n  )\n\n\n\n\n\n\n\n\n\nLet’s experiment with the themes. For this plot we have chosen theme_minimal()\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d(\n    guide = \"none\") +\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) +\n  scale_y_continuous(\n    name = \"Count\",\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  # pick a theme\n  theme_minimal()"
  },
  {
    "objectID": "04-dataviz.html#activity-4-column-plot-geom_col",
    "href": "04-dataviz.html#activity-4-column-plot-geom_col",
    "title": "4  Data viz I",
    "section": "\n4.5 Activity 4: Column plot (geom_col())",
    "text": "4.5 Activity 4: Column plot (geom_col())\nIf someone had already summarised those counts for you, you would not be able to use geom_bar(). In that case, you would switch to geom_col().\n\ngender_count &lt;- data_prp_viz %&gt;% \n  count(Gender)\n\ngender_count\n\n\n\n\nGender\nn\n\n\n\nfemales\n69\n\n\nmales\n17\n\n\nnon-binary\n3\n\n\n\n\n\n\nThe mapping for geom_col() requires both an x and a y aesthetics. In our example, x would be our categorical variable (e.g., Gender), and y would be the column name that stored the values (n). Note how the base version has now n as an axis title (instead of count).\n\nggplot(gender_count, aes(x = Gender, y = n, fill = Gender)) +\n  geom_col()\n\n\n\nFigure 4.2: Column plot with different coloured bars\n\n\n\n\n\n\n\n\n\nYour Turn: Make the column plot pretty\n\n\n\nThe other layers to change the colour scheme, axes labels and margins, removing the legend and altering the theme require exactly the same functions as with the boxplot above. Test yourself to see if you could…\n\n\nchange the colour scheme (e.g., viridis or any other colour palettes)\n\nremove the legend\n\nchange the title of the x and y axes\n\nmake the bars start directly on the x axis\n\nadd a theme of your linking\n\n\n\n\n\n\n\nPossible solution code for the column plot (with a different colour palette and a different theme)\n\n\n\n\n\n\nggplot(gender_count, aes(x = Gender, y = n, fill = Gender)) +\n  geom_col() +\n  # replaced vidiris with the brewer palette\n  scale_fill_brewer(\n    palette = \"Set1\", # try \"Set2\" or \"Dark2\" for some variety\n    guide = \"none\") + # legend removed\n  # labels of the categories changed\n  scale_x_discrete(labels = c(\"Male\", \"Female\", \"Non-Binary\")) + \n  scale_y_continuous(\n    # change y axis label\n    name = \"Count\",\n    # starts bars on x axis without any gaps but leaves some space at the top (this time 10%)\n    expand = expansion(mult = c(0, 0.1)) \n  ) +\n  # different theme\n  theme_light()"
  },
  {
    "objectID": "04-dataviz.html#activity-5-stacked-percent-stacked-and-grouped-barchart",
    "href": "04-dataviz.html#activity-5-stacked-percent-stacked-and-grouped-barchart",
    "title": "4  Data viz I",
    "section": "\n4.6 Activity 5: Stacked, percent stacked, and grouped barchart",
    "text": "4.6 Activity 5: Stacked, percent stacked, and grouped barchart\nIf we are dealing with two categorical variables, we have the three options to display stacked barcharts - “normal” Stacked barchart (default option), a Percent stacked barchart, or a Grouped barchart.\nFor this activity, we can look at the variable Plan_prereg measuring whether students planned to pre-register their undergraduate dissertation at time point 1, and Pre_reg_group whether they actually ended up doing a pre-registration for their undergraduate dissertations.\nOne way to display that data is by creating a Stacked barchart (default option) or a Percent stacked barchart. For both options, the subgroups are displayed on top of each other. To show the two plots next to each other for better comparison, we moved the legend to the bottom of the chart.\n\n## Stacked barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar() + # no position argument added\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n## Percent stacked barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n\n\n\n\nFigure 4.3: Stacked barchart (left), and Percent stacked barchart (right)\n\n\n\nIn the stacked barchart (Figure 4.3, left plot), you are able to plot participant numbers. Here we can see that the highest student number were unsure whether they wanted to pre-register their dissertation or not, followed closely by participants who answered yes. We can also see that the number of people who did not end up with a pre-registered dissertation (blue category) is the same for students who had planned to pre-register and those who did not want to pre-register. However, because No has quite a lower number than the other two categories, we are having a tough time seeing whether the ratio is the same across all 3 groups.\nIf we wanted to show that, a Percent stacked barchart (Figure 4.3, right plot) would make a lot more sense. Now we would see that approximately 80% of the students who wanted to pre-register their dissertations, 50% of the students who were initially unsure, and only 33% of the students who had no pre-registration plans ended up with a ended up with a pre-registered dissertation. BUT! We would lose the information about the raw values in the sample.\nIt’s all a trade-off and the plot you choose depends on which “story” of the data you want to tell.\n\n\n\n\n\n\nNote\n\n\n\nThe position argument position = \"stack\" is the default. Adding this argument to the code for the left plot in Figure 4.3 would produce the same plot a leaving the argument out.\n\n\nThe other option is a Grouped barchart which displays the bars next to each other. We would achieve that by changing the position argument to “dogde”. You can see the default version of the plot in the Figure 4.4 on the left, and one with more layers on the right.\nInstead of using an existing colour palette, we changed the colours manually using hex codes. These are some of the colours Gaby used in her PhD thesis, but you can either …\n\ncreate your own colour hex codes; check out this website OR\nuse a pre-defined colour name like “green” or “purple instead. See a full list here\n\n\nFeel free to explore.\nSince the legend title for the second plot is a bit long, we displayed the legend content across 2 rows by adding the layer guides(fill = guide_legend(nrow = 2)) at the end.\n\n## Default grouped barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"dodge\") + # add position argument here\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n## Prettier grouped barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"dodge\") + # add position argument here\n  # changing labels for x, y, and fill category - alternative method\n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registered dissertation\") +\n  # manual colour change for values\n  scale_fill_manual(values = c('#648FFF', '#DC267F'),\n                    labels = c(\"Yes\", \"No\")) +\n  scale_y_continuous(\n    # remove the space below the bars, but keep a tiny bit (5%) above\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  # pick a theme\n  theme_classic() + \n  # need to move this following line to the end otherwise the `theme_*` overrides it\n  theme(legend.position = \"bottom\") + \n  # display across 2 rows\n  guides(fill = guide_legend(nrow = 2))\n\n\n\n\n\nFigure 4.4: Default grouped barchart (left) and one with a few more layers added (right)\n\n\n\n\n\n\n\n\n\nSpecial case: Categorical variables with missing values\n\n\n\n\n\nIf we had chosen a different categorical variable containing missing values, such as Closely_follow then our plots would have included those missing values by default. The code for changing the colour of the missing values columns would need to be specified slightly differently using an na.value = argument within the scale_fill function. Here is an example of a grouped barchart.\n\n# default grouped barchart with missing values\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\n## Prettier grouped barchart with missing values\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registration followed\") +\n  # manual colour change for values of the factor and the NA responses\n  scale_fill_manual(values = c('#648FFF', '#DC267F'), na.value = '#FFB000') +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_classic() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\n\n\n\n\nFigure 4.5: Default grouped barchart (left) and one with a few more layers added (right) for a variable with missing values\n\n\n\nIf you did not want those missing values to appear in the plot, you would have to do some data wrangling first to remove them. The function for that is drop_na(). Here we used drop_na() on Closely_follow only.\n\n# remove NA\nprereg_plan_follow &lt;- data_prp_viz %&gt;% \n  select(Code, Plan_prereg, Closely_follow) %&gt;% \n  drop_na(Closely_follow)\n\n\n\n\n\n\n\ncheck NAs have been removed\n\n\n\n\n\n\n# check NA have been removed\nprereg_plan_follow %&gt;% \n  distinct(Plan_prereg, Closely_follow) %&gt;% \n  arrange(Plan_prereg, Closely_follow)\n\n\n\n\nPlan_prereg\nClosely_follow\n\n\n\nYes\nFollowed it somewhat\n\n\nYes\nFollowed it exactly\n\n\nUnsure\nFollowed it somewhat\n\n\nUnsure\nFollowed it exactly\n\n\nNo\nFollowed it somewhat\n\n\nNo\nFollowed it exactly\n\n\n\n\n\n\n\n\n\nBut keep in mind that it could misrepresent the data, e.g., giving a wrong impression about proportions. As a comparison…\n\n# with NA\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") + # move legend to the bottom\n  guides(fill = guide_legend(nrow = 2)) # display across 2 rows\n\n# without NA\nggplot(prereg_plan_follow, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") + # move legend to the bottom\n  guides(fill = guide_legend(nrow = 2)) # display across 2 rows\n\n\n\n\n\nFigure 4.6: Percent stacked barchart with (left) and without missing values (right)"
  },
  {
    "objectID": "04-dataviz.html#activity-6-save-your-plots",
    "href": "04-dataviz.html#activity-6-save-your-plots",
    "title": "4  Data viz I",
    "section": "\n4.7 Activity 6: Save your plots",
    "text": "4.7 Activity 6: Save your plots\nYou can save your figures with the function ggsave(). It will save to your project folder.\nThere are two ways you can use ggsave(). If you don’t tell ggsave() which plot you want to save, by default it will save the last plot you created. Our last plot was the one without NA from the special case scenario (Figure 4.6). However, if you did not follow along with the special case scenario, your last plot will be grouped bar chart on the right from Figure 4.4.\n\nggsave(filename = \"last_plot.png\")\n\n\n\n\n\n\n\nOur last plot saved\n\n\n\n\n\n\n\n\n\nThe second option is to save the plot as an object and refer to the object within ggsave(). As an example, let’s save the grouped barchart that contained missing values (Figure 4.4) as an object called grouped_bar.\n\ngrouped_bar &lt;- ggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registration followed\") +\n  # manual colour change for values of the factor and the NA responses\n  scale_fill_manual(values = c('#648FFF', '#DC267F'), na.value = '#FFB000') +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_classic() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\nThen you run the line\n\nggsave(filename = \"grouped_bar.png\", \n       plot = grouped_bar)\n\n\n\nSaving 7 x 5 in image\n\n\nFilename is the name you want your png to be called, plot refers to the object name.\n\n\n\n\n\n\nOur saved grouped_bar.png would look like this:\n\n\n\n\n\n\n\n\n\nThat is with default settings. If you like it, keep it, but if you think it looks a bit “off”, you can specify the width, the height, and the units (e.g., “cm”, “mm”, “in”, “px” are possible). You might need to play about with the dimension before it feels right.\n\nggsave(filename = \"grouped_bar2.png\", \n       plot = grouped_bar, \n       width = 16, height = 9, units = \"cm\")\n\n\n\n\n\n\n\ngrouped_bar.png with different dimensions"
  },
  {
    "objectID": "04-dataviz.html#pair-coding",
    "href": "04-dataviz.html#pair-coding",
    "title": "4  Data viz I",
    "section": "Pair-coding",
    "text": "Pair-coding\nProvide a barchart, a violin-boxplot, and a scatterplot from the loneliness data and the students have to try and recreate one of those in the lab. If they are overly fast, they can do the other 2."
  },
  {
    "objectID": "04-dataviz.html#test-your-knowledge",
    "href": "04-dataviz.html#test-your-knowledge",
    "title": "4  Data viz I",
    "section": "Test your knowledge",
    "text": "Test your knowledge\nWhich plot would you choose for"
  },
  {
    "objectID": "05-dataviz2.html#intended-learning-outcomes",
    "href": "05-dataviz2.html#intended-learning-outcomes",
    "title": "5  Data viz II",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\nchoose an appropriate plot for continuous variables\nchoose an appropriate plot when you’ve got a mix of continuous/categorical variables\ncreate a basic version of an appropriate plot\napply extra layers to change the appearance of the plot\n\nIn this chapter, we are continuing our journey of appropriate plots. Last week, we looked at which plots are appropriate for categorical variables. Today, we’ll focus on continuous variables and which plots to choose with a mix of continuous and categorical variables."
  },
  {
    "objectID": "05-dataviz2.html#individual-walkthrough",
    "href": "05-dataviz2.html#individual-walkthrough",
    "title": "5  Data viz II",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "05-dataviz2.html#activity-1-set-up-and-data-for-today",
    "href": "05-dataviz2.html#activity-1-set-up-and-data-for-today",
    "title": "5  Data viz II",
    "section": "\n5.1 Activity 1: Set-up and data for today",
    "text": "5.1 Activity 1: Set-up and data for today\n\nWe are still working with the data by Pownall et al. (2023). Open the project.\nYou could use the same .Rmd file as last week if you want to keep all plotting in one document or create a new .Rmd to separate plots for categorical and continuous variables. Up to you.\nThe aggregated data is the same as last week. It should be in your project folder but in case it got lost, download it again and place it in your project folder: data_prp_for_ch4.csv.\nIf you need a reminder about the data and variables, have a look at the codebook and/or Section 1.4."
  },
  {
    "objectID": "05-dataviz2.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "href": "05-dataviz2.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "title": "5  Data viz II",
    "section": "\n5.2 Activity 2: Load in libraries, read in data, and adjust data types",
    "text": "5.2 Activity 2: Load in libraries, read in data, and adjust data types\nWe need the package tidyverse today, and the data data_prp_ch3.csv.\n\n## packages \n???\n\n## data\ndata_prp_viz &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp_viz &lt;- read_csv(\"data_prp_for_ch4.csv\")\n\n\n\n\nThis is the same as last week. We need to turn our categorical variables into factors to make plotting easier.\n\ndata_prp_viz &lt;- data_prp_viz %&gt;% \n  mutate(Gender = factor(Gender,\n                         levels = c(2, 1, 3),\n                         labels = c(\"females\", \"males\", \"non-binary\")),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         Plan_prereg = factor(Plan_prereg,\n                              levels = c(1, 3, 2),\n                              labels = c(\"Yes\", \"Unsure\", \"No\")),\n         Closely_follow = factor(Closely_follow,\n                                 levels = c(2, 3),\n                                 labels = c(\"Followed it somewhat\", \"Followed it exactly\")),\n         Research_exp = factor(Research_exp),\n         Pre_reg_group = factor(Pre_reg_group))\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are working within the same .Rmd file as last week, you can skip these initial steps but you have to run the code you had already placed at the start of last-week’s .Rmd file to load tidyverse into the library, read in the data, and convert some of the variables into factors."
  },
  {
    "objectID": "05-dataviz2.html#activity-3-histogram-geom_histogram",
    "href": "05-dataviz2.html#activity-3-histogram-geom_histogram",
    "title": "5  Data viz II",
    "section": "\n5.3 Activity 3: Histogram (geom_histogram())",
    "text": "5.3 Activity 3: Histogram (geom_histogram())\nIf you wanted to show the distribution of a continuous variable, you can use a histogram. As with every plot, you need at least 3 layers to create a base version of the plot. Similar to geom_bar(), geom_histogram() only requires an x variable as it does the counting “in the background”.\nA histogram splits the data into “bins” (i.e., groupings displayed in a single bar). These values are plotted along the x-axis and shows the count of how many observations are in each bin along the y-axis. It’s basically a bar chart for continuous variables.\nLet’s have a look at the age distribution in our dataset.\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nFigure 5.1: Default histogram\n\n\n\nThe default bin number is 30 (as displayed in Figure 5.1 above). Changing the number of bins (argument bins) can help to show more or less fine tuning in the data. Bigger numbers of bins means more finetuning.\nPerhaps it’s more intuitive to modify the width of each bin instead via the argument (binwidth). So for example binwidth = 1 for category age would mean 1 year of age per “age group”; binwidth = 5 would mean 5 years of age span per “age group”, etc. The plots below show modifications for both bin and binwidth.\n\n#less finetuning\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(bins = 10) \n\n# more fineturning\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1) \n\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range (`stat_bin()`).\nRemoved 2 rows containing non-finite outside the scale range (`stat_bin()`).\n\n\n\n\nFigure 5.2: Bins vs binwidth arguments\n\n\n\nThe warning message telling us 2 row of data were removed due to containing non-finite values outside the scale range. Have a look at the age column in hp_data to see if you can decipher the warning message.\nThey were removed because \nthey fall outside of the plot range\nthey contain missing values.\nColours are getting manipulated slightly differently to the barchart. Click through each tab to see how you can alter colour, axes labels, margins and breaks, and add a different theme.\n\n\nColour\nAxes labels, margins, and breaks\nThemes\n\n\n\nWe can change the plot colours by adding a fill argument and a colour argument. The fill argument manipulates the colour of the bars, and the colour argument changes the outline of the bars. Pay attention that they are added directly to the geom_histogram arguments, not the overall aes() like we did with the boxplot.\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou could use\n\nhex codes for fill and color, like we used here, geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\"). If you want create your own colours, check out this website. OR\na pre-defined colour name geom_histogram(binwidth = 1, fill = \"purple\", colour = \"green\"). See a full list here OR\n\n\n\n\n\nHere we removed the label for the y axes Count (to show you some variety) and modified the breaks. The y axis is now displayed in increasing steps of 5 (rather than 10), and the x axis has 1-year increments rather than 5.\nNotice how the breaks = argument manipulates the labels of the break ticks but not the limit of the scale. You can manipulate the limits of the scale with the limits = argument. To exaggerate, we set the limits to 15 and 50. See how the values for 15 to 19, and 44 to 50 do not have a label. You would need to adjust that in breaks = argument.\nThe expansion() function removes the gap between x axis and bars. It is exactly the same code as we used in Chapter 4 .\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\") +\n  labs(x = \"Age (in years)\", # renaming x axis label\n       y = \"\") + # removing the y axis label\n  scale_y_continuous(\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05)),\n    # changing break points on y axis\n    breaks = seq(from = 0, to = 30, by = 5)\n    ) +\n  scale_x_continuous(\n    # changing break points on x axis\n    breaks = seq(from = 20, to = 43, by = 1),\n    # Experimenting with\n    limits = c(15, 50)\n    )\n\n\n\n\n\n\n\n\n\nLet’s experiment with the themes. For this plot we have chosen theme_bw()\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\") +\n  labs(x = \"Age (in years)\", # renaming x axis label\n       y = \"\") + # removing the y axis label\n  scale_y_continuous(\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05)),\n    # changing break points on y axis\n    breaks = seq(from = 0, to = 30, by = 5)\n    ) +\n  scale_x_continuous(\n    # changing break points on x axis\n    breaks = seq(from = 19, to = 44, by = 1)\n    ) +\n  # pick a theme\n  theme_bw()"
  },
  {
    "objectID": "05-dataviz2.html#activity-4-scatterplot-geom_point",
    "href": "05-dataviz2.html#activity-4-scatterplot-geom_point",
    "title": "5  Data viz II",
    "section": "\n5.4 Activity 4: Scatterplot (geom_point())",
    "text": "5.4 Activity 4: Scatterplot (geom_point())\nScatterplots are appropriate when you want to plot two continuous variables. Here, we want to display the relationship between Acceptance of QRPs at Time point 1 and 2. The default scatterplot would be created with geom_point().\nWe could also add a trendline by adding geom_smooth(). The default trendline is loess. If you want a linear trendline, you would need to add method = lm into geom_smooth() function.\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean)) +\n  geom_point() +\n  geom_smooth()\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nFigure 5.3: Default Scatterplot with added trendline - loess (left) and linear (right)\n\n\n\nCustomising the colour of plot is slightly different to the other plots we’ve encountered so far. Technically, the point is not a “filled-in black area”, but an extremely wide outline of a circle. Therefore, we cannot use the usual fill argument, but have to switch to the colour argument (like we did for the outline of the histogram). See the tabs below how to change the colour for all points or if you want to change the colour according to groupings.\n\n\nColour for all points\nColour with grouping\nLegend title and labels\n\n\n\nIf we want to change the colour of all the points, we would add the colour argument to the geom_point() function. Likewise, changing the colour of the trendline would also require a colour argument. Here we went with pre-defined colour names, but HEX codes would work too\n\n# colour of all points and the trendline\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean)) +\n  geom_point(colour = 'magenta') +\n  geom_smooth(method = lm, colour = 'turquoise')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIf we wanted the points to change colour due to another grouping variable, the colour argument would go into the aes() If you don’t want to define the colours manually, use a colour palette like brewer (scale_colour_brewer()) or viridis (scale_colour_viridis_d()).\n\n## adding grouping variable Pre_reg_group and changing the colour values manually\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean, colour = Pre_reg_group)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  scale_colour_manual(values = c('mediumvioletred', 'steelblue1'))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nYou can tidy the legend title and group labels via the scale_colour_? function\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean, colour = Pre_reg_group)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  scale_colour_manual(values = c('mediumvioletred', 'steelblue1'),\n                      name = \"Pre-registered Dissertation\",\n                      labels = c(\"Yes\", \"No\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAll other layers would be exactly the same as in other plots. Try to add layers to the make the plot above prettier:\n\n\n1. relabel axes x and y\n\n2. set the x and y axis range from 1 to 7\n\n3. move the legend to a different position (either top, left, or bottom)\n\n4. add a theme\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\ncan be done in 2 different ways - labs() or scale_x_?\nwe did that for the histogram\nWe did that for the bar charts\npick a theme you like\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean, colour = Pre_reg_group)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  scale_colour_manual(values = c('mediumvioletred', 'steelblue1'),\n                      name = \"Pre-registered Dissertation\",\n                      labels = c(\"Yes\", \"No\")) +\n  labs (x = \"Acceptance of Questionable Research Practices (Time 1)\", \n        y = \"Acceptance of Questionable Research Practices (Time 2)\") +\n  theme_light() + # place before moving the legend position\n  theme(legend.position = \"top\") # move legend to the top\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "05-dataviz2.html#activity-5-boxplot-geom_boxplot",
    "href": "05-dataviz2.html#activity-5-boxplot-geom_boxplot",
    "title": "5  Data viz II",
    "section": "\n5.5 Activity 5: Boxplot (geom_boxplot())",
    "text": "5.5 Activity 5: Boxplot (geom_boxplot())\nA boxplot is one of the options to display a continuous variable with categorical grouping variable. Here, we want to create a boxplot to investigate whether their understanding of open science depends on whether or not students had research experience. Our default boxplot would look like this:\n\n# default boxplot\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nTada. As usual, we can make the plot pretty by adding various layers. Click on the tabs to see how\n\n\nColour\nAxes labels\nLegend and Theme\n\n\n\nWe can change the colour by adding a fill argument in the aes(). If we want to modify these colours further, we would add a scale_fill_? layer If you have specific colours in mind, you would use scale_fill_manual() or if you want to stick with pre-defined ones, like brewer scale_fill_brewer().\nBtw, this is exactly the same code we used for the barcharts.\n\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS, fill = Research_exp)) +\n  geom_boxplot() +\n  # customise colour\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\nWe need to relabel the axes labels for x and y with scale_x_discrete() and scale_y_continuous(). We can also tidy the labels for the groups and the breaks on the y-axis (in steps of 1 rather than 2) within the same functions\n\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS, fill = Research_exp)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_x_discrete(\n    # changing the label of x\n    name = \"Research Experience\",\n    # changing the group labels of the 2 groups\n    labels = c(\"Yes\", \"No\")) + \n  scale_y_continuous(\n    # changing name of the y axis\n    name = \"Confidence in Understanding Open Science (Time 1)\",\n    # changing break labels\n    breaks = c(seq(from = 1, to = 7, by = 1))\n  )\n\n\n\n\n\n\n\n\n\nThe legend is superfluous; best to take it off. As before, we can remove the legend by adding the argument guide = \"none\" into the scale_fill function.\nLet’s pick a theme we haven’t used yet: theme_dark()\n\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS, fill = Research_exp)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\",\n                    # removing the legend\n                    guide = \"none\") +\n  scale_x_discrete(\n    name = \"Research Experience\",\n    labels = c(\"Yes\", \"No\")) + \n  scale_y_continuous(\n    name = \"Confidence in Understanding Open Science (Time 1)\",\n    breaks = c(seq(from = 1, to = 7, by = 1))\n  ) +\n  # pick a theme\n  theme_dark()"
  },
  {
    "objectID": "05-dataviz2.html#activity-6-violin-plot-geom_violin",
    "href": "05-dataviz2.html#activity-6-violin-plot-geom_violin",
    "title": "5  Data viz II",
    "section": "\n5.6 Activity 6: Violin plot (geom_violin())",
    "text": "5.6 Activity 6: Violin plot (geom_violin())\nAn alternative to display a continuous variable with categorical grouping variable is a violin plot. Here, we want to create a violin plot to investigate whether the perception of supervisor support depended on planning to pre-register the dissertation. Our default violin plot would look like this:\n\n# default boxplot\nggplot(data_prp_viz, aes(x = Plan_prereg, y = Mean_Supervisor_Support)) +\n  geom_violin()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAdjusting the violin plot would be exactly the same as the boxplot. Try to add layers to the base plot above to\n\n\nchange the colours either manually or using a pre-defined colour palette\n\ntidy the axes labels and group names\n\nin case a legend appears, take it off\n\nadd a theme\n\n\n\n\n\n\n\nOne possible Solution\n\n\n\n\n\n\nggplot(data_prp_viz, aes(x = Plan_prereg, y = Mean_Supervisor_Support, fill = Plan_prereg)) +\n  geom_violin() +\n  scale_fill_manual(values = c('mediumspringgreen', 'orangered', 'slateblue'),\n                    # removing the legend\n                    guide = \"none\") +\n  scale_x_discrete(name = \"Plan to pre-register the dissertation\") + \n  scale_y_continuous(\n    name = \"Perceived Supervisory Support\",\n    breaks = c(seq(from = 1, to = 7, by = 1))\n  ) +\n  # pick a theme\n  theme_minimal()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_ydensity()`)."
  },
  {
    "objectID": "05-dataviz2.html#activity-7-violin-boxplots",
    "href": "05-dataviz2.html#activity-7-violin-boxplots",
    "title": "5  Data viz II",
    "section": "\n5.7 Activity 7: Violin-boxplots",
    "text": "5.7 Activity 7: Violin-boxplots\nSo far, we’ve only added one geom_ to our plots. Due to the layered system, we could add multiple geoms, for example, when creating a violin-boxplot.\nRemember, the order of the layers makes a difference sometimes. We’ve seen already, with themes we added at the very end that could override the argument for a legend position earlier. Here, ggplot + violinplot + boxplot looks different to ggplot + boxplot + violinplot.\nLet’s take the example of QRPs at timepoint 2 and a grouping variable of Second-year Grade.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean)) +\n  geom_violin() +\n  geom_boxplot()\n\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean)) +\n  geom_boxplot() +\n  geom_violin()\n\n\n\n\n\nFigure 5.4: Default violin-boxplot: Order of the layer matters\n\n\n\nCustomising a few elements such as width of the boxes and the colour.\n\n\nWidth of the boxes\nColour\n\n\n\nIf we want to get any information from the boxplot, we need to use order 1. But still, the boxplot is pretty wide and covers up important information from the violin. We could adjust the width of the boxes to make information more visible. This takes a bit of trial and error which width looks appropriate.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean)) +\n  geom_violin() +\n  geom_boxplot(width = 0.2)\n\n\n\nFigure 5.5: Default violin-boxplot: adjusting width of the box\n\n\n\n\n\nAdding colour should be pretty straight forward by now. This code is no different to the one we used in the boxplot or violin plot. We need to add the fill argument within the aes(), and a scale_fill_?layer.\nHowever, we can modify this further by adding an opacity argument alpha to the violin plot geom.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean, fill = Secondyeargrade)) +\n  geom_violin(alpha = 0.4) + # alpha for opacity\n  geom_boxplot(width = 0.2) + # change width of the boxes\n  scale_fill_brewer(palette = \"RdPu\") # customise colour\n\n\n\nFigure 5.6: Violin-boxplot with a different colour palette\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\n\n\nChange the x- and y-axis labels\n\nRemove the legend\n\nadd a theme\n\n\n\n\n\n\n\nOne possible Solution\n\n\n\n\n\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean, fill = Secondyeargrade)) +\n  geom_violin(alpha = 0.4) +\n  geom_boxplot(width = 0.2) +\n  scale_fill_brewer(palette = \"RdPu\",\n                    guide = \"none\") + # removes the legend\n  # change labels of x and y\n  labs (x = \"Second-year Grade\", y = \"Acceptance of Questionable Research Practices (Time 2)\") +\n  theme_classic()"
  },
  {
    "objectID": "05-dataviz2.html#activity-8-faceting---adding-another-grouping-variable",
    "href": "05-dataviz2.html#activity-8-faceting---adding-another-grouping-variable",
    "title": "5  Data viz II",
    "section": "\n5.8 Activity 8: Faceting - adding another grouping variable",
    "text": "5.8 Activity 8: Faceting - adding another grouping variable\nFaceting is really useful when you have subsets in the data. We will use this here on the violin-boxplot from above, but you could add this to pretty much any plot. The function to split up the plots into facets is called facet_wrap().\nLet’s add another grouping variable, Pre_reg_group, so we can see separate plots for the yes and no groups.\nSince the group labels on the x-axis are quite long, we need to adjust them. Adding guide = guide_axis(n.dodge = 2) to the scale_x_discrete() function helps to display labels across multiple rows.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean, fill = Secondyeargrade)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.2) +\n  scale_fill_brewer(palette = \"RdPu\",\n                    guide = \"none\") + \n  labs (x = \"Second-year Grade\", y = \"Acceptance of Questionable Research Practices (Time 2)\") +\n  theme_classic() +\n  facet_wrap(~Pre_reg_group) + # faceting to split into subplots for yes and no\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) # want display labels in 2 rows\n\n\n\nFigure 5.7: Pretty violin-boxplot split into pre-registration groups (yes and no)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe labels of Pre_reg_group are displayed as numbers 1 and 2. If this bugs you, fix the labels in the dataset. This would be less hassle than trying to fix it in the plot.\n\n\n\n\n\n\n\n\nSpecial case: Variables with subscales\n\n\n\n\n\nFor example, we want to show the relationship between SATs scores at timepoints 1 and 2, separately for all 4 subscales of the SATs questionnaires, we would need to wrangle the data first. The dataframe we need should look like this:\n\nhead(data_facet, n=5)\n\n\n\n\nCode\nSubscale\nTime1\nTime2\n\n\n\nTr10\nAffect\n4.000000\n3.500000\n\n\nTr10\nCognitiveCompetence\n5.166667\n4.166667\n\n\nTr10\nValue\n6.000000\n3.000000\n\n\nTr10\nDifficulty\n3.571429\n2.857143\n\n\nBi07\nAffect\n3.833333\n3.166667\n\n\n\n\n\n\nTry wrangling the data so that it looks like data_facet above\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nstep 1: select variables of interest\nstep 2: pivot\nstep 3: try to access information on subscales and timepoints from the variable names\nstep 4: pivot in the other direction\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndata_facet &lt;- data_prp_viz %&gt;% \n  select(Code, starts_with(\"SATS\")) %&gt;% \n  pivot_longer(cols = starts_with(\"SATS\"), names_to = \"Variable\", values_to = \"Mean_Scores\") %&gt;% \n  separate(Variable, into = c(NA, \"Subscale\", \"Timepoint\", NA), sep = \"_\") %&gt;% \n  pivot_wider(names_from = Timepoint, values_from = Mean_Scores)\n\n\n\n\n\n\n\nThen we can build a scatterplot with facets for the subscales\n\nggplot(data_facet, aes(x = Time1, y = Time2)) +\n  geom_point() +\n  facet_wrap(~Subscale)"
  },
  {
    "objectID": "05-dataviz2.html#pair-coding",
    "href": "05-dataviz2.html#pair-coding",
    "title": "5  Data viz II",
    "section": "Pair-coding",
    "text": "Pair-coding\nProvide a barchart, a violin-boxplot, and a scatterplot from the loneliness data and the students have to try and recreate one of those in the lab. If they are overly fast, they can do the other 2."
  },
  {
    "objectID": "05-dataviz2.html#test-your-knowledge-on-chapters-3-and-4",
    "href": "05-dataviz2.html#test-your-knowledge-on-chapters-3-and-4",
    "title": "5  Data viz II",
    "section": "Test your knowledge on Chapters 3 and 4",
    "text": "Test your knowledge on Chapters 3 and 4\nWhich plot would you choose for\n\n5.8.1 Activity 5: Your Turn\nPick any single or two categorical variables from the dataset and choose one of the appropriate plot choices. Start with a base plot and add other layers if you please.\nSave your plot and share it with us on Teams."
  },
  {
    "objectID": "06-chi-square-one-sample.html#intended-learning-outcomes",
    "href": "06-chi-square-one-sample.html#intended-learning-outcomes",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\ncompute a Cross-tabulation Chi-square test and report the results\ncompute a one-sample t-test and report the results\nunderstand when to use a non-parametric equivalent for the one-sample t-test, compute it, and report the results"
  },
  {
    "objectID": "06-chi-square-one-sample.html#individual-walkthrough",
    "href": "06-chi-square-one-sample.html#individual-walkthrough",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "06-chi-square-one-sample.html#overview",
    "href": "06-chi-square-one-sample.html#overview",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.1 Overview",
    "text": "6.1 Overview\nthey are all part of a general linear model.\nThe general linear model incorporates a number of different statistical models: ANOVA, ANCOVA, MANOVA, MANCOVA, ordinary linear regression, t-test and F-test.\nWhich test to use depends on what type of variable(s) you have, what design you chose, and what your research question is about. You can see them in the flowchart below.\n\n\nSimplified flowchart to help select the most appropriate test (created with drawio). View larger version\n\nYou can see below in which chapter they will be discussed:\n\nCross-tabulation chi-Square test (this chapter, Section 6.5)\nOne-sample t-test (this chapter, Section 6.6)\nTwo-sample or independent t-test (between-subjects design, Chapter 7)\nPaired t-test (within-subjects design, Chapter 8)\nCorrelation (Chapter 9)\nSimple regression (Chapter 10)\nMultiple regression (Chapter 11)\nOne-way ANOVA (Chapter 12)\nFactorial ANOVA (Chapter 13)"
  },
  {
    "objectID": "06-chi-square-one-sample.html#activity-1-setup-download-the-data",
    "href": "06-chi-square-one-sample.html#activity-1-setup-download-the-data",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.2 Activity 1: Setup & download the data",
    "text": "6.2 Activity 1: Setup & download the data\n\n\nCreate a new project and name it something meaningful (e.g., “2A_chapter6”, or “06_chi_square_one_sample_t”). See Section 1.2 if you need some guidance.\n\nCreate a new .Rmd file and save it to your project folder. See Section 1.3 if you get stuck.\nDelete everything after the setup code chunk (e.g., line 12 and below)\n\nDownload a reduced dataset here: data_ch6.zip. You’ll see one csv file with demographic information and questionnaire data as well as the codebook.\nExtract the data files from the zip folder and place them in your project folder. If you need help, see Section 1.4.\n\nCitation\n\nBallou, N., Vuorre, M., Hakman, T., Magnusson, K., & Przybylski, A. K. (2024, July 12). Perceived value of video games, but not hours played, predicts mental well-being in adult Nintendo players. https://doi.org/10.31234/osf.io/3srcw\n\nAs you can see, the study is a pre-print published on PsyArXiv Preprints. The data and supplementary materials are available on OSF: https://osf.io/6xkdg/\nAbstract\n\nStudies on video games and well-being often rely on self-report measures or data from a single game. Here, we study how 703 US adults’ time spent playing for over 140,000 hours across 150 Nintendo Switch games relates to their life satisfaction, affect, depressive symptoms, and general mental well-being. We replicate previous findings that playtime over the past two weeks does not predict well-being, and extend these findings to a wider range of timescales (one hour to one year). Results suggest that relationships, if present, dissipate within two hours of gameplay. Our non-causal findings suggest substantial confounding would be needed to shift a meaningful true effect to the observed null. Although playtime was not related to well-being, players’ assessments of the value of game time—so called gaming life fit—was. Results emphasise the importance of defining the gaming population of interest, collecting data from more than one game, and focusing on how players integrate gaming into their lives rather than the amount of time spent.\n\nChanges made to the dataset\n\nWe selected demographic variables, such as age, gender, ethnicity, employment, education level, and the Warwick-Edinburgh Mental Wellbeing Scale from the rich dataset.\nWe removed rows with missing values and categorical groupings for which observed frequencies were considered too small for the purpose of this chapter.\nWe won’t be looking into any game-related associations, but feel free to download the full dataset to explore further.\nThe original authors had quite strict inclusion criteria for their analysis. We did not, hence we ended up with more participants than the original study"
  },
  {
    "objectID": "06-chi-square-one-sample.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "06-chi-square-one-sample.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.3 Activity 2: Load in the library, read in the data, and familiarise yourself with the data",
    "text": "6.3 Activity 2: Load in the library, read in the data, and familiarise yourself with the data\nToday, we’ll need the following packages tidyverse, lsr, scales, qqplotr, car, pwr, and rcompanion as well as the data data_ballou_reduced.\n\n# load in the packages\n???\n\n# read in the data\ndata_ballou &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# load in the packages\nlibrary(tidyverse)\nlibrary(lsr)\nlibrary(scales)\nlibrary(qqplotr)\nlibrary(car)\nlibrary(pwr)\nlibrary(rcompanion)\n\n# read in the data\ndata_ballou &lt;- read_csv(\"data_ballou_reduced.csv\")"
  },
  {
    "objectID": "06-chi-square-one-sample.html#activity-3-data-wrangling",
    "href": "06-chi-square-one-sample.html#activity-3-data-wrangling",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.4 Activity 3: Data wrangling",
    "text": "6.4 Activity 3: Data wrangling\nThe categorical variables look quite tidy, but we need to convert gender and education level into factors. Our statistical test requires factors and categories might be sorted more easily when plotting.\nWe also have to calculate the overall score for the Warwick-Edinburgh Mental Wellbeing Scale. According to the official WEMWBS website, the scores of the individual items are summed up.\n\ncreate a new data object data_wemwbs to calculate the summed up scores for the wemwbs.\nconvert gender and education level into factors in the original data_ballou object. Feel free to sort them in a meaningful order.\njoin this data_ballou with data_wembs.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWe converted categorical variables into factors in Chapter 4 if you need a refresher\nCheck the QRPs questionnaire in Chapter 2 to see how we approached aggregating scores\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndata_wemwbs &lt;- data_ballou %&gt;% \n  pivot_longer(cols = wemwbs_1:wemwbs_14, names_to = \"Questions\", values_to = \"Scores\") %&gt;% \n  group_by(pid) %&gt;% \n  summarise(wemwbs_sum = sum(Scores))\n\ndata_ballou &lt;- data_ballou %&gt;% \n  mutate(gender = factor(gender,\n                         levels = c(\"Woman\", \"Man\", \"Non-binary\")),\n         eduLevel = factor(eduLevel,\n                           levels = c(\"Completed Secondary School\", \"Some University but no degree\", \"University Bachelors Degree\", \"Vocational or Similar\", \"Graduate or professional degree (MA, MS, MBA, PhD, etc)\"))) %&gt;% \n  left_join(data_wemwbs)\n\nJoining with `by = join_by(pid)`"
  },
  {
    "objectID": "06-chi-square-one-sample.html#sec-chi_square",
    "href": "06-chi-square-one-sample.html#sec-chi_square",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.5 Activity 4: Cross-tabulation Chi-square test",
    "text": "6.5 Activity 4: Cross-tabulation Chi-square test\nA Cross-Tabulation Chi-Square Test, also known as a Chi-square test of association/independence, tests how one variable is associated with the distribution of outcomes in another variable.\nWe will be performing a Chi-Square test using the categorical variables gender and eduLevel:\n\nPotential research question: “Is there an association between gender and level of education in the population?”\nNull Hypothesis (H0): “Gender and student status are independent; there is no association between gender and level of education.”\nAlternative Hypothesis (H1): “Gender and student status are not independent; there is an association between gender and level of education.”\n\n\n6.5.1 Task 1: Preparing the dataframe\nWe need to select your variables of interest. We don’t have missing values in this dataset, but if your future dataframe might contain some, use drop_na() before you turn categorical variables into factors.\n\nchi_square &lt;- data_ballou %&gt;% \n  select(pid, gender, eduLevel)\n\n\n6.5.2 Task 2: Compute descriptives\nWe need to calculate counts for each combination of the variables. This is best done in a frequency table. This also allows us to check that we don’t have missing values in the cells. The function we are using does not work with missing values.\n\nchi_square_frequency &lt;- chi_square %&gt;% \n  count(gender, eduLevel) %&gt;% \n  pivot_wider(names_from = eduLevel, values_from = n)\n\nchi_square_frequency\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nCompleted Secondary School\nSome University but no degree\nUniversity Bachelors Degree\nVocational or Similar\nGraduate or professional degree (MA, MS, MBA, PhD, etc)\n\n\n\nWoman\n63\n118\n169\n42\n65\n\n\nMan\n70\n125\n250\n34\n81\n\n\nNon-binary\n9\n23\n20\n4\n10\n\n\n\n\n\n\nWe should be ok here, even though the count for non-binary/vocational is quite low.\n\n6.5.3 Task 3: Check assumptions\nAssumption 1: categorical data\nThe two variables should be categorical data measured either at an ordinal or nominal level.\nWe can confirm that for our dataset. Gender is \nordinal\nnominal, and level of education is \nordinal\nnominal.\nAssumption 2: Independent observartions\nThe value of one observation in the dataset does not affect the value of any other observation.\nAnd we assume as much for our data.\nAssumption 3: Cells in the contingency table are mutually exclusive\nIndividuals can only belong to one cell in the contingency table.\nWe can confirm that by looking at the data and the contingency table.\nAssumption 4: Expected frequencies are sufficiently large\nNot an assumption that is listed consistently across various sources. When it is, it suggests that expected frequencies are larger than 5 or at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, Danielle Navarro points out that this seems to be a “somewhat conservative” criterion and should be taken as “rough guidelines” only (see https://learningstatisticswithr.com/book/chisquare.html#chisqassumptions.\nThis is information, we can either compute manually (see lecture slides) or wait till we get the output from the inferential statistics later.\n\n6.5.4 Task 4: Create an appropriate plot\nNow we can create the appropriate plot. Which plot would you choose when building one from object chi_square? A \nBarchart\nHistogram\nScatterplot\nViolin-Boxplot with geom layer \ngeom_col\ngeom_bar\ngeom_histogram\ngeom_point\ngeom_boxplot and geom_violin\nTry first before looking at the solution. Feel free to practice adding other layers to make the plot pretty.\n\n\n\n\n\n\nOne possible solution\n\n\n\n\n\n… is a grouped bar chart.\nI played about with the labels of the x-axis categories since the graduate label is super long. Google was my friend in this instance and showed me a nifty function called label_wrap() from the scales package which automatically adds line breaks after X characters. Here we chose 12; looked best. (See other options for long labels at https://www.andrewheiss.com/blog/2022/06/23/long-labels-ggplot/).\n\nggplot(chi_square, aes(x = eduLevel, fill = gender)) +\n  geom_bar(position = \"dodge\") + \n  scale_fill_viridis_d(name = \"Gender\") +\n  scale_x_discrete(name = \"Level of Education\",\n                   labels = label_wrap(12)) +\n  scale_y_continuous(name = \"Count\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n6.5.5 Task 5: Compute a chi-square test\nBefore we can do that, we need to turn our tibble into a dataframe - the associationTest() function we are using to compute the Chi-square test does not like tibbles. [you have nooooo clue how long that took to figure out - let’s say the error message was not entirely useful]\n\nchi_square_df &lt;- as.data.frame(chi_square)\n\nNow we can run the associationTest() function from the lsr package. The first argument is a formula. It starts with a ~ and then expects the 2 variables we want to associate connected with a +. The second argument is the dataframe.\n\nassociationTest(formula = ~ eduLevel + gender, data = chi_square_df)\n\nWarning in associationTest(formula = ~eduLevel + gender, data = chi_square_df):\nExpected frequencies too small: chi-squared approximation may be incorrect\n\n\n\n     Chi-square test of categorical association\n\nVariables:   eduLevel, gender \n\nHypotheses: \n   null:        variables are independent of one another\n   alternative: some contingency exists between variables\n\nObserved contingency table:\n                                                         gender\neduLevel                                                  Woman Man Non-binary\n  Completed Secondary School                                 63  70          9\n  Some University but no degree                             118 125         23\n  University Bachelors Degree                               169 250         20\n  Vocational or Similar                                      42  34          4\n  Graduate or professional degree (MA, MS, MBA, PhD, etc)    65  81         10\n\nExpected contingency table under the null hypothesis:\n                                                         gender\neduLevel                                                  Woman   Man\n  Completed Secondary School                               59.9  73.4\n  Some University but no degree                           112.2 137.5\n  University Bachelors Degree                             185.2 227.0\n  Vocational or Similar                                    33.8  41.4\n  Graduate or professional degree (MA, MS, MBA, PhD, etc)  65.8  80.7\n                                                         gender\neduLevel                                                  Non-binary\n  Completed Secondary School                                    8.65\n  Some University but no degree                                16.21\n  University Bachelors Degree                                  26.75\n  Vocational or Similar                                         4.88\n  Graduate or professional degree (MA, MS, MBA, PhD, etc)       9.51\n\nTest results: \n   X-squared statistic:  13.594 \n   degrees of freedom:  8 \n   p-value:  0.093 \n\nOther information: \n   estimated effect size (Cramer's v):  0.079 \n   warning: expected frequencies too small, results may be inaccurate\n\n\nThe output is quite informative. It gives information about:\n\nthe variables that were tested,\nthe null and alternative hypotheses,\na table with the observed frequencies (which matches what we calculated in chi_square_assumptions without the rows/columns of the missing values we removed),\nan output of the frequencies you’d expect if the null hypothesis were true,\nthe result of the hypothesis test, and\nthe effect size Cramer’s v.\n\nAnd it gives us a warning message saying that expected frequencies are too small and that the chi-squared approximation may be incorrect. This ties in with Assumption 4. Depending on your stance on Assumption 4, you may or may not want to ignore the warning.\nThe p-value tells us that the null hypothesis is not being rejected, since the p-value is larger than 0.05.\n\n6.5.6 Task 6: The write-up\nThe Chi-Square test revealed that there is no statistically significant association between Gender and Student Status, \\(\\chi^2(8) = 13.59, p = .093, V = .079\\). The strength of the association between the variables is considered small. We therefore fail to reject the null hypothesis."
  },
  {
    "objectID": "06-chi-square-one-sample.html#sec-onesample",
    "href": "06-chi-square-one-sample.html#sec-onesample",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.6 Activity 5: One-sample t-test",
    "text": "6.6 Activity 5: One-sample t-test\nThe one-sample t-test is used to determine whether a sample comes from a population with a specific mean. This population mean is not always known, but is sometimes hypothesized.\nWe will be performing a one-sample t-test using the continuous variable wemwbs_sum. The official website for the Warwick-Edinburgh Mental Wellbeing Scales states that the “WEMWBS has a mean score of 51.0 in general population samples in the UK with a standard deviation of 7 (Tennant et al., 2007)”.\n\nPotential research question: “Is the average mental well-being of gamers different from the general population’s average well-being?”\nNull Hypothesis (H0): “The summed-up WEMWBS score of gamers is not different to 51.0.”\nAlternative Hypothesis (H1): “The summed-up WEMWBS score of gamers is different from 51.0.”\n\n\n6.6.1 Task 1: Preparing the dataframe\nWe need to select your variables of interest.\n\none_sample &lt;- data_ballou %&gt;% \n  select(pid, wemwbs_sum)\n\n\n6.6.2 Task 2: Compute descriptives\nWe want to compute means and standard deviations for our variable of interest. This should be straight forward. Try it yourself and then compare your result with the solution below.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndescriptives &lt;- one_sample %&gt;% \n  summarise(mean_wemwbs = mean(wemwbs_sum),\n            sd = sd(wemwbs_sum))\n\ndescriptives\n\n\n\n\nmean_wemwbs\nsd\n\n\n45.42013\n10.88615\n\n\n\n\n\n\n\n\n\n6.6.3 Task 3: Create an appropriate plot\nThis is the one you want to include in your report, so make sure everything is clearly labelled. Which plot would you choose? Start creating one first before comparing with the solution below.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(one_sample, aes(x = \"\", y = wemwbs_sum)) +\n  geom_violin(fill = \"#FB8D61\", alpha = 0.4) + # alpha for opacity, fill for adding colour\n  geom_boxplot(fill = \"#FB8D61\", width = 0.5) + # change width of the boxes\n  theme_classic() +\n  labs(x = \"\",\n       y = \"Total WEMWBS Scores\")\n\n\n\n\n\n\n\n\n\n\n\n6.6.4 Task 4: Check assumptions\nAssumption 1: Continuous DV\nThe dependent variable needs to be measured at interval or ratio level. We can confirm that by looking at one_sample.\nAssumption 2: Data are independent\nThere is no relationship between the observations. Whilst this is an important assumption, it’s not one we can really test for. It has more to do with study design. Anyway, we assume this assumption holds for our data.\nAssumption 3: No significant outliers\nWe can check for that visually, for example in the violin-boxplot above.\nIt appears that there is one outlier on the lower tail, however, when inspecting the data in one_sample, we can see that it’s one person who got a score of 14 which is a possible value. Furthermore, when sample sizes are sufficiently large, like ours with 1083 participants, removing a single outlier makes not much sense. So we have checked this assumption, consider this outlier not significant, and therefore keep this observation in the dataset.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are removing any of the outliers, you need to recalculated the descriptive stats.\n\n\nAssumption 4: DV should be approximately normally distributed\nWe can already check normality from the violin-boxplot above but you could plot a histogram, a density plot, or a qqplot as an alternative to assess normality visually.\nAll of these options show that the data is normally distributed. That means, we will conduct a parametric test, i.e. a one-sample t-test.\n\n\n\n\n\n\nAlternatives to visually assess normality\n\n\n\n\n\n\n\nHistogram\nDensity plot\nQ-Q plot\n\n\n\nWe’ve already covered histograms in Chapter 5.\n\nggplot(one_sample, aes(x = wemwbs_sum)) +\n  geom_histogram(binwidth = 1, fill = \"magenta\")\n\n\n\n\n\n\n\n\n\nA density plot shows a smooth distribution curve of the data. The curve represents the proportion of the data in each range rather than the frequency. This means that the height of the curve doesn’t show how many times a value appears but rather the proportion of the data that falls into that range.\n\nggplot(one_sample, aes(x = wemwbs_sum)) +\n  geom_density(fill = \"magenta\")\n\n\n\n\n\n\n\n\n\nQ-Q plot stands for Quantile-Quantile Plot and compare two distributions by matching a common set of quantiles. It basically means that it’s comparing the distribution you have in your data with a normal distribution and plots this along a 45 degree line.\nIf the dots in the Q-Q plot fall roughly along that line, you can assume normality of the data. If they stray away from the line (and worse in some sort of pattern), we might not assume normality and conduct a non-parametric test instead. For the non-parametric equivalent, see Section 6.7.\nWe can either use the package car or qqplotr to build the qqplot.\n\nThe function qqPlot() is a single line but uses BaseR coding (i.e., the $ symbol) to access the column in the data object.\n\n\n# Version 1 with the car package\nqqPlot(one_sample$wemwbs_sum)\n\n[1] 295 394\n\n\n\n\nFigure 6.1: Q-Q plot created with the car package\n\n\n\n\nIf you have gotten used to ggplot by now, and prefer avoiding BaseR, you can use the package qqplotr. The downside is that you have to add the points, the line, and the confidence envelope yourself. On the plus, it has layers like ggplot, and is more customisable (just in case you wanted to look at something more colourful in the 2 seconds it’ll take you to assess normality).\n\n\n# Version 2 with package qqplotr\nggplot(one_sample, aes(sample = wemwbs_sum)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n\n\n\nFigure 6.2: Q-Q plot created with the qqplotr package\n\n\n\n\n\n\n\n\n\nYou could also assess normality with the Shapiro-Wilk’s test. The null hypothesis is that the population is distributed normally. Therefore, if the p-value of the Shapiro-Wilk’s test smaller than .05, normality is rejected.\n\n\n\n\n\n\nImportant\n\n\n\nShapiro-Wilk is an OK method for small sample sizes (e.g., smaller than 50 samples) if the deviation from normality is fairly obvious. If we are dealing with slight deviations from normality, it might not be sensitive enough to pick that up. But t-tests, ANOVAs etc. should be robust for slight deviations from normality anyway.\nIn contrast, when you have large sample sizes, Shapiro-Wilk is overly sensitive and will definitely produce a significant p-value regardless of what the distribution looks like. So don’t rely on its output when you have large sample sizes, and be mindful of its output when you have small sample sizes.\n\n\nThe function in R is shapiro.test() and it’s built into BaseR, meaning, we need our data object, the $ and the column we want to address.\n\nshapiro.test(one_sample$wemwbs_sum)\n\n\n    Shapiro-Wilk normality test\n\ndata:  one_sample$wemwbs_sum\nW = 0.99404, p-value = 0.0002619\n\n\nNo surprise here, the test shows p &lt; .001 because we have more than 1000 participants. Nevertheless, if you decided on using the Shapiro-Wilk test in your report, you’d need to write this result up in APA style: \\(W = .99, p &lt; .001\\).\n\n\n\n\n\n\nReport-writing Tip\n\n\n\nEither choose visual or computational inspection for normality tests. NO NEED TO DO BOTH!!!\n\nState what method you used and your reasons for choosing the method (visual/computational and what plot/test you used)\nState the outcome of the test - for visual inspection just say whether normality assumption held or not (no need to include that extra plot in the results section). For computational methods, report the test result in APA style\nState the conclusions you draw from it - parametric or non-parametric test\n\n\n\n\n6.6.5 Task 5: Compute a One-sample t-test and effect size\nWe can use the t.test() function to compute a one-sample t-test. The t.test() function is part of BaseR, and yes, you guessed it, our first argument has to follow the pattern data$column. The second argument (mu) lists the population mean we are testing our sample against (here 51.0). The alternative is “two.sided” by default, so you could leave the argument out.\n\nt.test(one_sample$wemwbs_sum, mu = 51.0, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  one_sample$wemwbs_sum\nt = -16.868, df = 1082, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 51\n95 percent confidence interval:\n 44.77106 46.06920\nsample estimates:\nmean of x \n 45.42013 \n\n\nThe output is quite informative. It gives us information about:\n\nthe variable column that was tested,\nthe t value, degrees of freedom, and p,\nthe alternative hypothesis,\na 95% confidence interval,\nand the mean of the column (which matches the one we computed in the descriptive - yay)\n\nWhat it doesn’t give us is an effect size. Meh. So we have to compute one ourselves.\nWe will calculate Cohen’s d using the function cohensD() from the lsr package. Similar to the t-test we just conducted, the first argument is data$column, the second argument is mu.\n\ncohensD(one_sample$wemwbs_sum, mu = 51.0)\n\n[1] 0.5125662\n\n\n\n6.6.6 Task 6: Sensitivity power analysis\nA sensitivity power analysis allows you to determine the minimum effect size that the study could reliably detect given the number of participants you have in the sample, the alpha level at 0.05, and an assumed power of 0.8.\nThe function we need to compute this is pwr.t.test() which is part of the pwr package. There are 4 factors - the APES (alpha, power, effect size, and sample size) - if you have 3, you can calculate the 4th. As stated above, we have 3. We also need to specify the type, i.e., that we are using it for a one-sample t-test, and that alternative is “two.sided” because we have a non-directional hypothesis.\n\npwr.t.test(n = 1083, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 1083\n              d = 0.08520677\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSo the smallest effect size we can detect with a sample size of 1083, an alpha level of 0.05, and power of 0.8 is 0.09. This is a smaller value than the actual effect size we calculated with our CohensD function above (i.e., 0.51) which means our analysis is sufficiently powered.\n\n6.6.7 Task 7: The write-up\nA one-sample t-test was computed to determine whether the average mental well-being of gamers as measured by the WEMWBS was different to the population well-being mean. The average WEMWBS of the gamers \\((N = 1083, M = 45.42, SD = 10.89)\\) was significantly lower than the population mean well-being score of 51.0, \\(t(1082) = 16.87, p &lt; .001, d = .51\\). The strength of the effect is considered medium and the study was sufficiently powered. We therefore reject the null hypothesis in favour of H1."
  },
  {
    "objectID": "06-chi-square-one-sample.html#sec-alternative_one_sample",
    "href": "06-chi-square-one-sample.html#sec-alternative_one_sample",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.7 Activity 6: Non-parametric alternative",
    "text": "6.7 Activity 6: Non-parametric alternative\nIf any of the assumptions are violated, you need to switch to the non-parametric alternative. For the one-sample t-test, this is a One-sample Wilcoxon signed-rank test. Instead of the mean, it compares the median of a sample against a single value (i.e., the population median).\nThat means we need to find the population median, and calculate some summary stats for our sample:\n\nThe population median is listed in a supporting document on the official WEMWBS website as 53.0.\nWe can easily calculate the summary statistics using the function summary.\n\n\nsummary(one_sample)\n\n     pid              wemwbs_sum   \n Length:1083        Min.   :14.00  \n Class :character   1st Qu.:38.00  \n Mode  :character   Median :46.00  \n                    Mean   :45.42  \n                    3rd Qu.:53.00  \n                    Max.   :70.00  \n\n\nThe function to compute a one-sample Wilcoxon test is wilcox.test(). It’s part of BaseR and code-wise, it works very similar to the one-sample t-test.\n\nwilcox.test(one_sample$wemwbs_sum, mu = 53.0)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  one_sample$wemwbs_sum\nV = 87218, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 53\n\n\nAs we can see, the output gives us a V value, but we need to report the standardised test statistic Z in the final write-up. Unfortunately, we have to calculate Z manually. According to Andy Field (2012, p. 665), we need to use the qnorm function on the halved p-value from our Wilcoxon test above. Here, we store the p-value in the Global Environment as p_wilcoxon because it can store more decimal places than we see on the output above which means the Z value will be more accurate.\n\n# storing the p-value\np_wilcoxon &lt;- wilcox.test(one_sample$wemwbs_sum, mu = 53.0)$p.value\n\n# calculate the z value from half the p-value\nz = qnorm(p_wilcoxon/2)\nz\n\n[1] -19.12264\n\n\nWe also need to calculate the effect size r, which we can do via the wilcoxonOneSampleR from the rcompanion package. The default value will be 3 decimal places, but you can change that with the digits argument.\n\nwilcoxonOneSampleR(one_sample$wemwbs_sum, mu = 53.0, digits = 3)\n\n    r \n-0.59 \n\n\nNow we have all the numbers we need to write up the results:\nA One-sample Wilcoxon signed-rank test was used to compare Gamers’ mental-wellbeing median scores (Mdn = 46.0) to the population median of 53.0. The test showed a significant difference, \\(Z = -19.12, p &lt; .001, r = .590\\). The strength of the effect is considered medium. We therefore reject the null hypothesis in favour of H1."
  },
  {
    "objectID": "06-chi-square-one-sample.html#pair-coding",
    "href": "06-chi-square-one-sample.html#pair-coding",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "06-chi-square-one-sample.html#test-your-knowledge-on-chapters-3-and-4",
    "href": "06-chi-square-one-sample.html#test-your-knowledge-on-chapters-3-and-4",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Test your knowledge on Chapters 3 and 4",
    "text": "Test your knowledge on Chapters 3 and 4"
  },
  {
    "objectID": "07-independent.html#intended-learning-outcomes",
    "href": "07-independent.html#intended-learning-outcomes",
    "title": "7  Two-sample t-test",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\na\nb\nc"
  },
  {
    "objectID": "07-independent.html#individual-walkthrough",
    "href": "07-independent.html#individual-walkthrough",
    "title": "7  Two-sample t-test",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "07-independent.html#activity-1-setup-download-the-data",
    "href": "07-independent.html#activity-1-setup-download-the-data",
    "title": "7  Two-sample t-test",
    "section": "\n7.1 Activity 1: Setup & download the data",
    "text": "7.1 Activity 1: Setup & download the data\n\n\nCreate a new project and name it something meaningful (e.g., “2A_chapter7”, or “07_independent_ttest”). See Section 1.2 if you need some guidance.\n\nCreate a new .Rmd file and save it to your project folder. See Section 1.3 if you get stuck.\nDelete everything after the setup code chunk (e.g., line 12 and below)\n\nDownload a reduced dataset here: data_ch7.zip. You’ll see the Codebook, a demographics file, a file containing the mean response times, and a docx file of Supplementary Materials with extra information about the Simon Task, the results, etc. We also provided the raw data file for you to see what experimental data looks like when it hasn’t been pre-processed yet.\nExtract the data files from the zip folder and place them in your project folder. If you need help, see Section 1.4.\n\nCitation\n\nZwaan, R. A., Pecher, D., Paolacci, G., Bouwmeester, S., Verkoeijen, P., Dijkstra, K., & Zeelenberg, R. (2018). Participant nonnaiveté and the reproducibility of cognitive psychology. Psychonomic Bulletin & Review, 25, 1968-1972. https://doi.org/10.3758/s13423-017-1348-y\n\nThe data and supplementary materials are available on OSF: https://osf.io/ghv6m/\nAbstract\n\nMany argue that there is a reproducibility crisis in psychology. We investigated nine well-known effects from the cognitive psychology literature—three each from the domains of perception/action, memory, and language, respectively—and found that they are highly reproducible. Not only can they be reproduced in online environments, but they also can be reproduced with nonnaïve participants with no reduction of effect size. Apparently, some cognitive tasks are so constraining that they encapsulate behavior from external influences, such as testing situation and prior recent experience with the experiment to yield highly robust effects.\n\nChanges made to the dataset\n\nWe reduced the dataset, demographic information, and Supplementary Materials to only include information about the Simon Task. The full dataset including the other 8 tasks can be found on OSF.\nNo other changes were made."
  },
  {
    "objectID": "07-independent.html#activity-2-library-and-data-for-today",
    "href": "07-independent.html#activity-2-library-and-data-for-today",
    "title": "7  Two-sample t-test",
    "section": "\n7.2 Activity 2: Library and data for today",
    "text": "7.2 Activity 2: Library and data for today\nToday, we’ll need the following packages rstatix, tidyverse, car, lsr, and pwr. Make sure that rstatix is read in before tidyverse, otherwise, it will mask some functions we will need later on. We also need to read in the data from MeansSimonTask.csv and the demographics information from DemoSimonTask.csv.\n\n# load in the packages\n???\n\n# read in the data\nzwaan_data &lt;- ???\nzwaan_demo &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# load in the packages\nlibrary(rstatix)\nlibrary(tidyverse)\nlibrary(car)\nlibrary(lsr)\nlibrary(pwr)\n\n# read in the data\nzwaan_data &lt;- read_csv(\"MeansSimonTask.csv\")\nzwaan_demo &lt;- read_csv(\"DemoSimonTask.csv\")"
  },
  {
    "objectID": "07-independent.html#activity-3-familiarise-yourself-with-the-data",
    "href": "07-independent.html#activity-3-familiarise-yourself-with-the-data",
    "title": "7  Two-sample t-test",
    "section": "\n7.3 Activity 3: Familiarise yourself with the data",
    "text": "7.3 Activity 3: Familiarise yourself with the data\nAs usual, familiarise yourself with the data before starting on the between-subjects t-test. Also, more importantly, have a look at the Supplementary Materials in which the Simon effect is explained in more depth.\nIn general, the Simon effect refers to the observation that participants have shorter response times when the stimulus appears on the same side of the screen as the button they need to press (i.e., a congruent condition). In contrast, when the stimulus appears on the opposite side of the screen from the button they are supposed to press (i.e., an incongruent condition), their response times are longer.\nIn this experiment, all participants completed 2 sessions of trials. Half of the participants received the same stimulus set across sessions 1 and 2, whereas for the other half the stimulus set they received in session 2 differed from the one already encountered in session 1.\n\nPotential research question: “Is there a significant difference in the Simon effect between participants who received the same stimuli in both sessions compared to those who received different stimuli?”\nNull Hypothesis (H0): “There is no significant difference in the Simon effect between participants who received the same stimuli in both sessions and those who received different stimuli.”\nAlternative Hypothesis (H1): “There is a significant difference in the Simon effect between participants who received the same stimuli in both sessions and those who received different stimuli.”"
  },
  {
    "objectID": "07-independent.html#activity-4-preparing-the-dataframe",
    "href": "07-independent.html#activity-4-preparing-the-dataframe",
    "title": "7  Two-sample t-test",
    "section": "\n7.4 Activity 4: Preparing the dataframe",
    "text": "7.4 Activity 4: Preparing the dataframe\nThe data is already in a very good shape, however, we do have some wrangling to do to compute this Simon effect.\nTo calculate the Simon effect, we need\n\none mean response time (RT) value for congruent and one for incongruent trials per participant, and then\nsubtract the mean RT of congruent trials from the mean RT of incongruent trials.\n\nAnd to have all data in one place, we should join this output with the demographics.\nBasically, we want to create a tibble that has the following content. [Note that I re-arranged the columns and re-labelled some of them in a final step, so your column names and/or order might be slightly different, but content should match.]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant\ngender\nage\neducation\nsimilarity\ncongruent\nincongruent\nsimon_effect\n\n\n\nT1\nFemale\n50\nHigh school\nsame\n475.0032\n508.2835\n33.28029\n\n\nT10\nMale\n45\nAssociate’s degree\nsame\n420.1515\n401.5800\n-18.57148\n\n\nT109\nMale\n33\nBachelor’s degree\nsame\n339.5343\n375.7152\n36.18085\n\n\nT11\nFemale\n71\nHigh school\nsame\n516.9722\n542.3111\n25.33889\n\n\nT111\nFemale\n34\nHigh school\nsame\n373.5778\n394.0665\n20.48874\n\n\n\n\n\n\nObviously, there are various ways of doing this, so feel free to come up with your own way. However, we will provide step-by-step instructions for one of those ways that will get you the output:\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\nStep 1: the data is currently in wide format but would be better if it were in long format (so that all RT values are in 1 column; each participant has now 4 rows)\n\nStep 2: there should be a column now that contained the previous column headings with information on session number and congruency. It would be best if that was separated into 2 columns instead.\n\nStep 3: now we can calculate the mean RT for each participant, similarity, and congruency\n\n\nStep 4: pivot the data again, this time into wide format so that congruent and incongruent values are in 2 columns\n\nStep 5: create a new column called the simon_effect that subtracts congruent from incongruent values\n\nStep 6: join this together with the demographic information\n\nStep 7: feel free to rearrange the order of columns and/or rename them to match your output with ours (not strictly necessary tbh)\n\n\n\n\n\n\n\nSolution to the steps outlined above\n\n\n\n\n\n\nsimon_effect &lt;- zwaan_data %&gt;% \n  pivot_longer(cols = session1_congruent:session2_incongruent, names_to = \"col_headings\", values_to = \"RT\") %&gt;% \n  separate(col_headings, into = c(\"Session_number\", \"congruency\"), sep = \"_\") %&gt;% \n  group_by(participant, similarity, congruency) %&gt;% \n  summarise(mean_RT = mean(RT)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = congruency, values_from = mean_RT) %&gt;% \n  mutate(simon_effect = incongruent - congruent) %&gt;% \n  full_join(zwaan_demo, by = join_by(participant == twosubjectnumber)) %&gt;% \n  select(participant, gender = gender_response, age = age_response, education = education_response, similarity:simon_effect)"
  },
  {
    "objectID": "07-independent.html#activity-5-compute-descriptives",
    "href": "07-independent.html#activity-5-compute-descriptives",
    "title": "7  Two-sample t-test",
    "section": "\n7.5 Activity 5: Compute descriptives",
    "text": "7.5 Activity 5: Compute descriptives\nWe want to compute means and standard deviations for each group of our variable of interest, i.e., the mean RT and sd for our variable simon_effect for the same and the different group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndescriptives &lt;- simon_effect %&gt;% \n  group_by(similarity) %&gt;% \n  summarise(mean_RT = mean(simon_effect),\n            sd_RT = sd(simon_effect))\n\ndescriptives\n\n\n\n\nsimilarity\nmean_RT\nsd_RT\n\n\n\ndifferent\n32.85726\n20.79313\n\n\nsame\n35.99415\n22.39601"
  },
  {
    "objectID": "07-independent.html#activity-6-create-an-appropriate-plot",
    "href": "07-independent.html#activity-6-create-an-appropriate-plot",
    "title": "7  Two-sample t-test",
    "section": "\n7.6 Activity 6: Create an appropriate plot",
    "text": "7.6 Activity 6: Create an appropriate plot\nWhich plot would you choose to represent the data appropriately? Create an appropriate plot, then compare with the solution below.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(simon_effect, aes(x = similarity, y = simon_effect, fill = similarity)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.4, alpha = 0.8) +\n  scale_fill_viridis_d(guide = \"none\") +\n  theme_classic() +\n  labs(x = \"Similarity\", y = \"Simon effect\")"
  },
  {
    "objectID": "07-independent.html#activity-7-check-assumptions",
    "href": "07-independent.html#activity-7-check-assumptions",
    "title": "7  Two-sample t-test",
    "section": "\n7.7 Activity 7: Check assumptions",
    "text": "7.7 Activity 7: Check assumptions\nAssumption 1: Continuous DV\nThe dependent variable needs to be measured at interval or ratio level. We can confirm that by looking at simon_effect.\nAssumption 2: Data are independent\nThere is no relationship between the observations. The scores in one condition/observation can’t influence the scores in another. We assume this assumption holds for our data.\nAssumption 3: Homoskedasticity (homogeneity of variance)\nIf the variances between the 2 groups are similar/equal, we have homoskedasticity. If the variances between the 2 groups are dissimilar/unequal, we have heteroskedasticity.\nWe can test this with a Levene’s Test for Equality of Variance. The Levene’s test is part of the package car. The first argument is a formula, and it’s structured as DV ~ IV. In our data, the DV would be our continuous simon_effect variable, and the IV is the grouping variable similarity. Separate those 2 variables with a tilde. The second argument is the data.\n\nleveneTest(simon_effect ~ similarity, data = simon_effect)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n\n\n\n\nDf\nF value\nPr(&gt;F)\n\n\n\ngroup\n1\n0.7263221\n0.3953679\n\n\n\n158\nNA\nNA\n\n\n\n\n\n\nThe warning message tells us that the grouping variable was converted into a factor. Oops, I guess we forgot to turn the variables into a factor during data wrangling.\nFrom the above result, we see that p-value is greater than .05. That means, we do have not enough evidence to reject the null hypothesis. So the variance across the 2 groups are assumed equal.\nYou would report this in APA style: A Levene’s test of homogeneity of variances was used to compare the variances of the same and the different groups. It indicated that the variances were homogenous, \\(F(1,158) = 0.73, p = .395\\).\n\n\n\n\n\n\nImportant\n\n\n\nOne other thing to note is the t-test we are conducting is a Welch t-test by default. Welch gives similar results to a Student’s t-test when variances are equal, but is to be favoured when variances are not equal.\nSo even if Levene’s returns a significant p-value indicating groups have unequal variances, we can still use the Welch t-test.\n\n\nAssumption 4: DV should be approximately normally distributed\nHere, we need to pay attention that this means normally distributed in each group.\nWe can either use our eyeballs again on the violin-boxplot we created earlier (or use a qqplot, density plot, or histogram instead), OR compute a statistic like the Shapiro-Wilk’s test we already mentioned for the one-sample t-test. However, might still have the issue of large sample sizes (i.e. ca 80 participants in each group).\nVisual inspection would tell us that both groups look pretty normally distributed - the “same” group slightly more so than the “different” group because of the peak in the lower tail. But it still looks pretty normal for real-life data.\n\n\n\n\n\n\nTip\n\n\n\nIf you wanted to use a histogram, density plot or qqplot (the ones created with the ggplot2 and qqplotr packages), you could just add a facet_wrap() function to display the figures separate for each group.\nIf you used the Q-Q plot function from the car package, you would need to create different data objects with the filtered data for each group first before you can create the Q-Q plots for both groups separately.\n\n\nYou can still use computational methods, like the Shapiro-Wilk’s test we mentioned in the last chapter. The function does not allow for a formula, which means we would have to use different objects for the 2 different groups first. I guess this is a good way of practicing the filter function.\nTask: Create separate data object for the same and different group and then run the Shapiro-Wilk test on them. What do you conclude from the results?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## same group\nsame &lt;- simon_effect %&gt;% \n  filter(similarity == \"same\")\n\nshapiro.test(same$simon_effect)\n\n## different group\ndifferent &lt;- simon_effect %&gt;% \n  filter(similarity == \"different\")\n\nshapiro.test(different$simon_effect)\n\n\n    Shapiro-Wilk normality test\n\ndata:  same$simon_effect\nW = 0.98921, p-value = 0.7447\n\n\n    Shapiro-Wilk normality test\n\ndata:  different$simon_effect\nW = 0.96949, p-value = 0.05262\n\n\nShapiro-Wilk’s test also showed the data for both groups, “same” and “different”, are normally distributed as all p-values are above .05. Again, if you used this method in your report, you would have to write up the results in APA style (see one-sample t-test).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you have read the Delacre et al. (2017) paper (https://rips-irsp.com/articles/10.5334/irsp.82), you might be aware that the normality assumption is not overly important for the Welch t-test.\nSo whether you judge both groups as “normally distributed” or interpret one as deviating slightly from normality, the Welch t-test should still be ok to use for this dataset.\n\n\nAfter checking all of these assumptions, we decided that all of them held, and we will compute a Welch two-sample t-test."
  },
  {
    "objectID": "07-independent.html#activity-8-compute-a-two-sample-t-test-and-effect-size",
    "href": "07-independent.html#activity-8-compute-a-two-sample-t-test-and-effect-size",
    "title": "7  Two-sample t-test",
    "section": "\n7.8 Activity 8: Compute a Two-sample t-test and effect size",
    "text": "7.8 Activity 8: Compute a Two-sample t-test and effect size\nThe t.test() function we used for the one-sample t-test can be used here, however, we can use it in a slightly different way. It does allow for a formula option. So instead of having to wrangle the data again and having to use $ to access columns, we can use the formula DV ~ IV. The t.test() function expects the following arguments:\n\nThe first argument in the formula with the pattern DV ~ IV\n\nThe second argument is the data\nThe third argument is specifying whether variances are equal between the groups. By default is is var.equal = FALSE which means a Welch t-test is getting conducted. If you were to set var.equal to TRUE, you would conduct a Student t-test.\nThe 4th argument alternative is “two.sided” by default, meaning we are checking the alternative hypothesis in both directions (i.e., a non-directional hypothesis)\n\n\nt.test(simon_effect ~ similarity, data = simon_effect, var.equal = FALSE, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  simon_effect by similarity\nt = -0.91809, df = 157.14, p-value = 0.36\nalternative hypothesis: true difference in means between group different and group same is not equal to 0\n95 percent confidence interval:\n -9.885574  3.611799\nsample estimates:\nmean in group different      mean in group same \n               32.85726                35.99415 \n\n\nThe output tells us:\n\nthe test that was conducted (here Welch)\nthe variables that were tested (here simon_effect by similarity),\nthe t value, degrees of freedom, and p,\nthe alternative hypothesis,\na 95% confidence interval,\nand the mean of both groups (which again match with our descriptives)\n\nThe t.test() function does not give us an effect size, so we have to compute it once again. We can use the CohensD() function from the lsr package as we did for the one-sample t-test. We can use the formula approach here as well. It requires us to add an extra argument method = \"unequal\" for the Welch version of the t-test.\n\ncohensD(simon_effect ~ similarity, data = simon_effect, method = \"unequal\")\n\n[1] 0.1451628"
  },
  {
    "objectID": "07-independent.html#activity-9-sensitivity-power-analysis",
    "href": "07-independent.html#activity-9-sensitivity-power-analysis",
    "title": "7  Two-sample t-test",
    "section": "\n7.9 Activity 9: Sensitivity power analysis",
    "text": "7.9 Activity 9: Sensitivity power analysis\nNext up is the sensitivity power analysis to determine the minimum effect size that we could have reliably detected with the number of participants that took part, the alpha level at 0.05, and an assumed power of 0.8.\nThe function we need to compute this is pwr.t.test() which is part of the pwr package. The arguments in the formula are the same as for the one-sample t-test; we just need to adjust the number of participants (which is the number of observations per sample) and set the type to “two.sample”.\n\npwr.t.test(n = 80, sig.level = 0.05, power = 0.8, type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 80\n              d = 0.445672\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSo the smallest effect size we can detect with a sample size of 80 participants in each group, an alpha level of 0.05, and power of 0.8 is 0.45. This is a larger value than the actual effect size we calculated with the CohensD function above (i.e., 0.14) which means our analysis is underpowered to detect this extremely small effect.\nJust out of curiosity, if we were to replicate this study, and we wanted to be able to detect an effect size that small, how many participants would we need to recruit? We run the pwr.t.test() function again, but replacing n with the effect size d. Ooft; we would need ca 1500 participants in total. To be honest, 0.145 would not be a meaningful effect size.\n\npwr.t.test(d = 0.145, sig.level = 0.05, power = 0.8, type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 747.5833\n              d = 0.145\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\n\n\n\n\nBut my two groups have unequal sample sizes, and there is only one n in pwr.t.test. What do I do?\n\n\n\nNo problem. You can use the function pwr.t2n.test() that allows you to specify two different sample sizes n1 and n2. The rest stays pretty much the same, even though there is no need to specify the type anymore.\n\npwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL, alternative = c(\"two.sided\", \"less\",\"greater\"))\n\nLet’s try it for our example. We should get the same result though.\n\npwr.t2n.test(n1 = 80, n2= 80, sig.level = 0.05, power = 0.8, alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 80\n             n2 = 80\n              d = 0.445672\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided"
  },
  {
    "objectID": "07-independent.html#activity-10-the-write-up",
    "href": "07-independent.html#activity-10-the-write-up",
    "title": "7  Two-sample t-test",
    "section": "\n7.10 Activity 10: The write-up",
    "text": "7.10 Activity 10: The write-up\nWe hypothesised that there would be a significant difference in the Simon effect between participants who received the same stimuli in both sessions \\((N = 80, M = 35.99 msec, SD = 22.40 msec)\\) and those who received different stimuli \\((N = 80, M = 32.86 msec, SD = 20.79 msec)\\). Using a two-sample Welch t-test, the effect was found to be non-significant and of a small magnitude, \\(t(157.14) = 0.92, p = .360, d = 0.15\\). The overall mean difference between groups was small \\((M_{diff} = 3.14 msec)\\). Therefore, we fail to reject the null hypothesis."
  },
  {
    "objectID": "07-independent.html#sec-alternative_two_sample",
    "href": "07-independent.html#sec-alternative_two_sample",
    "title": "7  Two-sample t-test",
    "section": "\n7.11 Activity 11: Non-parametric alternative",
    "text": "7.11 Activity 11: Non-parametric alternative\nThe Mann-Whitney U-test is the non-parametric equivalent to the independent two-sample t-test. The test can be used for any situation requiring a test to compare the median of two samples.\nAccording to the paper by Delacre et al. (2017), the Mann-Whitney U-test can cope with normality issues, but it remains sensitive to heteroscedasticity. Here, we won’t have a problem, since the variances in the two groups were equal, but perhaps be mindful in other datasets when assessing assumptions and drawing conclusions from them.\nFirst, let’s start of by computing some summary statistics for each group.\n\nsimon_effect %&gt;% \n  group_by(similarity) %&gt;% \n  summarise(n = n(), \n            median = median(simon_effect))\n\n\n\n\nsimilarity\nn\nmedian\n\n\n\ndifferent\n80\n34.44134\n\n\nsame\n80\n35.68470\n\n\n\n\n\n\nTo conduct a Mann-Whitney U-test, use the function wilcox.test(). This time, use the formula approach DV ~ IV - again, this is the same code structure we just used for the independent t-test.\n\nwilcox.test(simon_effect ~ similarity, data = simon_effect)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  simon_effect by similarity\nW = 3001, p-value = 0.4981\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe should compute the standardised test statistic Z once again. We need to calculate Z manually, using the qnorm function on the halved p-value from our Wilcoxon test above.\n\n# storing the p-value\np_wilcoxon &lt;- wilcox.test(simon_effect ~ similarity, data = simon_effect)$p.value\n\n# calculate the z value from half the p-value\nz = qnorm(p_wilcoxon/2)\nz\n\n[1] -0.6774047\n\n\nThe effect size for the Mann-Whitney U-test is r. To compute r, we’d need the standardised test statistic z and divide that the square-root of the number of pairs n: \\(r = \\frac{|z|}{\\sqrt n}\\). Or we could just use the wilcox_effsize() function from the rstatix package.\nThe arguments are in a slightly different order, but exactly the same as in the Wilcox test we used above.\n\nwilcox_effsize(data = simon_effect, formula = simon_effect ~ similarity)\n\n\n\n\n.y.\ngroup1\ngroup2\neffsize\nn1\nn2\nmagnitude\n\n\nsimon_effect\ndifferent\nsame\n0.0536884\n80\n80\nsmall\n\n\n\n\n\nThis is once again considered a small effect. Anyway, we do have all the numbers now to write up the results:\nA Mann-Whitney U-test was conducted to determine whether there was a significant difference in the Simon effect between participants who received the same stimuli in both sessions \\((N = 80, Mdn = 35.68 msec)\\) and those who received different stimuli \\((N = 80, Mdn = 34.44 msec)\\). The results indicate that the median response time difference was non-significant and of small magnitude, \\(W = 3001, Z = -0.68, p = .498, r = .054\\). Therefore, we fail to reject the null hypothesis."
  },
  {
    "objectID": "07-independent.html#pair-coding",
    "href": "07-independent.html#pair-coding",
    "title": "7  Two-sample t-test",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "07-independent.html#test-your-knowledge",
    "href": "07-independent.html#test-your-knowledge",
    "title": "7  Two-sample t-test",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "08-paired.html#intended-learning-outcomes",
    "href": "08-paired.html#intended-learning-outcomes",
    "title": "8  Paired t-test",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\na\nb\nc"
  },
  {
    "objectID": "08-paired.html#individual-walkthrough",
    "href": "08-paired.html#individual-walkthrough",
    "title": "8  Paired t-test",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "08-paired.html#activity-1-setup",
    "href": "08-paired.html#activity-1-setup",
    "title": "8  Paired t-test",
    "section": "\n8.1 Activity 1: Setup",
    "text": "8.1 Activity 1: Setup\nWe will still be working with the dataset from the study by Zwaan et al. (2018) in this chapter. Have a look at Chapter 7 or the SupMats document if you need a refresher about the Simon Task data.\n\nOpen last week’s project\nCreate a new .Rmd file and save it to your project folder\nDelete everything after the setup code chunk"
  },
  {
    "objectID": "08-paired.html#activity-2-library-and-data-for-today",
    "href": "08-paired.html#activity-2-library-and-data-for-today",
    "title": "8  Paired t-test",
    "section": "\n8.2 Activity 2: Library and data for today",
    "text": "8.2 Activity 2: Library and data for today\nToday, we’ll need the following packages rstatix, tidyverse, qqplotr, lsr, and pwr. Again, we also need to read in the data from MeansSimonTask.csv and the demographics information from DemoSimonTask.csv.\n\n# load in the packages\n???\n\n# read in the data\nzwaan_data &lt;- ???\nzwaan_demo &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# load in the packages\nlibrary(rstatix)\nlibrary(tidyverse)\nlibrary(qqplotr)\nlibrary(lsr)\nlibrary(pwr)\n\n# read in the data\nzwaan_data &lt;- read_csv(\"MeansSimonTask.csv\")\nzwaan_demo &lt;- read_csv(\"DemoSimonTask.csv\")\n\n\n\n\nAs usual, take some time to familiarize yourself with the data before starting the within-subjects t-test.\nToday, we’ll focus on the Simon effect. Remember that the Simon effect predicts that congruent trials will have shorter response times than incongruent trials.\n\nPotential research question: “Is there a significant difference in response times between congruent and incongruent trials in a Simon task?”\nNull Hypothesis (H0): “There is no significant difference in response times between congruent and incongruent trials in a Simon task.”\nAlternative Hypothesis (H1): “There is a significant difference in response times between congruent and incongruent trials in a Simon task.”"
  },
  {
    "objectID": "08-paired.html#activity-3-preparing-the-dataframe",
    "href": "08-paired.html#activity-3-preparing-the-dataframe",
    "title": "8  Paired t-test",
    "section": "\n8.3 Activity 3: Preparing the dataframe",
    "text": "8.3 Activity 3: Preparing the dataframe\nAgain, we need to calculate one mean response time (RT) value for congruent and one for incongruent trials per participant. Like last week, we can also compute the Simon effect again as the difference score between incongruent and congruent trails.\nTo keep all the data in one place, we should join this output with the demographics. While you won’t need the demographic information for the t-test itself, having it included will give you a complete dataframe. This can be useful when you need to calculate demographics for the Methods section; for example if you end up excluding data points you can compute sample size, age and gender splits straight away rather than having to apply the same exclusions to a different data object.\nFor the paired version of the t.test, we need congruent and inconguent trials in separate columns, so that each participant still only has one row in the dataframe (i.e., wide format). Here is the output we are after:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant\ngender\nage\neducation\nsimilarity\ncongruent\nincongruent\nsimon_effect\n\n\n\nT1\nFemale\n50\nHigh school\nsame\n475.0032\n508.2835\n33.28029\n\n\nT10\nMale\n45\nAssociate’s degree\nsame\n420.1515\n401.5800\n-18.57148\n\n\nT109\nMale\n33\nBachelor’s degree\nsame\n339.5343\n375.7152\n36.18085\n\n\nT11\nFemale\n71\nHigh school\nsame\n516.9722\n542.3111\n25.33889\n\n\nT111\nFemale\n34\nHigh school\nsame\n373.5778\n394.0665\n20.48874\n\n\n\n\n\n\nChallenge yourself: See if you could reproduce the table without hints this time.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsimon_effect &lt;- zwaan_data %&gt;% \n  pivot_longer(cols = session1_congruent:session2_incongruent, names_to = \"col_headings\", values_to = \"RT\") %&gt;% \n  separate(col_headings, into = c(\"Session_number\", \"congruency\"), sep = \"_\") %&gt;% \n  group_by(participant, similarity, congruency) %&gt;% \n  summarise(mean_RT = mean(RT)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = congruency, values_from = mean_RT) %&gt;% \n  mutate(simon_effect = incongruent - congruent) %&gt;% \n  full_join(zwaan_demo, by = join_by(participant == twosubjectnumber)) %&gt;% \n  select(participant, gender = gender_response, age = age_response, education = education_response, similarity:simon_effect)\n\n\n\n\n\n\n\n\n\n\nYou could have also\n\n\n\n\n\n\nlooked at the hints from the last chapter, since this is exactly the same dataframe we need today.\nsaved the data object from last week as a csv file and read it in here."
  },
  {
    "objectID": "08-paired.html#activity-4-compute-descriptives",
    "href": "08-paired.html#activity-4-compute-descriptives",
    "title": "8  Paired t-test",
    "section": "\n8.4 Activity 4: Compute descriptives",
    "text": "8.4 Activity 4: Compute descriptives\nWe want to compute means and standard deviations for the congruent and the incongruent trials. Then we want to subtract the mean RT of the congruent trials from the mean RT of the incongruent trials to get the average difference score between the two conditions.\n\ndescriptives &lt;- simon_effect %&gt;% \n  summarise(mean_congruent = mean(congruent),\n            sd_congruent = sd(congruent),\n            mean_incongruent = mean(incongruent),\n            sd_incongruent = sd(incongruent),\n            diff = mean_incongruent - mean_congruent, # diff = mean(simon_effect) would also work\n            sd_diff = sd(simon_effect))\n\ndescriptives\n\n\n\n\n\n\n\n\n\n\n\n\nmean_congruent\nsd_congruent\nmean_incongruent\nsd_incongruent\ndiff\nsd_diff\n\n\n427.6528\n74.17418\n462.0785\n74.69692\n34.42571\n21.59876\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how we did not have to use group_by() here since the data is in wide format."
  },
  {
    "objectID": "08-paired.html#activity-5-create-an-appropriate-plot",
    "href": "08-paired.html#activity-5-create-an-appropriate-plot",
    "title": "8  Paired t-test",
    "section": "\n8.5 Activity 5: Create an appropriate plot",
    "text": "8.5 Activity 5: Create an appropriate plot\nTo create an appropriate plot, we’d like the data to be in long format, with a column for congruency that hold the labels of congruent and incorgruent trials, and a column that stores the mean_RT for each. Each participant should have 2 rows now:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant\ngender\nage\neducation\nsimilarity\nsimon_effect\ncongruency\nmean_RT\n\n\n\nT1\nFemale\n50\nHigh school\nsame\n33.28029\ncongruent\n475.0032\n\n\nT1\nFemale\n50\nHigh school\nsame\n33.28029\nincongruent\n508.2835\n\n\nT10\nMale\n45\nAssociate’s degree\nsame\n-18.57148\ncongruent\n420.1515\n\n\nT10\nMale\n45\nAssociate’s degree\nsame\n-18.57148\nincongruent\n401.5800\n\n\nT109\nMale\n33\nBachelor’s degree\nsame\n36.18085\ncongruent\n339.5343\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nCreate the table above and then create an appropriate plot\n\n\n\n\n\n\nSolution for the data\n\n\n\n\n\n\nsimon_effect_long &lt;- simon_effect %&gt;% \n  pivot_longer(cols = c(congruent, incongruent), names_to = \"congruency\", values_to = \"mean_RT\")\n\n\n\n\n\n\n\n\n\n\nSolution for the plot\n\n\n\n\n\n\nggplot(simon_effect_long, aes(x = congruency, y = mean_RT, fill = congruency)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.4, alpha = 0.8) +\n  scale_fill_viridis_d(guide = \"none\") +\n  theme_classic() +\n  labs(x = \"Congruency\", y = \"mean Response Time\")"
  },
  {
    "objectID": "08-paired.html#activity-6-check-assumptions",
    "href": "08-paired.html#activity-6-check-assumptions",
    "title": "8  Paired t-test",
    "section": "\n8.6 Activity 6: Check assumptions",
    "text": "8.6 Activity 6: Check assumptions\nThe assumptions for a paired t-test are fairly similar to the one-sample t-test.\nAssumption 1: Continuous DV\nThe dependent variable needs to be measured at interval or ratio level. We can confirm that by looking at either the columns congruent and incongruent in the object simon_effect. Or the variable mean_RT in simon_effect_long. This assumption holds.\nAssumption 2: Data are independent\nFor a paired t-test this assumption applies to the pair of values, i.e., each pair of values needs to be from a separate participant. We assume this assumption holds for our data.\nAssumption 3: Normality\nThis assumption requires the difference scores to be approximately normally distributed. We cannot see that from the violin-boxplot above and have to plot the difference score, i.e., variable simon_effect.\n\n\n\n\n\n\nYour Turn\n\n\n\nPlot the difference score\n\n\n\n\n\n\nHint\n\n\n\n\n\nThink back to the one-sample t-test. How did we plot the normality assumption there?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nOption 1: Q-Q plot\nOption 2: Violin-boxplot of the difference score\nOption 3: Shapiro-Wilk test\n\n\n\n\nggplot(simon_effect, aes(sample = simon_effect)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n\n\n\n\n\n\n\n\n\n\nggplot(simon_effect, aes(x = \"\", y = simon_effect)) +\n  geom_violin(fill = \"#FB8D61\", alpha = 0.4) + # alpha for opacity, fill for adding colour\n  geom_boxplot(fill = \"#FB8D61\", width = 0.5) + # change width of the boxes\n  theme_classic() +\n  labs(x = \"\",\n       y = \"Difference in mean Response Time scores\")\n\n\n\n\n\n\n\n\n\n\nshapiro.test(simon_effect$simon_effect)\n\n\n    Shapiro-Wilk normality test\n\ndata:  simon_effect$simon_effect\nW = 0.98588, p-value = 0.1047\n\n\n\n\n\nBoth plots would suggest there are slight deviations from normality in the tails, but Shapiro-Wilk does not pick those up. Hence we conclude, the difference scores are approximately normally distributed.\n\n\n\n\n\nIf any of the assumptions are violated, use the non-parametric equivalent to the paired t-test, see Section 8.10."
  },
  {
    "objectID": "08-paired.html#activity-7-compute-a-paired-t-test-and-effect-size",
    "href": "08-paired.html#activity-7-compute-a-paired-t-test-and-effect-size",
    "title": "8  Paired t-test",
    "section": "\n8.7 Activity 7: Compute a paired t-test and effect size",
    "text": "8.7 Activity 7: Compute a paired t-test and effect size\nWe can use the t.test() function again to compute the paired t-test. However, we are stuck with the BaseR pattern data$column once more.\nIn case you haven’t picked it up by now, I am not much a fan of data$column (i.e., wide format) and prefer the DV ~ IV (i.e., long format) pattern. And there was a time when the t.test() function allowed to add an extra argument paired = TRUE to the formula version but that is no longer the case. Now, the argument only works on the default method, specifying arguments x and y separately. And because the default version doesn’t allow us to add a data = argument, we have to revert to data$column.\nLong story short, here are the arguments you need from the data object in wide format (in this case simon_effect:\n\n\ndata$column for condition 1\n\ndata$column for condition 2\nthe extra argument paired = TRUE to tell the function we are conducting a paired rather than a two-sample t-test\n\n\nt.test(simon_effect$congruent, simon_effect$incongruent, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  simon_effect$congruent and simon_effect$incongruent\nt = -20.161, df = 159, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -37.79808 -31.05334\nsample estimates:\nmean difference \n      -34.42571 \n\n\nThe output tells us pretty much what we need to know:\n\nthe test that was conducted (here a paired t-test)\nthe conditions that were compared (here congruent and incongruent),\nthe t value, degrees of freedom, and p,\nthe alternative hypothesis,\na 95% confidence interval,\nand the mean difference score between both conditions (which also matches with our descriptives above)\n\nThe t.test() function does not give us an effect size, so, again, we have to compute it ourselves. We can use the CohensD() function from the lsr package as we did for the one-sample and the two-sample t-test. We can use the formula approach here as well, and add the extra argument method = \"paired\".\n\ncohensD(simon_effect$congruent, simon_effect$incongruent, method = \"paired\")\n\n[1] 1.593874\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe cohensD() function would take a long format formula approach, such as from simon_effect_long, but you would need to assure that the columns are ordered correctly, i.e., Participant 1: condition 1, condition 2; Participant 2: condition 1, condition 2; etc.\n\ncohensD(mean_RT ~ congruency, data = simon_effect_long, method = \"paired\")\n\nWarning in cohensD(mean_RT ~ congruency, data = simon_effect_long, method =\n\"paired\"): calculating paired samples Cohen's d using formula input. Results\nwill be incorrect if cases do not appear in the same order for both levels of\nthe grouping factor\n\n\n[1] 1.593874"
  },
  {
    "objectID": "08-paired.html#activity-8-sensitivity-power-analysis",
    "href": "08-paired.html#activity-8-sensitivity-power-analysis",
    "title": "8  Paired t-test",
    "section": "\n8.8 Activity 8: Sensitivity power analysis",
    "text": "8.8 Activity 8: Sensitivity power analysis\nAs we the other t-test, we are conducting a sensitivity power analysis to determine the minimum effect size we could have determined with the number of participants (n = 160), alpha of 0.05 and power of 0.8. This will tell us if our analysis was sufficiently powered or not.\nThe function is once again pwr.t.test() from the pwr package. The arguments in the formula are the same as for the one-sample t-test; we just need to adjust the number of participants and set the type to “paired”.\n\npwr.t.test(n = 160, sig.level = 0.05, power = 0.8, type = \"paired\", alternative = \"two.sided\")\n\n\n     Paired t test power calculation \n\n              n = 160\n              d = 0.222858\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\n\nThe minimum effect size we could reliably detect is 0.22. Our actual effect size was 1.59, so this analysis was sufficiently powered."
  },
  {
    "objectID": "08-paired.html#activity-9-the-write-up",
    "href": "08-paired.html#activity-9-the-write-up",
    "title": "8  Paired t-test",
    "section": "\n8.9 Activity 9: The write-up",
    "text": "8.9 Activity 9: The write-up\nUsing a , a medium significant effect was found (t(31) = 2.4, p = .023, d = .42) which supports the alternative hypothesis that preferential looking time is higher for the familiar melody compared to the baseline stage.\nWe hypothesised that there would be a significant difference in the response times between congruent \\((M = 427.65 msec, SD = 74.17 msec)\\) and incongruent trials \\((M = 462.08 msec, SD = 74.70 msec)\\) of a Simon task. On average, participants were faster in the congruent compared to the incongruent condition \\((M_{diff} = 34.43 msec, SD_{diff} = 21.60 msec)\\). Using a within-subjects t-test, the effect was found to be significant and of a large magnitude, \\(t(159) = 20.16, p &lt; .001, d = 1.59\\). Therefore, we reject the null hypothesis in favour of H1."
  },
  {
    "objectID": "08-paired.html#sec-alternative_paired",
    "href": "08-paired.html#sec-alternative_paired",
    "title": "8  Paired t-test",
    "section": "\n8.10 Activity 10: Non-parametric alternative",
    "text": "8.10 Activity 10: Non-parametric alternative\nThe Wilcoxon signed-rank test is the non-parametric equivalent to the paired t-test, comparing the difference between the median for two measurements.\nBefore we compute the test, we need to determine some summary stats (e.g., the median) for the congruent and incongruent conditions. Similar to the One-sample Wilcoxon signed-rank test, we can use the summary() function again because our data is in wide format.\n\nsummary(simon_effect)\n\n participant           gender               age         education        \n Length:160         Length:160         Min.   :19.00   Length:160        \n Class :character   Class :character   1st Qu.:30.75   Class :character  \n Mode  :character   Mode  :character   Median :37.00   Mode  :character  \n                                       Mean   :39.86                     \n                                       3rd Qu.:49.00                     \n                                       Max.   :71.00                     \n  similarity          congruent      incongruent     simon_effect   \n Length:160         Min.   :290.0   Min.   :324.9   Min.   :-18.57  \n Class :character   1st Qu.:379.4   1st Qu.:413.9   1st Qu.: 21.62  \n Mode  :character   Median :411.3   Median :449.7   Median : 34.77  \n                    Mean   :427.7   Mean   :462.1   Mean   : 34.43  \n                    3rd Qu.:458.7   3rd Qu.:496.3   3rd Qu.: 46.22  \n                    Max.   :741.5   Max.   :727.9   Max.   :103.84  \n\n\nNow we can move on to the Wilcoxon signed-rank test. We will use the wilcox.test() function again, but add the argument paired = TRUE.\n\nwilcox.test(simon_effect$congruent, simon_effect$incongruent, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  simon_effect$congruent and simon_effect$incongruent\nV = 144, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe could have also run a One-sample Wilcoxon signed-rank test on the difference score, but instead of comparing that to a population median, we would compare it to 0.\n\nwilcox.test(simon_effect$simon_effect, mu = 0)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  simon_effect$simon_effect\nV = 12736, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 0\n\n\n\n\n\n\n\n\n\n\nThe order of the arguments matters\n\n\n\nThe p-value is the same but the V values seem to differ. Yes and no. The order in which you input variables into the function will affect the value of V - has to do with how the ranks are getting assigned. In our column simon_effect we subtracted congruent RT from incongruent RT. To have the exact equivalent, we would have had to switch the columns in the wilcox.test() function. And as you can see, the V values match.\n\nwilcox.test(simon_effect$incongruent, simon_effect$congruent, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  simon_effect$incongruent and simon_effect$congruent\nV = 12736, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\nWe should also compute the standardised test statistic Z. Again, we need to calculate Z manually, using the qnorm function on the halved p-value from our Wilcoxon test above.\n\n# storing the p-value\np_wilcoxon &lt;- wilcox.test(simon_effect$incongruent, simon_effect$congruent, paired = TRUE)$p.value\n\n# calculate the z value from half the p-value\nz = qnorm(p_wilcoxon/2)\nz\n\n[1] -10.72532\n\n\nTo calculate an effect size, we would need to use the function wilcox_effsize() from the rstatix package. Unlike, the wilcox.test() and the t.test() function, wilcox_effsize() expects data to be in long format to be able to use the DV ~ IV pattern. Fortunately, we still have that available in simon_effect_long. We also need to add the argument paired = TRUE.\n\nwilcox_effsize(data = simon_effect_long, formula = mean_RT ~ congruency, paired = TRUE)\n\n\n\n\n.y.\ngroup1\ngroup2\neffsize\nn1\nn2\nmagnitude\n\n\nmean_RT\ncongruent\nincongruent\n0.8479786\n160\n160\nlarge\n\n\n\n\n\nNow we have all the numbers to write this up:\nA Wilcoxon signed-rank test was conducted to determine whether there was a significant difference in response times between congruent \\((Mdn = 411.3 msec)\\) and incongruent trials \\((Mdn = 449.7 msec)\\) in a Simon task. Median response times of Congruent trials were significantly faster \\((Mdn = 34.77 msec)\\) than incongruent trials, \\(Z = -10.73, p &lt; .001, r = .848\\). The difference can be classified as large according to Cohen (1992). Therefore, we reject the null hypothesis in favour of H1."
  },
  {
    "objectID": "08-paired.html#pair-coding",
    "href": "08-paired.html#pair-coding",
    "title": "8  Paired t-test",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "08-paired.html#test-your-knowledge",
    "href": "08-paired.html#test-your-knowledge",
    "title": "8  Paired t-test",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "09-correlation.html#intended-learning-outcomes",
    "href": "09-correlation.html#intended-learning-outcomes",
    "title": "9  Correlations",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "09-correlation.html#individual-walkthrough",
    "href": "09-correlation.html#individual-walkthrough",
    "title": "9  Correlations",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "09-correlation.html#activity-1-setup",
    "href": "09-correlation.html#activity-1-setup",
    "title": "9  Correlations",
    "section": "9.1 Activity 1: Setup",
    "text": "9.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "09-correlation.html#activity-2-download-the-data",
    "href": "09-correlation.html#activity-2-download-the-data",
    "title": "9  Correlations",
    "section": "9.2 Activity 2: Download the data",
    "text": "9.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "09-correlation.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "09-correlation.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "9  Correlations",
    "section": "9.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data",
    "text": "9.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data"
  },
  {
    "objectID": "09-correlation.html#assumption-tests",
    "href": "09-correlation.html#assumption-tests",
    "title": "9  Correlations",
    "section": "9.4 Assumption tests",
    "text": "9.4 Assumption tests\nhttps://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html\nSpearman’s Rank correlation: This is equivalent to Pearson’s correlation—it tests for an association between two variables. https://tuos-bio-data-skills.github.io/intro-stats-book/non-parametric-tests.html"
  },
  {
    "objectID": "09-correlation.html#pair-coding",
    "href": "09-correlation.html#pair-coding",
    "title": "9  Correlations",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "09-correlation.html#test-your-knowledge",
    "href": "09-correlation.html#test-your-knowledge",
    "title": "9  Correlations",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "10-regression.html#intended-learning-outcomes",
    "href": "10-regression.html#intended-learning-outcomes",
    "title": "10  Simple regression",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "10-regression.html#individual-walkthrough",
    "href": "10-regression.html#individual-walkthrough",
    "title": "10  Simple regression",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "10-regression.html#activity-1-setup",
    "href": "10-regression.html#activity-1-setup",
    "title": "10  Simple regression",
    "section": "\n10.1 Activity 1: Setup",
    "text": "10.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "10-regression.html#activity-2-download-the-data",
    "href": "10-regression.html#activity-2-download-the-data",
    "title": "10  Simple regression",
    "section": "\n10.2 Activity 2: Download the data",
    "text": "10.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "10-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "10-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "10  Simple regression",
    "section": "\n10.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data",
    "text": "10.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data\nleave pretty much as is but change the data to one of the STARS datasets and keep that for today and next week"
  },
  {
    "objectID": "10-regression.html#activity-1-setup-download-the-data",
    "href": "10-regression.html#activity-1-setup-download-the-data",
    "title": "10  Simple regression",
    "section": "\n10.4 Activity 1: Setup & download the data",
    "text": "10.4 Activity 1: Setup & download the data\n\ncreate a new project and name it something meaningful (e.g., “2A_chapter5”, or “05_chi_square_one_sample_t”). See Section 1.2 if you need some guidance.\ncreate a new Rmd file and save it to your project folder. See Section 1.3 if you get stuck.\ndelete everything after the setup code chunk (e.g., line 12 and below)\ndownload the data here: data_ch5.zip.\nExtract the data files from the zip folder and place them in your project folder. If you need help, see Section 1.4.\n\nCitation\n\nAlter, U., Dang, C., Kunicki, Z. J., & Counsell, A. (2024). The VSSL scale: A brief instructor tool for assessing students’ perceived value of software to learning statistics. Teaching Statistics, 46(3), 152-163. https://doi.org/10.1111/test.12374\n\nAbstract\n\nThe biggest difference in statistical training from previous decades is the increased use of software. However, little research examines how software impacts learning statistics. Assessing the value of software to statistical learning demands appropriate, valid, and reliable measures. The present study expands the arsenal of tools by reporting on the psychometric properties of the Value of Software to Statistical Learning (VSSL) scale in an undergraduate student sample. We propose a brief measure with strong psychometric support to assess students’ perceived value of software in an educational setting. We provide data from a course using SPSS, given its wide use and popularity in the social sciences. However, the VSSL is adaptable to any statistical software, and we provide instructions for customizing it to suit alternative packages. Recommendations for administering, scoring, and interpreting the VSSL are provided to aid statistics instructors and education researchers understand how software influences students’ statistical learning.\n\nThe data is available on OSF: https://osf.io/bk7vw/\nChanges made to the dataset\n\nWe turned the excel file into a csv\nWe aggregated the main scales by reverse-scoring reverse-coded items (as listed in the codebook) and averaging.\nHowever, the responses to the individual items of the questionnaires are the raw data, not the reverse-coded scores! If you want to practice your data wrangling skills, feel free to do so.\nWe have tidied up the columns RaceEthE, GradesE, and MajorE, but we’ve left Gender and Student Status for you to tidy."
  },
  {
    "objectID": "10-regression.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "10-regression.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "10  Simple regression",
    "section": "\n10.5 Activity 2: Load in the library, read in the data, and familiarise yourself with the data",
    "text": "10.5 Activity 2: Load in the library, read in the data, and familiarise yourself with the data\nToday, we’ll need the following packages tidyverse, lsr ETC as well as the data Alter_2024_data.csv.\n\n???\n\ndata_alter &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lsr)\ndata_alter &lt;- read_csv(\"Alter_2024_data.csv\")"
  },
  {
    "objectID": "10-regression.html#activity-3-data-wrangling",
    "href": "10-regression.html#activity-3-data-wrangling",
    "title": "10  Simple regression",
    "section": "\n10.6 Activity 3: Data Wrangling",
    "text": "10.6 Activity 3: Data Wrangling\nTo have more informative categories within the demographic data, we would recommend relabeling the remaining two columns Gender and Student status according to the information in the codebook. Add Gender_tidy and StuSta_tidy to the data_alter object.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nGender would be a case of recoding one value as another (we did that for the Understanding_OS questionnaire in Chapter 2)\nStudent Status would be slightly more intricate having multiple entries that would be recoded as the same category (we did that for the SATs questionnaire in Chapter 2)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndata_alter &lt;- data_alter %&gt;% \n  mutate(Gender_tidy = case_match(GenderE,\n                                  1 ~ \"Female\",\n                                  2 ~ \"Male\",\n                                  3 ~ \"Non-Binary\",\n                                  .default = NA),\n         StuSta_tidy = case_when(\n           StuStaE %in% c(\"1\", \"Freshman\") ~ \"Freshman\",\n           StuStaE %in% c(\"2\", \"Sophomore\") ~ \"Sophomore\",\n           StuStaE %in% c(\"3\", \"Junior\") ~ \"Junior\",\n           StuStaE %in% c(\"4\", \"senior\", \"Senior\", \"post-bac\") ~ \"Senior or Higher\",\n           .default = StuStaE))\n\n\n\n\n\n\n\n\nggplot(data_alter, aes(sample = Mean_MA)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QANX)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QINFL)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QSF)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(x = Mean_QSF)) +\n  geom_histogram()\n\n\nggplot(data_alter, aes(sample = Mean_QHIND)) +\n  stat_qq() +\n  stat_qq_line()\n\nggplot(data_alter, aes(x = Mean_QHIND)) +\n  geom_histogram(binwidth = 0.1)\n\n\nggplot(data_alter, aes(sample = Mean_QSC)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QSE)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(x = Mean_QSE)) +\n  geom_histogram(binwidth = 0.1)\n\n\nggplot(data_alter, aes(sample = Mean_SPSS)) +\n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "10-regression.html#pair-coding",
    "href": "10-regression.html#pair-coding",
    "title": "10  Simple regression",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "10-regression.html#test-your-knowledge",
    "href": "10-regression.html#test-your-knowledge",
    "title": "10  Simple regression",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "11-multiple-regression.html#intended-learning-outcomes",
    "href": "11-multiple-regression.html#intended-learning-outcomes",
    "title": "11  Multiple regression",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "11-multiple-regression.html#individual-walkthrough",
    "href": "11-multiple-regression.html#individual-walkthrough",
    "title": "11  Multiple regression",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "11-multiple-regression.html#activity-1-setup",
    "href": "11-multiple-regression.html#activity-1-setup",
    "title": "11  Multiple regression",
    "section": "11.1 Activity 1: Setup",
    "text": "11.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "11-multiple-regression.html#activity-2-download-the-data",
    "href": "11-multiple-regression.html#activity-2-download-the-data",
    "title": "11  Multiple regression",
    "section": "11.2 Activity 2: Download the data",
    "text": "11.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "11-multiple-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "11-multiple-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "11  Multiple regression",
    "section": "11.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data",
    "text": "11.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data"
  },
  {
    "objectID": "11-multiple-regression.html#pair-coding",
    "href": "11-multiple-regression.html#pair-coding",
    "title": "11  Multiple regression",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "11-multiple-regression.html#test-your-knowledge",
    "href": "11-multiple-regression.html#test-your-knowledge",
    "title": "11  Multiple regression",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "12-one-way-anova.html#intended-learning-outcomes",
    "href": "12-one-way-anova.html#intended-learning-outcomes",
    "title": "12  One-way ANOVA",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "12-one-way-anova.html#individual-walkthrough",
    "href": "12-one-way-anova.html#individual-walkthrough",
    "title": "12  One-way ANOVA",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "12-one-way-anova.html#activity-1-setup",
    "href": "12-one-way-anova.html#activity-1-setup",
    "title": "12  One-way ANOVA",
    "section": "12.1 Activity 1: Setup",
    "text": "12.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "12-one-way-anova.html#activity-2-download-the-data",
    "href": "12-one-way-anova.html#activity-2-download-the-data",
    "title": "12  One-way ANOVA",
    "section": "12.2 Activity 2: Download the data",
    "text": "12.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables\n\nKruskal-Wallis test: This is equivalent to the global significance test in a one-way ANOVA—it tests for differences between the central tendency of several samples."
  },
  {
    "objectID": "12-one-way-anova.html#pair-coding",
    "href": "12-one-way-anova.html#pair-coding",
    "title": "12  One-way ANOVA",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "12-one-way-anova.html#test-your-knowledge",
    "href": "12-one-way-anova.html#test-your-knowledge",
    "title": "12  One-way ANOVA",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "13-factorial-anova.html#intended-learning-outcomes",
    "href": "13-factorial-anova.html#intended-learning-outcomes",
    "title": "13  Factorial ANOVA",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "13-factorial-anova.html#individual-walkthrough",
    "href": "13-factorial-anova.html#individual-walkthrough",
    "title": "13  Factorial ANOVA",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "13-factorial-anova.html#activity-1-setup",
    "href": "13-factorial-anova.html#activity-1-setup",
    "title": "13  Factorial ANOVA",
    "section": "13.1 Activity 1: Setup",
    "text": "13.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "13-factorial-anova.html#activity-2-download-the-data",
    "href": "13-factorial-anova.html#activity-2-download-the-data",
    "title": "13  Factorial ANOVA",
    "section": "13.2 Activity 2: Download the data",
    "text": "13.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "13-factorial-anova.html#pair-coding",
    "href": "13-factorial-anova.html#pair-coding",
    "title": "13  Factorial ANOVA",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "13-factorial-anova.html#test-your-knowledge",
    "href": "13-factorial-anova.html#test-your-knowledge",
    "title": "13  Factorial ANOVA",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "appendix-a-installing-r.html#how-to-install-r-and-rstudio",
    "href": "appendix-a-installing-r.html#how-to-install-r-and-rstudio",
    "title": "Appendix A — Installing R",
    "section": "How to install R and RStudio",
    "text": "How to install R and RStudio\nThe RSetGo book provides detailed instructions on how to install R and RStudio on your computer. It also includes links to walkthroughs for installing R on different types of computers and operating systems."
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-rstudio",
    "href": "appendix-b-updating-packages.html#updating-rstudio",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.1 Updating RStudio",
    "text": "B.1 Updating RStudio\nRStudio is the easiest component to update. Typically, updates to RStudio won’t affect your code, instead they add in new features, like spell-check or upgrades to what RStudio can do. There’s usually very little downside to updating RStudio and it’s easy to do.\nClick Help &gt; Check for updates\n\n\n\n\nUpdating RStudio\n\n\n\nIf an update is available, it will prompt you to download it and you can install it as usual."
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-r",
    "href": "appendix-b-updating-packages.html#updating-r",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.2 Updating R",
    "text": "B.2 Updating R\nFinally, you may also wish to update R itself. The key thing to be aware of is that when you update R, if you just download the latest version from the website, you will lose all your packages.\n\nB.2.1 Windows\nThe easiest way to update R on Windows and not cause yourself a huge headache is to use the installr package. When you use the updateR() function, a series of dialogue boxes will appear. These should be fairly self-explanatory but there is a full step-by-step guide available for how to use installr, the important bit is to select “Yes” when it asked if you would like to copy your packages from the older version of R.\n\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()\n\n\nB.2.2 Mac\nFor a Mac, you can use the updateR package. You’ll need to install this from GitHub. You will be asked to type your system password (that you use to log into your computer) in the console pane. If relevant, it will ask you if you want to restore your packages for a new major version.\n\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()"
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-packages",
    "href": "appendix-b-updating-packages.html#updating-packages",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.3 Updating packages",
    "text": "B.3 Updating packages\nPackage developers will occasionally release updates to their packages. This is typically to add in new functions to the package, or to fix or amend existing functions. Be aware that some package updates may cause your previous code to stop working. This does not tend to happen with minor updates to packages, but occasionally with major updates, you can have serious issues if the developer has made fundamental changes to how the code works. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) - don’t do it before an assessment or deadline just in case!\nTo update an individual package, the easiest way is to use the install.packages() function, as this always installs the most recent version of the package.\n\ninstall.packages(\"tidyverse\")\n\nTo update multiple packages, or indeed all packages, RStudio provides helpful tools. Click Tools &gt; Check for Package Updates. A dialogue box will appear and you can select the packages you wish to update. Be aware that if you select all packages, this may take some time and you will be unable to use R whilst the process completes.\n\n\n\n\nUpdating packages with RStudio"
  },
  {
    "objectID": "appendix-b-updating-packages.html#sec-package-install-troubleshooting",
    "href": "appendix-b-updating-packages.html#sec-package-install-troubleshooting",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.4 Troubleshooting",
    "text": "B.4 Troubleshooting\nOccasionally, you might have a few problem packages that seemingly refuse to update. For me, rlang and vctrs cause me no end of trouble. These aren’t packages that you will likely every explicitly load, but they’re required beneath the surface for R to do things like knit your Markdown files etc.\n\nB.4.1 Non-zero exit status\nIf you try to update a package and get an error message that says something like Warning in install.packages : installation of package ‘vctrs’ had non-zero exit status or perhaps Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :  namespace 'rlang' 0.4.9 is being loaded, but &gt;= 0.4.10 is required one solution I have found is to manually uninstall the package, restart R, and then install the package new, rather than trying to update an existing version. The installr package also has a useful function for uninstalling packages.\n\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")\n\n\nB.4.2 Cannot open file\nYou may get the following error after trying to install any packages at all:\n\nError in install packages : Cannot open file ‘C:/…..’: Permission denied\n\nThis usually indicates a permissions problem with writing to the default library (the folder that packages are kept in). Sometimes this means that you need to install R and RStudio as administrator or run it as administrator.\nOne other fix may be to change the library location using the following code (check in “C:/Program Files/R” for what version you should have instead of “R-3.5.2”):\n\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nIf that works and you can install packages, set this library path permanently:\n\nInstall the usethis package\nRun usethis::edit_r_profile() in the console; it will open up a blank file\nPaste into the file (your version of): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nSave and close the file\nRestart R for changes to take effect\n\nThe code in your .Rprofile will now run every time you start up R.\nAs always, if you’re having issues, please ask on Teams or come to office hours."
  },
  {
    "objectID": "appendix-d-symbols.html",
    "href": "appendix-d-symbols.html",
    "title": "Appendix C — Symbols",
    "section": "",
    "text": "Symbol\npsyTeachR Term\nAlso Known As\n\n\n\n()\n(round) brackets\nparentheses\n\n\n[]\nsquare brackets\nbrackets\n\n\n{}\ncurly brackets\nsquiggly brackets\n\n\n&lt;&gt;\nchevrons\nangled brackets / guillemets\n\n\n&lt;\nless than\n\n\n\n&gt;\ngreater than\n\n\n\n&\nampersand\n“and” symbol\n\n\n#\nhash\npound / octothorpe\n\n\n/\nslash\nforward slash\n\n\n\\\nbackslash\n\n\n\n-\ndash\nhyphen / minus\n\n\n_\nunderscore\n\n\n\n*\nasterisk\nstar\n\n\n^\ncaret\npower symbol\n\n\n~\ntilde\ntwiddle / squiggle\n\n\n=\nequal sign\n\n\n\n==\ndouble equal sign\n\n\n\n.\nfull stop\nperiod / point\n\n\n!\nexclamation mark\nbang / not\n\n\n?\nquestion mark\n\n\n\n’\nsingle quote\nquote / apostrophe\n\n\n”\ndouble quote\nquote\n\n\n%&gt;%\npipe\nmagrittr pipe\n\n\n|\nvertical bar\npipe\n\n\n,\ncomma\n\n\n\n;\nsemi-colon\n\n\n\n:\ncolon\n\n\n\n@\n“at” symbol\nvarious hilarious regional terms\n\n\n…\nglossary(\"ellipsis\")\ndots\n\n\n\n\n\n\n\nImage by James Chapman/Soundimals"
  },
  {
    "objectID": "appendix-x-How-to-cite-R.html",
    "href": "appendix-x-How-to-cite-R.html",
    "title": "Appendix D — Citing R and RStudio",
    "section": "",
    "text": "How to cite R and RStudio\nYou may be some way off writing a scientific report where you have to cite and reference R, however, when the time comes it is important to do so to give the people who built it (most of them for free!) credit. You should provide separate citations for R, RStudio, and the packages you use.\nTo get the citation for the version of R you are using, simply run the citation() function which will always provide you with the most recent citation.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo generate the citation for any packages you are using, you can also use the citation() function with the name of the package you wish to cite.\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTo generate the citation for the version of RStudio you are using, you can use the RStudio.Vesion() function:\n\nRStudio.Version()\n\nFinally, here’s an example of how that might look in the write-up of your method section:\n\nAnalysis was conducted using R (R Core Team, 2020), RStudio (Rstudio Team, 2020), and the tidyverse package (Wickham, 2017).\n\nAs noted, you may not have to do this for a while, but come back to this when you do because it’s important to give the open-source community credit for their work."
  },
  {
    "objectID": "appendix-y-license.html",
    "href": "appendix-y-license.html",
    "title": "License",
    "section": "",
    "text": "This book is licensed under Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA 4.0). You are free to share and adapt this book. You must give appropriate credit, provide a link to the license, and indicate if changes were made. If you adapt the material, you must distribute your contributions under the same license as the original."
  }
]