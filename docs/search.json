[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis",
    "section": "",
    "text": "Overview\nBook Name: Analysis\nSummary: Materials for the 2nd year undergraduate programme at the University of Glasgow, School of Psychology & Neuroscience.\nAuthors: CKT & GM. This version of the book is adapted from a previous version written by Phil McAleer, Carolina E. Kuepper-Tetzel, & Helena M. Paterson\nAim: This course covers data skills such as R Markdown, data wrangling with tidyverse, and data visualisation with ggplot2. It also introduces statistical concepts such as probabilities, Null Hypothesis Significance Testing (NHST), alpha, power, effect size, and sample size. The most common statistical analyses are covered in this book such as t-test, correlations, ANOVAs, and regressions.\nNote: This book is currently being updated which means that chapters are being published on a rolling basis.\nContact: This book is a living document and will be regularly checked and updated for improvements. Should you have any issues using the book or queries, please contact Carolina E. Kuepper-Tetzel.\nR Version: This book has been written with R version 4.4.1 (2024-06-14 ucrt) (Race for Your Life) and RStudio version 2023.12.1+402 “Ocean Storm”.\nRandomising Seed: In chapters that use some level of randomisation, where we have remembered, the seed is set as 1409.\n\n\n In this book, we will help you learn a whole host of skills and methods based around being a psychologist. If you have completed the Data Skills book in the PsyTeachR series (https://psyteachr.github.io/) the first few chapters will be familiar to you, with some additions. This is deliberate in order to refresh your knowledge and skills before moving on to more advanced topics. First, we will remind you how to work with R Markdown, before recapping the main functions we use for visualisation and data wrangling. From there we will build your understanding of probability before going on to using all these refreshed skills to analyse a variety of different experiments. The main idea of this book is being reproducible in our data analysis approach.\nThis book requires a higher level of self-directed learning than the first book; part of learning is trying things out yourself and recognising where you need help. If you get stuck, google the problem or what you would like to do and see if that helps.\nWhen working through this book remember that this is not about learning a software. We do not teach R independent of statistical knowledge and content. Rather, we teach data and analytical skills and knowledge within R. The goal is to continuously improve your data analysis skills!\nYou can do this!\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important."
  },
  {
    "objectID": "01-basics.html#intended-learning-outcomes",
    "href": "01-basics.html#intended-learning-outcomes",
    "title": "1  Projects and R Markdown",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\nRe-familiarise yourself with setting up projects\nRe-familiarise yourself with RMarkdown documents\nRecap and apply data wrangling procedures to analyse data"
  },
  {
    "objectID": "01-basics.html#individual-walkthrough",
    "href": "01-basics.html#individual-walkthrough",
    "title": "1  Projects and R Markdown",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "01-basics.html#r-and-r-studio",
    "href": "01-basics.html#r-and-r-studio",
    "title": "1  Projects and R Markdown",
    "section": "\n1.1 R and R Studio",
    "text": "1.1 R and R Studio\nRemember, R is a programming language that you will write code in and RStudio is an Integrated Development Environment (IDE) which makes working with R easier as it’s more user friendly. You need both components for this course.\nIf this is not ringing any bells yet, have a quick browse through the materials from year 1 to refresh your memory.\n\n1.1.1 R server\nUse the server only if you are unable to install R and RStudio on your computer (e.g., if you are using a Chromebook) or if you encounter issues while installing R on your own machine. Otherwise, you should install R and RStudio directly on your own computer. R and RStudio are already installed on the R server.\nYou will find the link to the server on Moodle.\n\n1.1.2 Installing R and RStudio on your computer\nAppendix A provides detailed instructions on how to install R and RStudio on your computer. It also includes links to walkthroughs for installing R on different types of computers and operating systems.\nIf you had R and RStudio installed on your computer last year, we recommend updating to the latest versions. In fact, it’s a good practice to update them at the start of each academic year. Detailed guidance can be found in Appendix B.\nOnce you have installed or updated R and RStudio, return to this chapter.\n\n1.1.3 Settings for Reproducibility\nBy now, you should be aware that the Psychology department at the University of Glasgow places a strong emphasis on reproducibility, open science, and raising awareness about questionable research practices (QRPs) and how to avoid them. Therefore, it’s important that you work in a reproducible manner so that others (and your future self) can understand and check your work. This also makes it easier for you to reuse your work in the future.\nAlways start with a clear workspace. If your Global Environment contains anything from a previous session, you can’t be certain whether your current code is working as intended or if it’s using objects created earlier.\nTo ensure a clean and reproducible workflow, there are a few settings you should adjust immediately after installing or updating RStudio. In Tools &gt; Global Options… General tab\n\nUncheck the box labelled Restore .RData into workspace at startup to make sure no data from a previous session is loaded into the environment\nset Save workspace to .RData on exit to Never to prevent your workspace from being saved when you exit RStudio.\n\n\n\nReproducibility settings in Global Options\n\n\n\n\n\n\n\nTip for keeping taps on parentheses\n\n\n\n\n\nR has included rainbow parentheses to help with keeping count on the brackets.\nTo enable the feature, go to Tools &gt; Global Options… Code tab &gt; Display tab and tick the last checkbox “Use rainbow parentheses”\n\n\nEnable Rainbow parenthesis\n\n\n\n\n\n1.1.4 RStudio panes\nRStudio has four main panes each in a quadrant of your screen:\n\nSource pane\nEnvironment pane\nConsole pane\nOutput pane\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAre you ready for a quick quiz to see what you remember about the RStudio panes from last year? Click on Quiz to see the questions.\n\n\n\n\n\n\nQuiz\n\n\n\n\n\nWhat is their purpose?\nThe Source pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Environment pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Console pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nThe Output pane…\n\nallows users to view and edit various code-related files, such as .Rmd filescontains the Files, Plots, R Packages, Help, Tutorial, Viewer, and Presentation tabsincludes the Environment tab that displays currently saved objects, and the History tab that displays the commands that were executed in the current session along a search functionprovides an area to interactively execute code\n\nWhere are these panes located by default?\n\nThe Source pane is located? \ntop left\nbottom right\ntop right\nbottom left\n\nThe Environment pane is located? \nbottom right\nbottom left\ntop left\ntop right\n\nThe Console pane is located? \ntop left\nbottom right\ntop right\nbottom left\n\nThe Output pane is located? \nbottom right\nbottom left\ntop right\ntop left\n\n\n\n\n\n\n\nIf you were not quite sure about one/any of the panes, check out the materials from Level 1. If you want to know more about them, there is the RStudio guide on posit"
  },
  {
    "objectID": "01-basics.html#sec-project",
    "href": "01-basics.html#sec-project",
    "title": "1  Projects and R Markdown",
    "section": "\n1.2 Activity 1: Creating a new project",
    "text": "1.2 Activity 1: Creating a new project\nIt’s important to create a new RStudio project whenever you start a new project. This practice makes it easier to work in multiple contexts, such as when analysing different datasets simultaneously. Each RStudio project has its own folder location, workspace, and working directories, which keeps all your data and RMarkdown documents organised in one place.\nLast year, you learnt how to create projects on the server, so you already know the steps. If cannot quite recall how that was done, go back to the Level 1 materials.\nOn your own computer, open RStudio, and complete the following steps in this order:\n\nClick on File &gt; New Project…\n\nThen, click on “New Directory”\nThen, click on “New Project”\nName the directory something meaningful (e.g., “2A_chapter1”), and save it in a location that makes sense, for example, a dedicated folder you have for your level 2 Psychology labs - you can either select a folder you have already in place or create a new one (e.g., I named my new folder “Level 2 labs”)\nClick “Create Project”. RStudio will restart itself and open with this new project directory as the working directory. If you accidentally close it, you can open it by double-clicking on the project icon in your folder\nYou can also check in your folder structure that everything was created as intended\n\n\n\nCreating a new project\n\n\n\n\n\n\n\nWhy is the Colour scheme in the gif different to my version?\n\n\n\n\n\nIn case anyone is wondering why my colour scheme in the gif above looks different to yours, I’ve set mine to “Pastel On Dark” in Tools &gt; Global Options… &gt; Appearances. And my computer lives in “dark mode”.\n\n\n\n\n\n\n\n\n\nDon’t nest projects\n\n\n\nDon’t ever save a new project inside another project directory. This can cause some hard-to-resolve problems."
  },
  {
    "objectID": "01-basics.html#sec-rmd",
    "href": "01-basics.html#sec-rmd",
    "title": "1  Projects and R Markdown",
    "section": "\n1.3 Activity 2: Create a new R Markdown file",
    "text": "1.3 Activity 2: Create a new R Markdown file\n\nOpen a new R Markdown document: click File &gt; New File &gt; R Markdown or click on the little page icon with a green plus sign (top left).\nGive it a meaningful Title (e.g., Level 2 chapter 1) - you can also change the title later. Feel free to add your name or GUID in the Author field author name. Keep the Default Output Format as HTML.\nOnce the .Rmd opened, you need to save the file.\nTo save it, click File &gt; Save As… or click on the little disc icon. Name it something meaningful (e.g., “chapter_01.Rmd”, “01_intro.Rmd”). Make sure there are no spaces in the name - R is not very fond of spaces… This file will automatically be saved in your project folder (i.e., your working directory) so you should now see this file appear in your file viewer pane.\n\n\n\nCreating a new .Rmd file\n\nRemember, an R Markdown document or .Rmd has “white space” (i.e., the markdown for formatted text) and “grey parts” (i.e., code chunks) in the default colour scheme (see Figure 1.1). R Markdown is a powerful tool for creating dynamic documents because it allows you to integrate code and regular text seamlessly. You can then knit your .Rmd using the knitr package to create a final document as either a webpage (HTML), a PDF, or a Word document (.docx). We’ll only knit to HTML documents in this course.\n\n\nR markdown anatomy (image from https://intro2r.com/r-markdown-anatomy.html)\n\n\n1.3.1 Markdown\nThe markdown space in an .Rmd is ideal for writing notes that explain your code and document your thought process. Use this space to clarify what your code is doing, why certain decisions were made, and any insights or conclusions you have drawn along the way. These notes are invaluable when revisiting your work later, helping you (or others) understand the rationale behind key decisions, such as setting inclusion/exclusion criteria or interpreting the results of assumption tests. Effectively documenting your work in the markdown space enhances both the clarity and reproducibility of your analysis.\nThe markdown space offers a variety of formatting options to help you organise and present your notes effectively. Here are a few of them that can enhance your documentation:\nHeading levels\nThere is a variety of heading levels to make use of, using the # symbol.\n\n\nYou would incorporate this into your text as:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n#### Heading level 4\n##### Heading level 5\n###### Heading level 6\n\n\nAnd it will be displayed in your knitted html file as:\n\n\n\n\n\n\n\n\n\n\nERROR: My heading levels don’t render properly when knitting\n\n\n\n\n\nYou need a space between the # and the first letter. If the space is missing, the heading will be displayed in the HTML file as …\n#Heading 1\n\n\n\nUnordered and ordered lists\nYou can also include unordered lists and ordered lists. Click on the tabs below to see how they are incorporated\n\n\nunordered lists\nordered lists\nordered lists magic\n\n\n\nYou can add bullet points using either *, - or + and they will turn into:\n\nbullet point (created with *)\nbullet point (created with -)\nbullet point (created with +)\n\nor use bullet points of different levels using 1 tab key press or 2 spaces (for sub-item 1) or 2 tabs/4 spaces (for sub-sub-item 1):\n\nbullet point item 1\n\nsub-item 1\n\nsub-sub-item 1\nsub-sub-item 2\n\n\n\n\nbullet point item 2\n\n\n\n\n\n\n\nERROR: My bullet points don’t render properly when knitting\n\n\n\n\n\nYou need an empty row before your bullet points start. If I delete the empty row before the bullet points, they will be displayed in the HTML as …\nText without the empty row: * bullet point created with * - bullet point created with - + bullet point created with +\n\n\n\n\n\nStart the line with 1., 2., etc. When you want to include sub-items, either use the tab key twice or add 4 spaces. Same goes for the sub-sub-item: include either 2 tabs (or 4 manual spaces) from the last item or 4 tabs/ 8 spaces from the start of the line.\n\nlist item 1\nlist item 2\n\nsub-item 1 (with 4 spaces)\n\nsub-sub-item 1 (with an additional 4 spaces from the last indent)\n\n\n\n\n\n\n\n\n\n\n\nMy list items don’t render properly when knitting\n\n\n\n\n\nIf you don’t leave enough spaces, the list won’t be recognised, and your output looks like this:\n\nlist item 3\n\n\nsub-item 1 (with only 2 spaces) A. sub-sub-item 1 (with an additional 2 spaces from the last indent)\n\n\n\n\n\n\nThe great thing though is that you don’t need to know your alphabet or number sequences. R markdown will fix that for you\nIf I type into my .Rmd…\n\n…it will be rendered in the knitted HTML output as…\n\nlist item 3\nlist item 1\n\nsub-item labelled “a)”\nsub-item labelled “i)”\n\nsub-item labelled “C)”\nsub-item labelled “Z)”\n\n\n\n\nlist item 7\n\n\n\n\n\n\n\nERROR: The labels of the sub-items are not what I thought they would be. You said they are fixing themselves…\n\n\n\n\n\nYes, they do but you need to label your sub-item lists accordingly. The first label you list in each level is set as the baseline. If they are labelled 1) instead of i) or A., the output will show as follows, but the automatic-item-fixing still works:\n\nlist item 7\n\nlist item “1)” with 4 spaces\n\nlist item “1)” with 8 spaces\nthis is an item labelled “6)” (magically corrected to “2.”)\n\n\n\n\n\n\n\n\n\n\n\nEmphasis\nInclude emphasis to draw attention to keywords in your text:\n\n\nR markdown syntax\nDisplayed in the knitted HTML file\n\n\n\n**bold text**\nbold text\n\n\n*italic text*\nitalic text\n\n\n***bold and italic***\nbold and italic\n\n\n\nOther examples can be found in the R Markdown Cheat Sheet\n\n1.3.2 Code chunks\nEverything you write inside the code chunks will be interpreted as code and executed by R. Code chunks start with ``` followed by an {r} which specifies the coding language R, some space for code, and ends with ```. If you accidentally delete one of those backticks, your code won’t run and/or your text parts will be interpreted as part of the code chunks or vice versa. This should be evident from the colour change - more white than expected typically indicates missing starting backticks, whilst too much grey/not enough white suggests missing ending backticks. But no need to fret if that happens - just add the missing backticks manually.\nYou can insert a new code chunk in several ways:\n\nClick the Insert a new code chunk button in the RStudio Toolbar (green icon at the top right corner of the Source pane).\nSelect Code &gt; Insert Chunk from the menu.\nUsing the shortcut Ctrl + Alt + I for Windows or Cmd + Option + I on MacOSX.\nType ```{r} and ``` manually\n\n\n\n\n\nFigure 1.1: Default .Rmd with highlighting - names in pink and knitr display options in purple\n\n\n\nWithin the curly brackets of a code chunk, you can specify a name for the code chunk (see pink highlighting in Figure 1.1). The chunk name is not necessarily required; however, it is good practice to give each chunk a unique name to support more advanced knitting approaches. It also makes it easier to reference and manage chunks.\nWithin the curly brackets, you can also place rules and arguments (see purple highlighting in Figure 1.1) to control how your code is executed and what is displayed in your final HTML output. The most common knitr display options include:\n\n\n\n\n\n\n\n\nCode\nDoes code run\nDoes code show\nDo results show\n\n\n\neval=FALSE\nNO\nYES\nNO\n\n\necho=TRUE (default)\nYES\nYES\nYES\n\n\necho=FALSE\nYES\nNO\nYES\n\n\nresults=‘hide’\nYES\nYES\nNO\n\n\ninclude=FALSE\nYES\nNO\nNO\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe table above will be incredibly important for the data skills homework II. When solving error mode items you will need to pay attention to the first one eval = FALSE.\n\n\nOne last thing: In your newly created .Rmd file, delete everything below line 12 (keep the set-up code chunk) and save your .Rmd by clicking on the disc symbol.\n\n\nDelete everything below line 12\n\n\n\n\n\n\n\nYour Turn\n\n\n\nThat was quite a long section about what Markdown can do. I promise, we’ll practice that more later. For the minute, we want you to create a new level 2 heading on line 12 and give it a meaningful heading title (something like “Loading packages and reading in data” or “Chapter 1”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOn line 12, you should have typed ## Loading packages and reading in data (or whatever meaningful title you chose). This will create level 2 heading once we knit the .Rmd."
  },
  {
    "objectID": "01-basics.html#sec-download_data_ch1",
    "href": "01-basics.html#sec-download_data_ch1",
    "title": "1  Projects and R Markdown",
    "section": "\n1.4 Activity 3: Download the data",
    "text": "1.4 Activity 3: Download the data\nThe data for chapters 1-3. Download it here: data_ch1.zip. There are 2 csv files contained in a zip folder. One is the data file we are going to use today prp_data_reduced.csv and the other is an Excel file prp_codebook that explains the variables in the data.\nThe first step is to unzip the zip folder so that the files are placed within the same folder as your project.\n\nPlace the zip folder within your 2A_chapter1 folder\nRight mouse click –&gt; Extract All...\n\nCheck the folder location is the one to extract the files to\nCheck the extracted files are placed next to the project icon\nFiles and project should be visible in the Output pane in RStudio\n\n\n\n\n\n\n\nFor screenshots click here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnzipping a zip folder\n\n\n\n\n\n\nThe paper by Pownall et al. was a registered report published in 2023, and the original data can be found on OSF (https://osf.io/5qshg/).\nCitation\n\nPownall, M., Pennington, C. R., Norris, E., Juanchich, M., Smailes, D., Russell, S., Gooch, D., Evans, T. R., Persson, S., Mak, M. H. C., Tzavella, L., Monk, R., Gough, T., Benwell, C. S. Y., Elsherif, M., Farran, E., Gallagher-Mitchell, T., Kendrick, L. T., Bahnmueller, J., . . . Clark, K. (2023). Evaluating the Pedagogical Effectiveness of Study Preregistration in the Undergraduate Dissertation. Advances in Methods and Practices in Psychological Science, 6(4). https://doi.org/10.1177/25152459231202724\n\nAbstract\n\nResearch shows that questionable research practices (QRPs) are present in undergraduate final-year dissertation projects. One entry-level Open Science practice proposed to mitigate QRPs is “study preregistration,” through which researchers outline their research questions, design, method, and analysis plans before data collection and/or analysis. In this study, we aimed to empirically test the effectiveness of preregistration as a pedagogic tool in undergraduate dissertations using a quasi-experimental design. A total of 89 UK psychology students were recruited, including students who preregistered their empirical quantitative dissertation (n = 52; experimental group) and students who did not (n = 37; control group). Attitudes toward statistics, acceptance of QRPs, and perceived understanding of Open Science were measured both before and after dissertation completion. Exploratory measures included capability, opportunity, and motivation to engage with preregistration, measured at Time 1 only. This study was conducted as a Registered Report; Stage 1 protocol: https://osf.io/9hjbw (date of in-principle acceptance: September 21, 2021). Study preregistration did not significantly affect attitudes toward statistics or acceptance of QRPs. However, students who preregistered reported greater perceived understanding of Open Science concepts from Time 1 to Time 2 compared with students who did not preregister. Exploratory analyses indicated that students who preregistered reported significantly greater capability, opportunity, and motivation to preregister. Qualitative responses revealed that preregistration was perceived to improve clarity and organization of the dissertation, prevent QRPs, and promote rigor. Disadvantages and barriers included time, perceived rigidity, and need for training. These results contribute to discussions surrounding embedding Open Science principles into research training.\n\nChanges made to the dataset\nWe made some changes to the dataset for the purpose of increasing difficulty for data wrangling (Chapter 2 and Chapter 3) and data visualisation (Chapter 4 and Chapter 5). This will ensure some “teachable moments”. The changes are as follows:\n\nWe removed some of the variables to make the data more manageable for teaching purposes.\nWe recoded some values from numeric responses to labels (e.g., understanding).\nWe added the word “years” to one of the Age entries.\nWe tidied a messy column Ethnicity but introduced a similar but easier-to-solve “messiness pattern” when recoding the understanding data.\nThe scores in the original file were already corrected from reverse-coded responses. We reversed that process to present raw data here."
  },
  {
    "objectID": "01-basics.html#activity-4-loading-packages-and-reading-in-data",
    "href": "01-basics.html#activity-4-loading-packages-and-reading-in-data",
    "title": "1  Projects and R Markdown",
    "section": "\n1.5 Activity 4: Loading packages and reading in data",
    "text": "1.5 Activity 4: Loading packages and reading in data\nThe first step is to load in the packages we need and read in the data. Today, we’ll only be using tidyverse, and read_csv() will help us store the data from prp_data_reduced.csv in an object called data_prp.\nCopy the code into a code chunk in your .Rmd file and run it. You can either click the green error to run the entire code chunk, or use the shortcut Ctrl + Enter (Windows) or Cmd + Enter (Mac) to run a line of code/ pipe from the Rmd.\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nRows: 89 Columns: 91\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (17): Code, Age, Ethnicity, Opptional_mod_1_TEXT, Research_exp_1_TEXT, U...\ndbl (74): Gender, Secondyeargrade, Opptional_mod, Research_exp, Plan_prereg,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "01-basics.html#sec-familiarise",
    "href": "01-basics.html#sec-familiarise",
    "title": "1  Projects and R Markdown",
    "section": "\n1.6 Activity 5: Familiarise yourself with the data",
    "text": "1.6 Activity 5: Familiarise yourself with the data\n\nLook at the Codebook to get a feel of the variables in the dataset and how they have been measured. Note that some of the columns were deleted in the dataset you have been given.\nYou’ll notice that some questionnaire data was collected at 2 different time points (i.e., SATS28, QRPs, Understanding_OS)\nsome of the data was only collected at one time point (i.e., supervisor judgements, OS_behav items, and Included_prereg variables are t2-only variables)\n\n\n1.6.1 First glimpse at the data\nBefore you start wrangling your data, it is important to understand what kind of data you’re working with and what the format of your dataframe looks like.\nAs you may have noticed, read_csv() provides a message listing the data types in your dataset and how many columns are of each type. Plus, it shows a few examples columns for each data type.\nTo obtain more detailed information about your data, you have several options. Click on the individual tabs to see the different options available. Test them out in your own .Rmd file and use whichever method you prefer (but do it).\n\n\n\n\n\n\nWarning\n\n\n\nSome of the output is a bit long because we do have quite a few variables in the data file.\n\n\n\n\nvisual inspection 1\nglimpse()\nspec()\nvisual inspection 2\n\n\n\nIn the Global Environment, click the blue arrow icon next to the object name data_prp. This action will expand the object, revealing details about its columns. The $ symbol is commonly used in Base R to access a specific column within your dataframe.\n\n\nVisual inspection of the data\n\nCon: When you have quite a few variables, not all of them are shown.\n\n\nUse glimpse() if you want a more detailed overview you can see on your screen. The output will display rows and column numbers, and some examples of the first couple of observations for each variable.\n\nglimpse(data_prp)\n\nRows: 89\nColumns: 91\n$ Code                                  &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", …\n$ Gender                                &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,…\n$ Age                                   &lt;chr&gt; \"22\", \"20\", \"22\", \"26\", \"22\", \"2…\n$ Ethnicity                             &lt;chr&gt; \"White European\", \"White British…\n$ Secondyeargrade                       &lt;dbl&gt; 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1,…\n$ Opptional_mod                         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ Opptional_mod_1_TEXT                  &lt;chr&gt; \"Research methods in first year\"…\n$ Research_exp                          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Research_exp_1_TEXT                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Plan_prereg                           &lt;dbl&gt; 1, 3, 1, 2, 1, 1, 3, 3, 2, 2, 2,…\n$ SATS28_1_Affect_Time1                 &lt;dbl&gt; 4, 5, 5, 6, 2, 1, 6, 3, 2, 5, 2,…\n$ SATS28_2_Affect_Time1                 &lt;dbl&gt; 5, 6, 3, 3, 6, 1, 2, 2, 7, 3, 4,…\n$ SATS28_3_Affect_Time1                 &lt;dbl&gt; 3, 2, 5, 2, 6, 7, 2, 6, 6, 5, 2,…\n$ SATS28_4_Affect_Time1                 &lt;dbl&gt; 4, 5, 2, 2, 6, 6, 5, 5, 5, 5, 2,…\n$ SATS28_5_Affect_Time1                 &lt;dbl&gt; 5, 5, 5, 6, 1, 1, 5, 1, 2, 5, 2,…\n$ SATS28_6_Affect_Time1                 &lt;dbl&gt; 5, 6, 2, 5, 6, 7, 4, 5, 5, 3, 5,…\n$ SATS28_7_CognitiveCompetence_Time1    &lt;dbl&gt; 4, 2, 2, 5, 6, 7, 2, 5, 5, 2, 2,…\n$ SATS28_8_CognitiveCompetence_Time1    &lt;dbl&gt; 2, 2, 2, 1, 6, 7, 2, 5, 3, 2, 3,…\n$ SATS28_9_CognitiveCompetence_Time1    &lt;dbl&gt; 2, 2, 2, 3, 3, 7, 2, 6, 3, 3, 1,…\n$ SATS28_10_CognitiveCompetence_Time1   &lt;dbl&gt; 6, 7, 6, 6, 4, 2, 6, 4, 5, 6, 5,…\n$ SATS28_11_CognitiveCompetence_Time1   &lt;dbl&gt; 4, 3, 5, 5, 3, 1, 6, 2, 5, 6, 5,…\n$ SATS28_12_CognitiveCompetence_Time1   &lt;dbl&gt; 3, 5, 3, 5, 5, 7, 3, 4, 7, 2, 3,…\n$ SATS28_13_Value_Time1                 &lt;dbl&gt; 1, 1, 2, 1, 3, 7, 1, 2, 1, 2, 4,…\n$ SATS28_14_Value_Time1                 &lt;dbl&gt; 7, 7, 6, 6, 5, 1, 6, 5, 7, 6, 2,…\n$ SATS28_15_Value_Time1                 &lt;dbl&gt; 7, 7, 6, 6, 3, 5, 6, 6, 6, 5, 5,…\n$ SATS28_16_Value_Time1                 &lt;dbl&gt; 2, 1, 3, 2, 6, 5, 3, 7, 2, 2, 2,…\n$ SATS28_17_Value_Time1                 &lt;dbl&gt; 1, 1, 3, 3, 7, 7, 2, 7, 2, 2, 5,…\n$ SATS28_18_Value_Time1                 &lt;dbl&gt; 3, 6, 5, 3, 1, 1, 5, 1, 5, 2, 2,…\n$ SATS28_19_Value_Time1                 &lt;dbl&gt; 3, 3, 3, 3, 7, 7, 4, 5, 3, 5, 6,…\n$ SATS28_20_Value_Time1                 &lt;dbl&gt; 2, 1, 4, 2, 7, 7, 2, 4, 2, 2, 7,…\n$ SATS28_21_Value_Time1                 &lt;dbl&gt; 2, 1, 3, 2, 6, 7, 2, 5, 1, 3, 5,…\n$ SATS28_22_Difficulty_Time1            &lt;dbl&gt; 3, 2, 5, 3, 2, 1, 4, 2, 2, 5, 3,…\n$ SATS28_23_Difficulty_Time1            &lt;dbl&gt; 5, 6, 5, 6, 6, 7, 4, 6, 7, 5, 6,…\n$ SATS28_24_Difficulty_Time1            &lt;dbl&gt; 2, 2, 2, 3, 1, 4, 4, 2, 2, 2, 2,…\n$ SATS28_25_Difficulty_Time1            &lt;dbl&gt; 6, 7, 5, 5, 6, 7, 5, 6, 5, 5, 5,…\n$ SATS28_26_Difficulty_Time1            &lt;dbl&gt; 4, 2, 2, 2, 6, 7, 4, 5, 3, 5, 3,…\n$ SATS28_27_Difficulty_Time1            &lt;dbl&gt; 4, 5, 5, 3, 6, 7, 4, 3, 5, 3, 6,…\n$ SATS28_28_Difficulty_Time1            &lt;dbl&gt; 1, 7, 5, 5, 6, 6, 5, 4, 4, 4, 2,…\n$ QRPs_1_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 6, 2, 7, 6, 7,…\n$ QRPs_2_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 6, 7, 7, 7, 5,…\n$ QRPs_3_Time1                          &lt;dbl&gt; 5, 2, 6, 2, 6, 4, 6, 3, 7, 3, 3,…\n$ QRPs_4_Time1                          &lt;dbl&gt; 7, 7, 6, 6, 7, 4, 6, 7, 7, 7, 6,…\n$ QRPs_5_Time1                          &lt;dbl&gt; 3, 3, 7, 7, 2, 7, 4, 6, 7, 3, 2,…\n$ QRPs_6_Time1                          &lt;dbl&gt; 4, 7, 6, 5, 7, 4, 4, 5, 7, 6, 5,…\n$ QRPs_7_Time1                          &lt;dbl&gt; 5, 7, 7, 7, 7, 4, 5, 6, 7, 7, 5,…\n$ QRPs_8_Time1                          &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 2, 7,…\n$ QRPs_9_Time1                          &lt;dbl&gt; 6, 7, 7, 4, 7, 7, 3, 7, 6, 6, 2,…\n$ QRPs_10_Time1                         &lt;dbl&gt; 7, 6, 5, 2, 5, 4, 2, 6, 7, 7, 2,…\n$ QRPs_11_Time1                         &lt;dbl&gt; 7, 7, 7, 4, 7, 7, 4, 6, 7, 7, 5,…\n$ QRPs_12NotQRP_Time1                   &lt;dbl&gt; 2, 2, 1, 4, 1, 4, 2, 4, 2, 2, 1,…\n$ QRPs_13NotQRP_Time1                   &lt;dbl&gt; 1, 1, 1, 1, 1, 4, 2, 4, 1, 1, 1,…\n$ QRPs_14NotQRP_Time1                   &lt;dbl&gt; 1, 4, 3, 4, 1, 4, 2, 3, 3, 4, 3,…\n$ QRPs_15NotQRP_Time1                   &lt;dbl&gt; 2, 4, 2, 2, 1, 4, 2, 1, 4, 4, 2,…\n$ Understanding_OS_1_Time1              &lt;chr&gt; \"2\", \"2\", \"6\", \"2\", \"6\", \"Not at…\n$ Understanding_OS_2_Time1              &lt;chr&gt; \"2\", \"Not at all confident\", \"2\"…\n$ Understanding_OS_3_Time1              &lt;chr&gt; \"2\", \"Not at all confident\", \"3\"…\n$ Understanding_OS_4_Time1              &lt;chr&gt; \"6\", \"Not at all confident\", \"6\"…\n$ Understanding_OS_5_Time1              &lt;chr&gt; \"Entirely confident\", \"6\", \"6\", …\n$ Understanding_OS_6_Time1              &lt;chr&gt; \"Entirely confident\", \"Entirely …\n$ Understanding_OS_7_Time1              &lt;chr&gt; \"6\", \"Not at all confident\", \"2\"…\n$ Understanding_OS_8_Time1              &lt;chr&gt; \"6\", \"3\", \"5\", \"3\", \"5\", \"Not at…\n$ Understanding_OS_9_Time1              &lt;chr&gt; \"Entirely confident\", \"6\", \"5\", …\n$ Understanding_OS_10_Time1             &lt;chr&gt; \"Entirely confident\", \"6\", \"5\", …\n$ Understanding_OS_11_Time1             &lt;chr&gt; \"Entirely confident\", \"2\", \"4\", …\n$ Understanding_OS_12_Time1             &lt;chr&gt; \"Entirely confident\", \"2\", \"5\", …\n$ Pre_reg_group                         &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,…\n$ Other_OS_behav_2                      &lt;dbl&gt; 1, NA, NA, NA, 1, NA, NA, 1, NA,…\n$ Other_OS_behav_4                      &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, N…\n$ Other_OS_behav_5                      &lt;dbl&gt; NA, NA, NA, NA, 1, 1, NA, NA, NA…\n$ Closely_follow                        &lt;dbl&gt; 2, 2, 2, NA, 3, 3, 3, NA, NA, 2,…\n$ SATS28_Affect_Time2_mean              &lt;dbl&gt; 3.500000, 3.166667, 4.833333, 4.…\n$ SATS28_CognitiveCompetence_Time2_mean &lt;dbl&gt; 4.166667, 4.666667, 6.166667, 5.…\n$ SATS28_Value_Time2_mean               &lt;dbl&gt; 3.000000, 6.222222, 6.000000, 4.…\n$ SATS28_Difficulty_Time2_mean          &lt;dbl&gt; 2.857143, 2.857143, 4.000000, 2.…\n$ QRPs_Acceptance_Time2_mean            &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.…\n$ Time2_Understanding_OS                &lt;dbl&gt; 5.583333, 3.333333, 5.416667, 4.…\n$ Supervisor_1                          &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 6, 7, 5, 6,…\n$ Supervisor_2                          &lt;dbl&gt; 5, 6, 7, 4, 6, 2, 7, 5, 6, 5, 5,…\n$ Supervisor_3                          &lt;dbl&gt; 6, 7, 7, 1, 7, 1, 7, 5, 6, 6, 7,…\n$ Supervisor_4                          &lt;dbl&gt; 6, 7, 7, 1, 7, 1, 7, 6, 7, 6, 6,…\n$ Supervisor_5                          &lt;dbl&gt; 5, 7, 7, 4, 7, 3, 7, 7, 6, 6, 6,…\n$ Supervisor_6                          &lt;dbl&gt; 5, 7, 7, 4, 6, 3, 7, 6, 7, 6, 6,…\n$ Supervisor_7                          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Supervisor_8                          &lt;dbl&gt; 5, 5, 7, 1, 7, 1, 7, 5, 7, 5, 6,…\n$ Supervisor_9                          &lt;dbl&gt; 6, 7, 7, 4, 7, 3, 7, 5, 7, 6, 7,…\n$ Supervisor_10                         &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 6, 7, 6, 6,…\n$ Supervisor_11                         &lt;dbl&gt; NA, 7, 7, NA, 7, 1, 7, 5, 7, 6, …\n$ Supervisor_12                         &lt;dbl&gt; 4, 5, 7, 1, 4, 1, 7, 3, 6, 6, 5,…\n$ Supervisor_13                         &lt;dbl&gt; 4, 2, 5, 1, 2, 1, 6, 3, 5, 6, 5,…\n$ Supervisor_14                         &lt;dbl&gt; 5, 7, 7, 1, 7, 1, 7, 5, 7, 6, 6,…\n$ Supervisor_15_R                       &lt;dbl&gt; 1, 1, 1, 4, 1, 7, 1, 2, 1, 2, 1,…\n\n\n\n\nYou can also use spec() as suggested in the message above and then it shows you a list of the data type in every single column. But it doesn’t show you the number of rows and columns.\n\nspec(data_prp)\n\ncols(\n  Code = col_character(),\n  Gender = col_double(),\n  Age = col_character(),\n  Ethnicity = col_character(),\n  Secondyeargrade = col_double(),\n  Opptional_mod = col_double(),\n  Opptional_mod_1_TEXT = col_character(),\n  Research_exp = col_double(),\n  Research_exp_1_TEXT = col_character(),\n  Plan_prereg = col_double(),\n  SATS28_1_Affect_Time1 = col_double(),\n  SATS28_2_Affect_Time1 = col_double(),\n  SATS28_3_Affect_Time1 = col_double(),\n  SATS28_4_Affect_Time1 = col_double(),\n  SATS28_5_Affect_Time1 = col_double(),\n  SATS28_6_Affect_Time1 = col_double(),\n  SATS28_7_CognitiveCompetence_Time1 = col_double(),\n  SATS28_8_CognitiveCompetence_Time1 = col_double(),\n  SATS28_9_CognitiveCompetence_Time1 = col_double(),\n  SATS28_10_CognitiveCompetence_Time1 = col_double(),\n  SATS28_11_CognitiveCompetence_Time1 = col_double(),\n  SATS28_12_CognitiveCompetence_Time1 = col_double(),\n  SATS28_13_Value_Time1 = col_double(),\n  SATS28_14_Value_Time1 = col_double(),\n  SATS28_15_Value_Time1 = col_double(),\n  SATS28_16_Value_Time1 = col_double(),\n  SATS28_17_Value_Time1 = col_double(),\n  SATS28_18_Value_Time1 = col_double(),\n  SATS28_19_Value_Time1 = col_double(),\n  SATS28_20_Value_Time1 = col_double(),\n  SATS28_21_Value_Time1 = col_double(),\n  SATS28_22_Difficulty_Time1 = col_double(),\n  SATS28_23_Difficulty_Time1 = col_double(),\n  SATS28_24_Difficulty_Time1 = col_double(),\n  SATS28_25_Difficulty_Time1 = col_double(),\n  SATS28_26_Difficulty_Time1 = col_double(),\n  SATS28_27_Difficulty_Time1 = col_double(),\n  SATS28_28_Difficulty_Time1 = col_double(),\n  QRPs_1_Time1 = col_double(),\n  QRPs_2_Time1 = col_double(),\n  QRPs_3_Time1 = col_double(),\n  QRPs_4_Time1 = col_double(),\n  QRPs_5_Time1 = col_double(),\n  QRPs_6_Time1 = col_double(),\n  QRPs_7_Time1 = col_double(),\n  QRPs_8_Time1 = col_double(),\n  QRPs_9_Time1 = col_double(),\n  QRPs_10_Time1 = col_double(),\n  QRPs_11_Time1 = col_double(),\n  QRPs_12NotQRP_Time1 = col_double(),\n  QRPs_13NotQRP_Time1 = col_double(),\n  QRPs_14NotQRP_Time1 = col_double(),\n  QRPs_15NotQRP_Time1 = col_double(),\n  Understanding_OS_1_Time1 = col_character(),\n  Understanding_OS_2_Time1 = col_character(),\n  Understanding_OS_3_Time1 = col_character(),\n  Understanding_OS_4_Time1 = col_character(),\n  Understanding_OS_5_Time1 = col_character(),\n  Understanding_OS_6_Time1 = col_character(),\n  Understanding_OS_7_Time1 = col_character(),\n  Understanding_OS_8_Time1 = col_character(),\n  Understanding_OS_9_Time1 = col_character(),\n  Understanding_OS_10_Time1 = col_character(),\n  Understanding_OS_11_Time1 = col_character(),\n  Understanding_OS_12_Time1 = col_character(),\n  Pre_reg_group = col_double(),\n  Other_OS_behav_2 = col_double(),\n  Other_OS_behav_4 = col_double(),\n  Other_OS_behav_5 = col_double(),\n  Closely_follow = col_double(),\n  SATS28_Affect_Time2_mean = col_double(),\n  SATS28_CognitiveCompetence_Time2_mean = col_double(),\n  SATS28_Value_Time2_mean = col_double(),\n  SATS28_Difficulty_Time2_mean = col_double(),\n  QRPs_Acceptance_Time2_mean = col_double(),\n  Time2_Understanding_OS = col_double(),\n  Supervisor_1 = col_double(),\n  Supervisor_2 = col_double(),\n  Supervisor_3 = col_double(),\n  Supervisor_4 = col_double(),\n  Supervisor_5 = col_double(),\n  Supervisor_6 = col_double(),\n  Supervisor_7 = col_double(),\n  Supervisor_8 = col_double(),\n  Supervisor_9 = col_double(),\n  Supervisor_10 = col_double(),\n  Supervisor_11 = col_double(),\n  Supervisor_12 = col_double(),\n  Supervisor_13 = col_double(),\n  Supervisor_14 = col_double(),\n  Supervisor_15_R = col_double()\n)\n\n\n\n\nIn the Global Environment, click on the object name data_prp. This action will open the data in a new tab. Hovering over the column headings with your mouse will also reveal their data type. However, it seems to be a fairly tedious process when you have loads of columns.\n\n\n\n\n\n\nHang on, where is the rest of my data? Why do I only see 50 columns?\n\n\n\n\n\nOne common source of confusion is not seeing all your columns when you open up a data object as a tab. This is because RStudio shows you a maximum of 50 columns at a time. If you have more than 50 columns, navigate with the arrows to see the remaining columns.\n\n\nShowing 50 columns at a time\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nNow that you have tested out all the options in your own .Rmd file, you can probably answer the following questions:\n\nHow many observations? \n\nHow many variables? \n\nHow many columns are col_character or chr data type? \n\nHow many columns are col_double or dbl data type? \n\n\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThe visual inspections shows you the number of observations and variables. glimpse() also gives you that information but calls them rows and columns respectively.\nThe data type information actually comes from the output when using the read_csv() function. Did you notice the information on Column specification (see screenshot below)?\n\n\nmessage from read_csv() when reading in the data\n\nWhilst spec() is quite useful for data type information per individual column, it doesn’t give you the total count of each data type. So it doesn’t really help with answering the questions here - unless you want to count manually from its extremely long output.\n\n\n\nIn your .Rmd, include a new heading level 2 called “Information about the data” (or something equally meaningful) and jot down some notes about data_prp. You could include the citation and/or the abstract, and whatever information you think you should note about this dataset (e.g., any observations from looking at the codebook?). You could also include some notes on the functions used so far and what they do. Try to incorporate some bold, italic or bold and italic emphasis and perhaps a bullet point or two.\n\n\n\n\n\n\nPossible solution\n\n\n\n\n\n## Information about the data\nThe data is from Pownall et al. (2023), and I can find the paper here: https://doi.org/10.1177/25152459231202724.\nI’ve noticed in the prp codebook that the SATS-28 questionnaire has quite a few *reverse-coded items*, and the supervisor support questionnaire also has a reverse-coded item.\nSo far, I think I prefer **glimpse()** to show me some more detail about the data. Specs() is too text-heavy for me which makes it hard to read.\nThings to keep in mind:\n* **don’t forget to load in tidyverse first!!!**\n* always read in the data with **read_csv**, ***never ever use read.csv***!!!\n\n\nThe output rendered in a knitted html file\n\n\n\n\n\n\n\n1.6.2 Data types\nEach variable has a data type, such as numeric (numbers), character (text), and logical (TRUE/FALSE values), or a special class of factor. As you have just seen, our data_prp only has character and numeric columns (so far).\nNumeric data can be double (dbl) or integer (int). Doubles can have decimal places (e.g., 1.1). Integers are the whole numbers (e.g., 1, 2, -1) and are displayed with the suffix L (e.g., 1L). This is not overly important but might leave you less puzzled the next time you see an L after a number.\nCharacters (also called “strings”) is anything written between quotation marks. This is usually text, but in special circumstances, a number can be a character if it placed within quotation marks. This can happen when you are recoding variables. It might not be too obvious at the time, but you won’t be able to calculate anything if the number is a character\n\n\nExample data types\nnumeric computation\ncharacter computation\n\n\n\n\ntypeof(1)\ntypeof(1L)\ntypeof(\"1\")\ntypeof(\"text\")\n\n[1] \"double\"\n[1] \"integer\"\n[1] \"character\"\n[1] \"character\"\n\n\n\n\nNo problems here…\n\n1+1\n\n[1] 2\n\n\n\n\nWhen the data type is incorrect, you won’t be able to compute anything, despite your numbers being shown as numeric values in the dataframe. The error message tells you exactly what’s wrong with it, i.e., that you have non-numeric arguments.\n\n\"1\"+\"1\" # ERROR\n\nError in \"1\" + \"1\": non-numeric argument to binary operator\n\n\n\n\n\nLogical data (also sometimes called “Boolean” values) are one of two values: TRUE or FALSE (written in uppercase). They become really important when we use filter() or mutate() with conditional statements such as case_when(). More about those in Chapter 3.\nSome commonly used logical operators:\n\n\noperator\ndescription\n\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n%in%\nTRUE if any element is in the following vector\n\n\n\nA factor is a specific type of integer or character that lets you assign the order of the categories. This becomes useful when you want to display certain categories in “the correct order” either in a dataframe (see arrange) or when plotting (see Chapter 4/ Chapter 5).\n\n1.6.3 Variable types\nYou’ve already encountered them in Level 1 but let’s refresh. Variables can be classified as continuous (numbers) or categorical (labels).\nCategorical variables are properties you can count. They can be nominal, where the categories don’t have an order (e.g., gender) or ordinal (e.g., Likert scales either with numeric values 1-7 or with character labels such as “agree”, “neither agree nor disagree”, “disagree”). Categorical data may also be factors rather than characters.\nContinuous variables are properties you can measure and calculate sums/ means/ etc. They may be rounded to the nearest whole number, but it should make sense to have a value between them. Continuous variables always have a numeric data type (i.e. integer or double).\n\n\n\n\n\n\nWhy is this important you may ask?\n\n\n\nKnowing your variable and data types will help later on when deciding on an appropriate plot (see Chapter 4 and Chapter 5) or which inferential test to run (Chapter 6 to Chapter 13).\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAs we’ve seen earlier, data_prp only had character and numeric variables which hardly tests your understanding to see if you can identify a variety of data types and variable types. So, for this little quiz, we’ve spiced it up a bit. We’ve selected a few columns, shortened some of the column names, and modified some of the data types. Here you can see the first few rows of the new object data_quiz. You can find the code with explanations at the end of this section.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nAge\nGender\nEthnicity\nSecondyeargrade\nQRP_item\nQRPs_mean\nUnderstanding_item\nQRP_item &gt; 4\n\n\n\nTr10\n22\n2\nWhite European\n60-69% (2:1 grade)\n5\n5.636364\n2\nTRUE\n\n\nBi07\n20\n2\nWhite British\n50-59% (2:2 grade)\n2\n5.454546\n2\nFALSE\n\n\nSK03\n22\n2\nWhite British\n≥ 70% (1st class grade)\n6\n6.272727\n6\nTRUE\n\n\nSM95\n26\n2\nWhite British\n60-69% (2:1 grade)\n2\n5.000000\n2\nFALSE\n\n\nSt01\n22\n2\nWhite British\n60-69% (2:1 grade)\n6\n5.545454\n6\nTRUE\n\n\n\n\n\n\n\nglimpse(data_quiz)\n\nRows: 89\nColumns: 9\n$ Code               &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", \"St01\", \"St10\", \"Wa…\n$ Age                &lt;chr&gt; \"22\", \"20\", \"22\", \"26\", \"22\", \"20\", \"21\", \"21\", \"22…\n$ Gender             &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Ethnicity          &lt;chr&gt; \"White European\", \"White British\", \"White British\",…\n$ Secondyeargrade    &lt;fct&gt; 60-69% (2:1 grade), 50-59% (2:2 grade), ≥ 70% (1st …\n$ QRP_item           &lt;dbl&gt; 5, 2, 6, 2, 6, 4, 6, 3, 7, 3, 3, 4, 4, 4, 4, 6, 3, …\n$ QRPs_mean          &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.000000, 5.545455, 6…\n$ Understanding_item &lt;chr&gt; \"2\", \"2\", \"6\", \"2\", \"6\", \"Not at all confident\", \"4…\n$ `QRP_item &gt; 4`     &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE,…\n\n\nSelect from the dropdown menu the variable type and their data types for each of the columns.\n\n\n\n\n\n\n\nColumn\nVariable type\nData type\n\n\n\nAge\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nGender\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nEthinicity\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nSecondyeargrade\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRP_item\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRPs_mean\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nUnderstanding_item\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\nQRP_item &gt; 4\n\ncontinuous\nnominal\nordinal\n\nnumeric\ncharacter\nlogical\nfactor\n\n\n\n\n\n\n\n\n\n\n\nRevealing the mystery code that created data_quiz\n\n\n\n\n\nThe code might look a bit complex for the minute despite the line-by-line explanations below. Come back to it after completing chapter 2.\n\ndata_quiz &lt;- data_prp %&gt;% \n  select(Code, Age, Gender, Ethnicity, Secondyeargrade, QRP_item = QRPs_3_Time1, QRPs_mean = QRPs_Acceptance_Time2_mean, Understanding_item = Understanding_OS_1_Time1) %&gt;% \n  mutate(Gender = factor(Gender),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         `QRP_item &gt; 4` = case_when(\n           QRP_item &gt; 4 ~ TRUE, \n           .default = FALSE))\n\nLets go through this line by line:\n\n\nline 1: creates a new object called data_quiz and it is based on the already existing data object data_prp\n\n\nline 2: we are selecting a few variables of interest, such as Code, Age etc. Some of those variables were renamed in the process according to the structure new_name = old_name, for example QRP item 3 at time point 1 got renamed as QRP_item.\n\n\nline 3: The function mutate() is used to create a new column called Gender that turns the existing column Gender from a numeric value into a factor. R simply overwrites the existing column of the same name. If we had named the new column Gender_factor, we would have been able to retain the original Gender column and Gender_factor would have been added as the last column.\n\nline 4-6: See how the line starts with an indent which indicates we are still within the mutate() function. You can also see this by counting brackets - in line 3 there are 2 opening brackets but only 1 closes.\n\nSimilar to Gender, we are replacing the “old” Secondyeargrade with the new Secondyeargrade column that is now a factor.\nTurning our variable Secondyeargrade into a factor, spot the difference between this attempt and the one we used for Gender? Here we are using a lot more arguments in that factor function, namely levels and labels. Levels describes the unique values we have for that column, and in labels we want to define how these levels will be shown in the data object. If you don’t add the levels and labels argument, the labels will be the labels (as you can see in the Gender column in which we kept the numbers).\n\n\n\nline 7: Doesn’t start with a function name and has an indent, which means we are still within the mutate() function - count the opening and closing brackets to confirm.\n\nHere, we are creating a new column called QRP_item &gt; 4. Notice the two backticks we have to use to make this weird column name work? This is because it has spaces (and we did mention that R doesn’t like spaces). So the backticks help R to group it as a unit/ a single name.\nNext we have a case_when() function which helps executing conditional statements. We are using it to check whether a statement is TRUE or FALSE. Here, we ask whether the QRP item (column QRP_item) is larger than 4 (midpoint of the scale) using the Boolean operator &gt;. If the statement is TRUE, the label TRUE should appear in column QRP_item &gt; 4. Otherwise, if the value is equal to 4 or smaller, the label should read FALSE. We will come back to conditional statements in Chapter 2. But long story short, this Boolean expression created the only logical data type in data_quiz.\n\n\n\n\n\n\nAnd with this, we are done with the individual walkthrough. Well done :)"
  },
  {
    "objectID": "01-basics.html#pair-coding",
    "href": "01-basics.html#pair-coding",
    "title": "1  Projects and R Markdown",
    "section": "Pair-coding",
    "text": "Pair-coding\nI want to produce a short, knitted document that the students have to recreate including some headings, emphasis in the text, bullet points and the first steps of calculation we can do with the loneliness data with no proper data wrangling skills"
  },
  {
    "objectID": "01-basics.html#test-your-knowledge",
    "href": "01-basics.html#test-your-knowledge",
    "title": "1  Projects and R Markdown",
    "section": "Test your knowledge",
    "text": "Test your knowledge\nAre you ready for some knowledge check questions to test your understanding of the chapter? We also have some faulty codes. See if you can spot what’s wrong with them.\nKnowledge check\nQuestion 1\nOne of the key first steps when we open RStudio is to:\n\nput on some music as we will be here a whileopen an existing project or create a new onemake a coffeecheck out the news\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nOpening an existing project (e.g., when coming back to the same dataset) or creating a new project (e.g., for a new task or new dataset) ensures that subsequent .Rmd files, any output, figures, etc are saved within the same folder on your computer (i.e., the working directory). If the.Rmd files or data is not in the same folder as “the project icon”, things can get messy and code might not run.\n\n\n\nQuestion 2\nWhen using the default environment colour settings for RStudio, what colour would the background of a code chunk be in R Markdown? \nred\nwhite\ngreen\ngrey\nWhen using the default environment colour settings for RStudio, what colour would the background of normal text be in R Markdown? \nred\nwhite\ngreen\ngrey\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nAssuming you have not changed any of the settings in RStudio, code chunks will tend to have a grey background and normal text will tend to have a white background. This is a good way to check that you have closed and opened code chunks correctly.\n\n\n\nQuestion 3\nCode chunks start and end with:\n\nthree single quotesthree backticksthree double quotesthree single asterisks\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nCode chunks always take the same general format of three backticks followed by curly parentheses and a lower case r inside the parentheses ({r}). People often mistake these backticks for single quotes but that will not work. If you have set your code chunk correctly using backticks, the background colour should change to grey from white.\n\n\n\nQuestion 4\nWhat is the correct way to include a code chunk in RMarkdown that will be executed but neither the code nor its output will be shown in the final HTML document? \n{r, echo=FALSE}\n{r, eval=FALSE}\n{r, include=FALSE}\n{r, results=‘hide’}\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nCheck the table of knitr display options in Section 1.3.2.\n\n{r, echo=FALSE} also executes the code and does not show the code, but it does display the result in the knitted html file. (matches 2/3 criteria)\n{r, eval=FALSE} does not show the results but does not execute the code and it does show it in the knitted file. (matches 1/3 criteria)\n{r, results=“hide”} executes the code and does not show results, however, it does include the code in the knitted html document. (matches 2/3 criteria)\n\n\n\n\nError mode\nSome of these codes have mistakes in them, other code chunks are not quite producing what was aimed for. Your task is to spot anything faulty, explain why the things happened, and perhaps try to fix them.\nQuestion 5\nYou want to read in data with the read_csv() function. You have just stated R, created a new .Rmd file, and typed the following code into your code chunk.\n\ndata &lt;- read_csv(\"data.csv\")\n\nHowever, R gives you an error message: could not find function \"read_csv\". What could be the reason?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\n“Could not find function” is an indication that you have forgotten to load in tidyverse. Because read_csv() is a function in the tidyverse collection, R cannot find it.\nFIX: Add library(tidyverse) prior to reading in the data and run the code chunk again.\n\n\n\nQuestion 6\nYou want to read in data with the read_csv() function. This time, you are certain you have loaded in tidyverse first. The code is as follows:\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data.csv\")\n\nThe error message shows 'data.csv' does not exist in current working directory. You check your folder and it looks like this:\n\nWhy is there an error message?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nR is looking for a csv file that is called data which is currently not in the working directory. We may assume it’s in the data folder. Perhaps that happened when unzipping the zip file. So instead of placing the csv file on the same level as the project icon, it was unzipped into a folder named data.\nFIX - option 1: Take the data.csv out of the data folder and place it next to the project icon and the .Rmd file.\nFIX - option 2: Modify your R code to tell R that the data is in a separate folder called data, e.g., …\n\nlibrary(tidyverse)\ndata &lt;- read_csv(\"data/data.csv\")\n\n\n\n\nQuestion 7\nYou want to load tidyverse into the library. The code is as follows:\n\nlibrary(tidyverse)\n\nThe error message says: Error in library(tidyverse) : there is no package called ‘tidyverse’\nWhy is there an error message and how can we fix this?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nIf R says there is no package called tidyverse, means you haven’t installed the package yet. This could be an error message you receive either after switching computers or a fresh install of R and RStudio.\nFIX: Type install.packages(\"tidyverse\") into your Console.\n\n\n\nQuestion 8\nYou knitted your .Rmd into a html but the output is not as expected. You see the following:\n\nWhy did the file not knit properly?\n\n\n\n\n\n\nExplain the solution\n\n\n\n\n\nThere is a backtick missing in the code chunk. If you check your .Rmd file, you can see that the code chunk does not show up in grey which means it’s one of the 3 backticks at the beginning of the chunk.\n\nFIX: Add a single backtick manually where it’s missing."
  },
  {
    "objectID": "02-wrangling.html#intended-learning-outcomes",
    "href": "02-wrangling.html#intended-learning-outcomes",
    "title": "2  Data wrangling I",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nIn the next two chapters, we will build on the data wrangling skills from level 1. We will revisit all the functions you have already encountered (and might have forgotten over the summer break) and introduce 2 or 3 new functions. These two chapters will provide an opportunity to revise and apply the functions to a novel dataset.\nBy the end of this chapter, you should be able to:\n\napply familiar data wrangling functions to novel datasets\nread and interpret error messages\nrealise there are several ways of getting to the results\nexport data objects as csv files\n\nThe main purpose of this chapter and Chapter 3 is to wrangle your data into shape for data visualisation (Chapter 4 and Chapter 5). For the two chapters, we will:\n\ncalculate demographics\ntidy 3 different questionnaires with varying degrees of complexity\nsolve an error mode problem\njoin all data objects together"
  },
  {
    "objectID": "02-wrangling.html#individual-walkthrough",
    "href": "02-wrangling.html#individual-walkthrough",
    "title": "2  Data wrangling I",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough\nBefore we start, we need to set up some things."
  },
  {
    "objectID": "02-wrangling.html#activity-1-setup",
    "href": "02-wrangling.html#activity-1-setup",
    "title": "2  Data wrangling I",
    "section": "\n2.1 Activity 1: Setup",
    "text": "2.1 Activity 1: Setup\n\nWe will be working on the dataset by Pownall et al. (2023) again, which means we can still use the project we created last week. The data files will already be there, so no need to download them again.\nTo open the project in RStudio, go to the folder in which you stored the project and the data last time, and double click on the project icon.\n\nCreate a new .Rmd for chapter 2 and save it to your project folder. Name it something meaningful (e.g., “chapter_02”, “02_data_wrangling.Rmd”). See Section 1.3 if you need some guidance.\nIn your newly created .Rmd file, delete everything below line 12 (after the set-up code chunk)."
  },
  {
    "objectID": "02-wrangling.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "href": "02-wrangling.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "title": "2  Data wrangling I",
    "section": "\n2.2 Activity 2: Load in the libraries and read in the data",
    "text": "2.2 Activity 2: Load in the libraries and read in the data\nWe will use tidyverse today, and we want to create a data object data_prp that stores the data from the file prp_data_reduced.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(???)\ndata_prp &lt;- read_csv(\"???\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\n\n\n\n\nIf you need a quick reminder what the dataset was about, have a look at the abstract in Section 1.4. We also addressed the changes we made to the dataset there.\nAnd remember to have a quick glimpse() at your data."
  },
  {
    "objectID": "02-wrangling.html#activity-3-calculating-demographics",
    "href": "02-wrangling.html#activity-3-calculating-demographics",
    "title": "2  Data wrangling I",
    "section": "\n2.3 Activity 3: Calculating demographics",
    "text": "2.3 Activity 3: Calculating demographics\nLet’s start with some simple data wrangling steps to compute demographics for our original dataset, data_prp. First, we want to determine how many participants took part in the study by Pownall et al. (2023) and compute the mean age and the standard deviation of age for the sample.\n\n2.3.1 … for the full sample using summarise()\n\nThe summarise() function is part of the “Wickham 6” alongside group_by(), select(), filter(), mutate(), and arrange(). You used them plenty of times last year.\nWithin summarise(), we can use the n() function, which calculates the number of rows in the dataset. Since each row corresponds to a unique participant, this gives us the total number of participants.\nTo calculate the mean age and the standard deviation of age, we need to use the functions mean() and sd() on the column Age respectively.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age), # mean age\n            sd_age = sd(Age)) # standard deviation of age\n\nWarning: There were 2 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `mean_age = mean(Age)`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\nNA\nNA\n\n\n\n\n\nR did not give us an error message per se, but the output is not quite as expected either. There are NA values in the mean_age and sd_age columns. Looking at the warning message and at Age, can you explain what happened?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe warning message says: argument is not numeric or logical: returning NA If we look at the Age column more closely, we can see that it’s a character data type.\n\n\n\nFixing Age\n\nMight be wise to look at the unique answers in column Age to determine what is wrong. We can do that with the function distinct().\n\nage_distinct &lt;- data_prp %&gt;% \n  distinct(Age)\n\nage_distinct\n\n\n\n\n\n\n\nShow the unique values of Age.\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n\n22\n\n\n20\n\n\n26\n\n\n21\n\n\n29\n\n\n23\n\n\n39\n\n\nNA\n\n\n24\n\n\n43\n\n\n31\n\n\n25 years\n\n\n\n\n\n\n\n\n\n\n\nOne cell has the string “years” added to their number 25, which has converted the entire column into a character column.\nWe can easily fix this by extracting only the numbers from the column and converting it into a numeric data type. The parse_number() function, which is part of the tidyverse package, handles both steps in one go (so there’s no need to load additional packages).\nWe will combine this with the mutate() function to create a new column called Age (containing those numeric values), effectively replacing the old Age column (which had the character values).\n\n\n\nparse_number() illustration by Allison Horst (see https://allisonhorst.com/r-packages-functions)\n\n\n\n\ndata_prp &lt;- data_prp %&gt;% \n  mutate(Age = parse_number(Age))\n\ntypeof(data_prp$Age) # fixed\n\n[1] \"double\"\n\n\nComputing summary stats\nExcellent. Now that the numbers are in a numeric format, let’s try calculating the demographics for the total sample again.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age), # mean age\n            sd_age = sd(Age)) # standard deviation of age\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\nNA\nNA\n\n\n\n\n\nEven though there’s no error or warning, the table still shows NA values for mean_age and sd_age. So, what could possibly be wrong now?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nDid you notice that the Age column in age_distinct contains some missing values (NA)? To be honest, it’s easier to spot this issue in the actual R output than in the printed HTML page.\n\n\n\nComputing summary stats - third attempt\nTo ensure R ignores missing values during calculations, we need to add the extra argument na.rm = TRUE to the mean() and sd() functions.\n\ndemo_total &lt;- data_prp %&gt;% \n  summarise(n = n(), # participant number\n            mean_age = mean(Age, na.rm = TRUE), # mean age\n            sd_age = sd(Age, na.rm = TRUE)) # standard deviation of age\n\ndemo_total\n\n\n\n\nn\nmean_age\nsd_age\n\n\n89\n21.88506\n3.485603\n\n\n\n\n\nFinally, we’ve got it! 🥳 Third time’s the charm!\n\n2.3.2 … per gender using summarise() and group_by()\n\nNow we want to compute the summary statistics for each gender. The code inside the summarise() function remains unchanged; we just need to use the group_by() function beforehand to tell R that we want to compute the summary statistics for each group separately. It’s also a good practice to use ungroup() afterwards, so you are not taking groupings forward unintentionally.\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% # split data up into groups (here Gender)\n  summarise(n = n(), # participant number \n            mean_age = mean(Age, na.rm = TRUE), # mean age \n            sd_age = sd(Age, na.rm = TRUE)) %&gt;%  # standard deviation of age\n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\nmean_age\nsd_age\n\n\n\n1\n17\n23.31250\n5.770254\n\n\n2\n69\n21.57353\n2.738973\n\n\n3\n3\n21.33333\n1.154700\n\n\n\n\n\n\n\n2.3.3 Adding percentages\nSometimes, it may be useful to calculate percentages, such as for the gender split. You can do this by adding a line within the summarise() function to perform the calculation. All we need to do is take the number of female, male, and non-binary participants (stored in the n column of demo_by_gender), divide it by the total number of participants (stored in the n column of demo_total), and multiply by 100. Let’s add percentage to the summarise() function of demo_by_gender. Make sure that the code for percentages is placed after the value for n has been computed.\nAccessing the value of n for the different gender categories is straightforward because we can refer back to it directly. However, since the total number of participants is stored in a different data object, we need to use a base R function to access it – specifically the $ operator. To do this, you simply type the name of the data object (in this case, demo_total), followed by the $ symbol (with no spaces), and then the name of the column you want to retrieve (in this case, n). The general pattern is data$column.\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(n = n(), \n            # n from the line above divided by n from demo_total *100\n            percentage = n/demo_total$n *100, \n            mean_age = mean(Age, na.rm = TRUE), \n            sd_age = sd(Age, na.rm = TRUE)) %&gt;% \n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\npercentage\nmean_age\nsd_age\n\n\n\n1\n17\n19.101124\n23.31250\n5.770254\n\n\n2\n69\n77.528090\n21.57353\n2.738973\n\n\n3\n3\n3.370786\n21.33333\n1.154700\n\n\n\n\n\n\n\n\n\n\n\n\nTip for decimal places - use round()\n\n\n\n\n\nNot super important, because you could round the values by yourself when writing up your reports, but if you wanted to tidy up the decimal places in the output, you can do that using the round() function. You would need to “wrap” it around your computations and specify how many decimal places you want to display (for example mean(Age) would turn into round(mean(Age), 1)). It may look odd for percentage, just make sure the number that specifies the decimal places is placed within the round function. The default value is 0 (meaning no decimal spaces).\n\ndemo_by_gender &lt;- data_prp %&gt;% \n  group_by(Gender) %&gt;% \n  summarise(n = n(), \n            percentage = round(n/demo_total$n *100, 2), # percentage with 2 decimal places\n            mean_age = round(mean(Age, na.rm = TRUE), 1), # mean Age with 1 decimal place\n            sd_age = round(sd(Age, na.rm = TRUE), 3)) %&gt;% # sd Age with 3 decimal places\n  ungroup()\n\ndemo_by_gender\n\n\n\n\nGender\nn\npercentage\nmean_age\nsd_age\n\n\n\n1\n17\n19.10\n23.3\n5.770\n\n\n2\n69\n77.53\n21.6\n2.739\n\n\n3\n3\n3.37\n21.3\n1.155"
  },
  {
    "objectID": "02-wrangling.html#sec-ch2_act4",
    "href": "02-wrangling.html#sec-ch2_act4",
    "title": "2  Data wrangling I",
    "section": "\n2.4 Activity 4: Questionable Research Practices (QRPs)",
    "text": "2.4 Activity 4: Questionable Research Practices (QRPs)\nThe main goal is to compute the mean QRP score per participant for time point 1.\nLooking at the QRP data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nSo, we just have to compute an average score for items 1 to 11 as items 12 to 15 are distractor items. Seems quite straightforward.\nThe downside is that individual items are each in a separate column, i.e., in wide format, and everything would be easier if the items were arranged in long format.\nLet’s tackle this problem in steps. Best would be to create a separate data object for that. If we wanted to compute this within data_prp, it would turn into a nightmare.\n\n\nStep 1: select the relevant columns Code, and QRPs_1_Time1 to QRPs_1_Time1 and store them in an object called qrp_t1\n\n\nStep 2: pivot the data from wide format to long format using pivot_longer() so we can calculate the average score more easily (in step 3)\n\nStep 3: calculate the average QRP score (QRPs_Acceptance_Time1_mean) per participant using group_by() and summarise()\n\n\n\nqrp_t1 &lt;- data_prp %&gt;% \n  #Step 1\n  select(Code, QRPs_1_Time1:QRPs_11_Time1) %&gt;%\n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Scores\") %&gt;% \n  # Step 3\n  group_by(Code) %&gt;% # grouping py participant id\n  summarise(QRPs_Acceptance_Time1_mean = mean(Scores)) %&gt;% # calculating the average Score\n  ungroup() # just make it a habit\n\n\n\n\n\n\n\nExplain the individual functions\n\n\n\n\n\n\n\nselect ()\npivot_longer()\ngroup_by() and summarise()\n\n\n\nThe select function allows to include or exclude certain variables (columns). Here we want to focus on the participant ID column (i.e., Code) and the QRP items at time point 1. We can either list them all individually, i.e., Code, QRPs_1_Time1, QRPs_2_Time1, QRPs_3_Time1, and so forth (you get the gist), but that would take forever to type.\nA shortcut is to use the colon operator :. It allows us to select all columns that fall within the range of first_column_name to last_column_name. We can apply this here since the QRP items (1 to 11) are sequentially listed in data_prp.\n\nqrp_step1 &lt;- data_prp %&gt;% \n  select(Code, QRPs_1_Time1:QRPs_11_Time1)\n\n# show first 5 rows of qrp_step1\nhead(qrp_step1, n = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nQRPs_1_Time1\nQRPs_2_Time1\nQRPs_3_Time1\nQRPs_4_Time1\nQRPs_5_Time1\nQRPs_6_Time1\nQRPs_7_Time1\nQRPs_8_Time1\nQRPs_9_Time1\nQRPs_10_Time1\nQRPs_11_Time1\n\n\n\nTr10\n7\n7\n5\n7\n3\n4\n5\n7\n6\n7\n7\n\n\nBi07\n7\n7\n2\n7\n3\n7\n7\n7\n7\n6\n7\n\n\nSK03\n7\n7\n6\n6\n7\n6\n7\n7\n7\n5\n7\n\n\nSM95\n7\n7\n2\n6\n7\n5\n7\n7\n4\n2\n4\n\n\nSt01\n7\n7\n6\n7\n2\n7\n7\n7\n7\n5\n7\n\n\n\n\n\n\nHow many rows/observations and columns/variables do we have in qrp_step1?\n\nrows/observations: \n\ncolumns/variables: \n\n\n\n\nAs you can see, the table we got from Step 1 is in wide format. To get it into wide format, we need to define:\n\nthe columns that need to be reshuffled from wide into long format (col argument). Here we selected “everything except the Code column”, as indicated by -Code [minus Code]. However, QRPs_1_Time1:QRPs_11_Time1 would also work and give you the exact same result.\nthe names_to argument. R is creating a new column in which all the column names from the columns you selected in col will be stored in. Here we are naming this column “Items” but you could pick something equally sensible if you like.\nthe values_to argument. R creates this second column to store all responses the participants gave to the individual questions, i.e., all the numbers in this case. We named it “Scores” here, but you could have called it something different, like “Responses”\n\n\nqrp_step2 &lt;- qrp_step1 %&gt;% \n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Scores\")\n\n# show first 15 rows of qrp_step2\nhead(qrp_step2, n = 15)\n\n\n\n\nCode\nItems\nScores\n\n\n\nTr10\nQRPs_1_Time1\n7\n\n\nTr10\nQRPs_2_Time1\n7\n\n\nTr10\nQRPs_3_Time1\n5\n\n\nTr10\nQRPs_4_Time1\n7\n\n\nTr10\nQRPs_5_Time1\n3\n\n\nTr10\nQRPs_6_Time1\n4\n\n\nTr10\nQRPs_7_Time1\n5\n\n\nTr10\nQRPs_8_Time1\n7\n\n\nTr10\nQRPs_9_Time1\n6\n\n\nTr10\nQRPs_10_Time1\n7\n\n\nTr10\nQRPs_11_Time1\n7\n\n\nBi07\nQRPs_1_Time1\n7\n\n\nBi07\nQRPs_2_Time1\n7\n\n\nBi07\nQRPs_3_Time1\n2\n\n\nBi07\nQRPs_4_Time1\n7\n\n\n\n\n\n\nNow, have a look at qrp_step2. In total, we now have  rows/observations,  per participant, and  columns/variables.\n\n\nThis follows exactly the same sequence we used when calculating descriptive statistics by gender. The only difference is that we are now grouping the data by the participant’s Code instead of Gender.\nsummarise() works exactly the same way: summarise(new_column_name = function_to_calculate_something(column_name_of_numeric_values))\nThe function_to_calculate_something can be mean(), sd() or sum() for mean scores, standard deviations, or summed-up scores respectively. You could also use min() or max() if you wanted to determine the lowest or the highest score for each participant."
  },
  {
    "objectID": "02-wrangling.html#activity-5-knitting",
    "href": "02-wrangling.html#activity-5-knitting",
    "title": "2  Data wrangling I",
    "section": "\n2.5 Activity 5: Knitting",
    "text": "2.5 Activity 5: Knitting\nOnce you’ve completed your R Markdown file, the final step is to “knit” it, which converts the .Rmd file into a HTML file. Knitting combines your code, text, and output (like tables and plots) into a single cohesive document. This is a really good way to check your code is working.\nTo knit the file, click the Knit button at the top of your RStudio window. The document will be generated and, depending on your setting, automatically opened in the viewer in the Output pane or an external browser window.\nIf any errors occur during knitting, RStudio will show you an error message with details to help you troubleshoot.\nIf you want to intentionally keep any errors we tackled today to keep a reference on how you solved them, you could add error=TRUE or eval=FALSE to the code chunk that isn’t running."
  },
  {
    "objectID": "02-wrangling.html#activity-6-export-a-data-object-as-a-csv",
    "href": "02-wrangling.html#activity-6-export-a-data-object-as-a-csv",
    "title": "2  Data wrangling I",
    "section": "\n2.6 Activity 6: Export a data object as a csv",
    "text": "2.6 Activity 6: Export a data object as a csv\nTo avoid having to repeat the same steps in the next chapter, it’s a good idea to save the data objects you’ve created today as csv files. You can do this by using the write_csv() function from the readr package. The csv files will appear in your project folder.\nThe basic syntax is:\n\nwrite_csv(data_object, \"filename.csv\")\n\nNow, let’s export the objects data_prp and qrp_t1.\n\nwrite_csv(data_prp, \"data_prp_for_ch3.csv\")\n\nHere we named the file data_prp_for_ch3.csv, so we wouldn’t override the original data csv file prp_data_reduced.csv. However, feel free to choose a name that makes sense to you.\n\n\n\n\n\n\nYour Turn\n\n\n\nExport qrp_t1.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwrite_csv(qrp_t1, \"qrp_t1.csv\")\n\n\n\n\n\n\nCheck that your csv files have appeared in your project folder, and you’re all set!\nThat’s it for Chapter 2: Individual Walkthrough."
  },
  {
    "objectID": "02-wrangling.html#pair-coding",
    "href": "02-wrangling.html#pair-coding",
    "title": "2  Data wrangling I",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "02-wrangling.html#test-your-knowledge-and-challenge-yourself",
    "href": "02-wrangling.html#test-your-knowledge-and-challenge-yourself",
    "title": "2  Data wrangling I",
    "section": "Test your knowledge and challenge yourself",
    "text": "Test your knowledge and challenge yourself"
  },
  {
    "objectID": "03-wrangling2.html#intended-learning-outcomes",
    "href": "03-wrangling2.html#intended-learning-outcomes",
    "title": "3  Data wrangling II",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter, you should be able to:\n\napply familiar data wrangling functions to novel datasets\nread and interpret error messages\nrealise there are several ways of getting to the results\n\nIn this chapter, we will continue from where we left off in Chapter 2. We will compute average scores for two of the questionnaires, address an error mode problem, and finally, join all data objects together. This will finalise our data for the upcoming data visualization sections (Chapter 4 and Chapter 5)."
  },
  {
    "objectID": "03-wrangling2.html#individual-walkthrough",
    "href": "03-wrangling2.html#individual-walkthrough",
    "title": "3  Data wrangling II",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "03-wrangling2.html#activity-1-setup",
    "href": "03-wrangling2.html#activity-1-setup",
    "title": "3  Data wrangling II",
    "section": "\n3.1 Activity 1: Setup",
    "text": "3.1 Activity 1: Setup\n\nGo to the project folder we have been using in the last two weeks and double-click on the project icon to open the project in RStudio\nEither Create a new .Rmd for chapter 3 and save it to your project folder or continue the one from last week. See Section 1.3 if you need some guidance."
  },
  {
    "objectID": "03-wrangling2.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "href": "03-wrangling2.html#activity-2-load-in-the-libraries-and-read-in-the-data",
    "title": "3  Data wrangling II",
    "section": "\n3.2 Activity 2: Load in the libraries and read in the data",
    "text": "3.2 Activity 2: Load in the libraries and read in the data\nWe will use tidyverse today, as well as the two csv files we created at the end of the last chapter: data_prp_for_ch3.csv and qrp_t1.csv. If you need to download them for whatever reason, click on the following links: data_prp_for_ch3.csv and qrp_t1.csv.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nlibrary(???)\ndata_prp &lt;- read_csv(\"???\")\nqrp_t1 &lt;- read_csv(\"???\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp &lt;- read_csv(\"prp_data_reduced.csv\")\nqrp_t1 &lt;- read_csv(\"qrp_t1.csv\")\n\n\n\n\nIf you need a quick reminder what the dataset was about, have a look at the abstract in Section 1.4. We also addressed the changes we made to the dataset there.\nAnd remember to have a quick glimpse() at your data."
  },
  {
    "objectID": "03-wrangling2.html#activity-3-confidence-in-understanding-open-science-practices",
    "href": "03-wrangling2.html#activity-3-confidence-in-understanding-open-science-practices",
    "title": "3  Data wrangling II",
    "section": "\n3.3 Activity 3: Confidence in understanding Open Science practices",
    "text": "3.3 Activity 3: Confidence in understanding Open Science practices\nThe main goal is to compute the mean Understanding score per participant.\nThe mean Understanding score for time point 2 is already calculated in the (column Time2_Understanding_OS), but we have to compute it for time point 1.\nLooking at the Understanding data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nThe steps are quite similar to those for QRP, but we need to add an extra step: converting the character labels into numbers.\nAgain, let’s do this step by step:\n\n\nStep 1: select the relevant columns Code, and every Understanding column from time point 1 (e.g., from Understanding_OS_1_Time1 to Understanding_OS_12_Time1) and store them in an object called understanding_t1\n\n\nStep 2: pivot the data from wide format to long format using pivot_longer() so we can recode the labels into values (step 3) and calculate the average score (in step 4) more easily\n\nStep 3: Recode the values “Not at all confident” as 1 and “Entirely confident” as 7. All other values are already numbers. We can use functions mutate() in combination with case_match() for that\n\nStep 4: calculate the average QRP score (QRPs_Acceptance_Time1_mean) per participant using group_by() and summarise()\n\nSteps 1 and 2: select and pivot\nHow about you try the first 2 steps yourself using the code from Chapter 2 Activity 4 (Section 2.4) as a template?\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  select(???) %&gt;% # Step 1\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\") # Step 2\n\n\n\n\n\n\n\nSolution for steps 1 and 2\n\n\n\n\n\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, Understanding_OS_1_Time1:Understanding_OS_12_Time1) %&gt;% \n  # Step 2 - I picked different column labels this time for some variety\n  pivot_longer(cols = Understanding_OS_1_Time1:Understanding_OS_12_Time1, names_to = \"Understanding_Qs\", values_to = \"Responses\") \n\n\n\n\nStep 3: recoding the values\nOK, we now want to recode the values in the Responses column (or whatever name you picked for your column that has some of the numbers in it) so that “Not at all confident” = 1 and “Entirely confident” = 7. We want to keep all other values as they are (2-6 look already quite “numeric”).\nLet’s create a new column Responses_corrected that stores the new values with mutate(). Then we can combine that with the case_match() function.\n\nThe first argument in case_match() is the column name of the variable you want to recode.\nThen you can start recoding the values in the way of CurrentValue ~ NewValue (~ is a tilde). Make sure you use the ~ and not =.\nThe .default argument tells R what to do with values that are neither “Not at all confident” nor “Entirely confident”. Here, we want to replace them with the original value of the Responses column. In other datasets, you may want to set the default to NA for missing values, a character string or a number, and case_match() is happy to oblige.\n\n\nunderstanding_t1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = Responses # all other values taken from column Responses\n  ))\n\nError in `mutate()`:\nℹ In argument: `Responses_corrected = case_match(...)`.\nCaused by error in `case_match()`:\n! Can't combine `..1 (right)` &lt;double&gt; and `.default` &lt;character&gt;.\n\n\n\n\n\n\n\n\nError!!! Can you explain what is happening here?\n\n\n\n\n\nHave a look at the error message. It’s pretty helpful this time. It says Can't combine ..1 (right) &lt;double&gt; and .default &lt;character&gt;. It means that the replacement values are expected to be data type character since the original column type was type character.\n\n\n\nSo how do we fix this? Actually, there are several ways this could be done. Click on the tabs below to check out 3 possible solutions.\n\n\nFix option 1\nFix option 2\nFix option 3\n\n\n\nOne option is to modify the .default argument Responses so that the values are copied over from the original column but as a number rather than a character value. The function as.numeric() does the conversion.\n\nunderstanding_t1_step3_v1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  ))\n\n\n\nChange the numeric values on the right side of the ~ to character. Then in a second step, we would need to turn the character column into a numeric type. Again, we have several options to do so. We could either use the parse_number() function we encountered earlier during the demographics wrangling or the as.numeric() function.\n\nV1: Responses_corrected = parse_number(Responses_corrected)\n\nV2: Responses_corrected = as.numeric(Responses_corrected)\n\n\nJust pay attention that you are still working within the mutate() function.\n\nunderstanding_t1_step3_v2 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ \"1\",\n                                          \"Entirely confident\" ~ \"7\",\n                                          .default = Responses # all other values taken from column Responses (character)\n  ),\n  Responses_corrected = parse_number(Responses_corrected)) # turning Responses_corrected into a numeric column\n\n\n\nIf you recode all the labels into numbers (e.g., “2” into 2, “3” into 3, etc.) from the start, you won’t need to perform any additional conversions later.\n\nunderstanding_t1_step3_v2 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_recoded = case_match(Responses, # column of the values to recode\n                                        \"Not at all confident\" ~ 1, # recode all of them\n                                        \"2\" ~ 2,\n                                        \"3\" ~ 3,\n                                        \"4\" ~ 4,\n                                        \"5\" ~ 5,\n                                        \"6\" ~ 6,\n                                        \"Entirely confident\" ~ 7))\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nChoose the option that works best for you to modify the code above that didn’t work. You should now be able to calculate the mean Understanding Score per participant. Store the average scores in a variable called Time1_Understanding_OS. If you need help, refer to the hint below or use Chapter 2 Activity 4 (Section 2.4) as guidance.\n\n\n\n\n\n\nOne solution for Steps 3 and 4\n\n\n\n\n\n\nunderstanding_t1 &lt;- understanding_t1 %&gt;% \n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  )) %&gt;% \n  # Step 4: calculating averages per participant\n  group_by(Code) %&gt;%\n  summarise(Time1_Understanding_OS = mean(Responses_corrected)) %&gt;%\n  ungroup()\n\n\n\n\n\n\nOf course, this could have been written up as a single pipe.\n\n\n\n\n\n\nSingle pipe of activity 3\n\n\n\n\n\n\nunderstanding_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, Understanding_OS_1_Time1:Understanding_OS_12_Time1) %&gt;% \n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Understanding_Qs\", values_to = \"Responses\") %&gt;% \n  # Step 3\n  mutate(Responses_corrected = case_match(Responses, # column of the values to recode\n                                          \"Not at all confident\" ~ 1, # values to recode\n                                          \"Entirely confident\" ~ 7,\n                                          .default = as.numeric(Responses) # all other values taken from column Responses but as numeric data type \n  )) %&gt;% \n  # Step 4\n  group_by(Code) %&gt;%\n  summarise(Time1_Understanding_OS = mean(Responses_corrected)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "03-wrangling2.html#activity-4-survey-of-attitudes-toward-statistics-sats-28",
    "href": "03-wrangling2.html#activity-4-survey-of-attitudes-toward-statistics-sats-28",
    "title": "3  Data wrangling II",
    "section": "\n3.4 Activity 4: Survey of Attitudes Toward Statistics (SATS-28)",
    "text": "3.4 Activity 4: Survey of Attitudes Toward Statistics (SATS-28)\nThe main goal is to compute the mean SATS-28 score for each of the 4 subscales per participant for time point 1.\nLooking at the SATS data at time point 1, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\nAdditionally, we are looking to compute the means for the 4 different subscales of the SAT-28 which are , , , and .\n\nThis scenario is slightly more tricky than the previous ones due to the reverse-coding and the 4 subscales. So, let’s tackle this step by step again:\n\n\nStep 1: select the relevant columns Code, and every SATS28 column from time point 1 (e.g., from SATS28_1_Affect_Time1 to SATS28_28_Difficulty_Time1) and store them in an object called sats_t1\n\n\nStep 2: pivot the data from wide format to long format using pivot_longer() so we can recode the labels into values (step 3) and calculate the average score (in step 4) more easily\n\nStep 3: We need to know which items belong to which subscale - fortunately, we have that information in the variable name and can use the separate() function to access it.\n\nStep 4: We need to know which items are reverse-coded and then reverse-code them - unfortunately, the info is only in the codebook and we need to find a work-around. case_when() can help identify and re-score the reverse-coded items.\n\nStep 5: calculate the average SATS score per participant and subscale using group_by() and summarise()\n\n\nStep 6: use pivot_wider() to spread out the dataframe into wide format and rename() to tidy up the datanames\nSteps 1 and 2: select and pivot\nThe selecting and pivoting are exactly the same way as we already practised in the other 2 questionniares. Apply them here to this questionnaire.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  select(???) %&gt;% # Step 1\n  pivot_longer(cols = ???, names_to = \"???\", values_to = \"???\") # Step 2\n\n\n\n\n\n\n\nSolution for steps 1 and 2\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  select(Code, SATS28_1_Affect_Time1:SATS28_28_Difficulty_Time1) %&gt;% # Step 1\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Response\") # Step 2\n\n\n\n\n\n\n\nStep 3: separate Subscale information\nIf you look at the Items column more closely, you can see that there is information on the Questionnaire, the Item_number, the Subscale, and the Timepoint the data was collected at.\nWe can separate the information into separate columns using the separate() function. The function’s first argument is the column to separate, then define into which columns you want the original column to split up, and lastly, define the separator sep (here an underscore). For our example, we would write:\n\nV1: separate(Items, into = c(\"SATS\", \"Item_number\", \"Subscale\", \"Time\"), sep = \"_\")\n\n\nHowever, we don’t need all of those columns, so we could just drop the ones we are not interested in by replacing them with NA.\n\nV2: separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\")\n\n\nWe might also add an extra argument of convert = TRUE to have numeric columns (i.e., Item_number) converted to numeric as opposed to keeping them as characters. Saves us typing a few quotation marks later in Step 4.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  # Step 3\n  separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\", convert = TRUE)\n\nStep 4: identifying reverse-coded items and then correct them\nWe can use case_when() within the mutate() function here to create a new column FW_RV that stores information on whether the item is a reverse-coded item or not.\ncase_when() works similarly to case_match(), however case_match() only allows you to “recode” values (i.e., replace one value with another), whereas case_when() is more flexible. It allows you to use conditional statements on the left side of the tilde which is useful when you want to change only some of the data based on specific conditions.\nLooking at the codebook, it seems that items 2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28 are reverse-coded. The rest are forward-coded.\nWe want to tell R now, that\n\n\nif the Item_number is any of those numbers listed above, R should write “Reverse” into the new column FW_RV we are creating. Since we have a few possible matches for Item_number, we need the Boolean expression %in% rather than ==.\n\nif Item_number is none of those numbers, then we would like the word “Forward” in the FW_RV column to appear. We can achieve that by specifying a .default argument again, but this time we want a “word” rather than a value from another column.\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(FW_RV = case_when(\n    Item_number %in% c(2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28) ~ \"Reverse\",\n    .default = \"Forward\"\n  ))\n\nMoving on to correcting the scores. Once again, we can use case_when () within the mutate() function to create another conditional statement. This time, the condition is:\n\n\nif FW_RV column has a value of “Reverse” then we would like to turn all 1 into 7, 2 into 6, etc.\n\nif FW_RV column has a value of “Forward” then we would like to keep the score from the Response column\n\nThere is a quick way and a not-so-quick way to achieve the actual reverse-coding.\n\nOption 1 (quick): The easiest way to reverse-code scores is by taking the maximum value of the scale, adding 1 unit and then subtracting the original value. For example, on a 5-point Likert scale, it would be 6 minus the original rating; for a 7-point Likert scale, 8 minus the original rating, etc. (see tab Option 1).\nOption 2 (not so quick): This includes the use of 2 conditional statements (see tab Option 2).\n\nUse the one you find more intuitive.\n\n\nOption 1\nOption 2\n\n\n\nHere we are using the Boolean expression to determine if there is a string “Reverse” in the FW_RV column. And if that conditional statement is TRUE then the value in the new column we are creating Scores_corrected should be calculated as 8 minus the value from the Response column. If it’s not (i.e., the .default argument), then the values of the Response column should be kept.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(Scores_corrected = case_when(\n    FW_RV == \"Reverse\" ~ 8-Response,\n    .default = Response\n  ))\n\n\n\nAs stated above, the longer version would include 2 conditional statements. The first condition checks if the value in FW_RV is “Reverse”. The second condition checks if the value in Response is equal to a specific number. If both conditions are met, then the value on the right side of the tilde should be placed in the newly created Scores_corrected_v2 column.\nFor example, line 3 would read: if the FW_RV value is “Reverse” AND the value in the Response column is 1, then place a value of 7 into Scores_corrected_v2.\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  mutate(Scores_corrected_v2 = case_when(\n    FW_RV == \"Reverse\" & Response == 1 ~ 7,\n    FW_RV == \"Reverse\" & Response == 2 ~ 6,\n    FW_RV == \"Reverse\" & Response == 3 ~ 5,\n    # no need to recode 4 as 4\n    FW_RV == \"Reverse\" & Response == 5 ~ 3,\n    FW_RV == \"Reverse\" & Response == 6 ~ 2,\n    FW_RV == \"Reverse\" & Response == 7 ~ 1,\n    .default = Response\n  ))\n\nAs you can see now in sats_t1, both columns Scores_corrected and Scores_corrected_v2 are identical.\n\n\n\nOne way of checking whether our reverse-coding worked is to look at the distinct values of the original Response column and Scores_corrected. We would also need to keep information of the FW_RV column.\nTo see the pattern better, we want to use arrange() to sort the values in a more meaningful way. Remember from last year, the default order is ascending, and you would need to add the function desc() on your variable to sort values in descending order.\n\ncheck_coding &lt;- sats_t1 %&gt;% \n  distinct(FW_RV, Response, Scores_corrected) %&gt;% \n  arrange(desc(FW_RV), Response)\n\n\n\n\n\n\n\nShow check_coding output\n\n\n\n\n\n\ncheck_coding\n\n\n\n\nFW_RV\nResponse\nScores_corrected\n\n\n\nReverse\n1\n7\n\n\nReverse\n2\n6\n\n\nReverse\n3\n5\n\n\nReverse\n4\n4\n\n\nReverse\n5\n3\n\n\nReverse\n6\n2\n\n\nReverse\n7\n1\n\n\nForward\n1\n1\n\n\nForward\n2\n2\n\n\nForward\n3\n3\n\n\nForward\n4\n4\n\n\nForward\n5\n5\n\n\nForward\n6\n6\n\n\nForward\n7\n7\n\n\n\n\n\n\n\n\n\nStep 5\nNow that we know everything worked out as intended, we can calculate the mean scores of each subscale for each participant in sats_t1.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  group_by(???, ???) %&gt;% \n  summarise(mean_score = ???(???)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  group_by(Code, Subscale) %&gt;% \n  summarise(mean_score = mean(Scores_corrected)) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'Code'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\nStep 6\nOne final step is to turn the data back into wide format so that each subscale has its own column. That would make joining the data objects easier. The first argument in pivot_wider() is names_from and you should specify the column here that you want as your new column headings. The second argument is values_from and you need to specify the column that you want to get the cell values from\nWe should also rename the column names to match more with the column names in the codebook. Conveniently, we can use a function called rename() that works exactly like select() (i.e., pattern new_name = old_name) but keeps all the other column names the same (rather than reducing the number of columns)\n\nsats_t1 &lt;- sats_t1 %&gt;% \n  pivot_wider(names_from = Subscale, values_from = mean_score) %&gt;% \n  rename(SATS28_Affect_Time1_mean = Affect,\n         SATS28_CognitiveCompetence_Time1_mean = CognitiveCompetence,\n         SATS28_Value_Time1_mean = Value,\n         SATS28_Difficulty_Time1_mean = Difficulty)\n\n\n\n\n\n\n\nShow final sats_t1 output\n\n\n\n\n\n\nhead(sats_t1, n = 5)\n\n\n\n\n\n\n\n\n\n\n\nCode\nSATS28_Affect_Time1_mean\nSATS28_CognitiveCompetence_Time1_mean\nSATS28_Difficulty_Time1_mean\nSATS28_Value_Time1_mean\n\n\n\nAD03\n2.333333\n3.833333\n3.428571\n5.555556\n\n\nAD05\n3.500000\n5.000000\n2.142857\n4.777778\n\n\nAb01\n5.166667\n5.666667\n4.142857\n5.444444\n\n\nAl05\n2.166667\n2.666667\n2.857143\n3.777778\n\n\nAm05\n4.166667\n5.666667\n5.571429\n4.888889\n\n\n\n\n\n\n\n\n\nAgain, this could have been written up as a single pipe.\n\n\n\n\n\n\nSingle pipe of activity 4\n\n\n\n\n\n\nsats_t1 &lt;- data_prp %&gt;% \n  # Step 1\n  select(Code, SATS28_1_Affect_Time1:SATS28_28_Difficulty_Time1) %&gt;% \n  # Step 2\n  pivot_longer(cols = -Code, names_to = \"Items\", values_to = \"Response\") %&gt;% \n  # Step 3\n  separate(Items, into = c(NA, \"Item_number\", \"Subscale\", NA), sep = \"_\", convert = TRUE) %&gt;% \n  # step 4\n  mutate(FW_RV = case_when(\n    Item_number %in% c(2, 3, 4, 6, 7, 8, 9, 12, 13, 16, 17, 19, 20, 21, 23, 25, 26, 27, 28) ~ \"Reverse\",\n    .default = \"Forward\"\n  ),\n    Scores_corrected = case_when(\n      FW_RV == \"Reverse\" ~ 8-Response,\n      .default = Response\n  )) %&gt;% \n  # step 5\n  group_by(Code, Subscale) %&gt;% \n  summarise(mean_score = mean(Scores_corrected)) %&gt;% \n  ungroup() %&gt;% \n  # step 6\n  pivot_wider(names_from = Subscale, values_from = mean_score) %&gt;% \n  rename(SATS28_Affect_Time1_mean = Affect,\n         SATS28_CognitiveCompetence_Time1_mean = CognitiveCompetence,\n         SATS28_Value_Time1_mean = Value,\n         SATS28_Difficulty_Time1_mean = Difficulty)"
  },
  {
    "objectID": "03-wrangling2.html#activity-5-error-mode-perceptions-of-supervisory-support",
    "href": "03-wrangling2.html#activity-5-error-mode-perceptions-of-supervisory-support",
    "title": "3  Data wrangling II",
    "section": "\n3.5 Activity 5 (Error Mode): Perceptions of supervisory support",
    "text": "3.5 Activity 5 (Error Mode): Perceptions of supervisory support\nThe main goal is to compute the mean score for perceived supervisory support per participant.\nLooking at the supervisory support data, you determine that\n\nindividual item columns are \nnumeric\ncharacter, and\naccording to the codebook, there are \nno\nsome reverse-coded items in this questionnaire.\n\nI have outlined my steps as follows:\n\n\nStep 1: reverse-code the single column first because that’s less hassle than having to do that with conditional statements (Supervisor_15_R). mutate() is my friend.\n\nStep 2: I want to filter out everyone who failed the attention check in Supervisor_7. I can do this with a Boolean expression within the filter() function. The correct response was “completely disagree” which is 1.\n\nStep 3: select their id from time point 2 and all the columns that start with the word “super”, apart from Supervisor_7 and the original Supervisor_15_R column\n\nStep 4: pivot into long format so I can calculate the averages better\n\nStep 5: calculate the average scores per participant\n\nI’ve started coding but there are some errors in my code. Help me find and fix all of them. Try to go through the code line by line and read the error messages.\n\nsuper &lt;- data_ppr %&gt;% \n  mutate(Supervisor_15 = 9-supervisor_15_R) %&gt;% \n  filter(Supervisor_7 = 1) %&gt;% \n  select(Code, starts_with(\"Super\"), -Supervisor_7, -Supervisor_15_R) \npivot_wider(cols = -Code, names_to = \"Item\", values_to = \"Response\") %&gt;% \n  group_by(Time2_Code) %&gt;% \n  summarise(Mean_Supervisor_Support = mean(Score_corrected, na.rm = TRUE)) %&gt;% \n  ungroup()\n\n\n\n\n\n\n\nHow many mistakes am I supposed to find?\n\n\n\n\n\nThere are 8 mistakes in the code.\n\n\n\n\n\n\n\n\n\nReveal solution\n\n\n\n\n\nDid you spot all 8 mistakes? Let’s go through them line by line.\n\nsuper &lt;- data_prp %&gt;% # spelling mistake in data object\n  mutate(Supervisor_15 = 8-Supervisor_15_R) %&gt;% # semantic error: 8 minus response for a 7-point scale and supervisor_15_R needs a capital S\n  filter(Supervisor_7 == 1) %&gt;% # needs a Boolean expression == instead of =\n  select(Code, starts_with(\"Super\"), -Supervisor_7, -Supervisor_15_R) %&gt;% # no pipe at the end, the rest is actually legit\n  pivot_longer(cols = -Code, names_to = \"Item\", values_to = \"Response\") %&gt;% # pivot_longer instead of pivot_wider\n  group_by(Code) %&gt;% # Code rather than Time2_Code - the reduced dataset does not contain Time2_Code\n  summarise(Mean_Supervisor_Support = mean(Response, na.rm = TRUE)) %&gt;% # Score_corrected doesn't exist; needs to be Response\n  ungroup()\n\n\nNote that the semantic error in line 2 will not give you an error message.\nWere you thrown off by the starts_with(\"Super\") expression in line 4? starts_with() and ends_with() are great alternatives to selecting columns via : But, using select(Code, Supervisor_1:Supervisor_6, Supervisor_8:Supervisor_14) would have given us the same result. [I admit, that one was a bit mean]"
  },
  {
    "objectID": "03-wrangling2.html#activity-6-join-everything-together-with-_join",
    "href": "03-wrangling2.html#activity-6-join-everything-together-with-_join",
    "title": "3  Data wrangling II",
    "section": "\n3.6 Activity 6: Join everything together with ???_join()\n",
    "text": "3.6 Activity 6: Join everything together with ???_join()\n\nTime to join all of the relevant data files together so we have a single dataframe ready for the next chapter on data visualisation. There are 4 options of joining data together, namely inner_join(), left_join(), right_join(), and full_join(). Each of these functions differs in terms of what information is retained from the two data objects being joined. Here is a quick overview:\n\n\n\n\n\n\nInfo on mutating joins\n\n\n\nYou have 4 types of join functions you could make use of. Click on the panels to know more\n\n\ninner_join()\nleft_join()\nright_join()\nfull_join()\n\n\n\ninner_join() returns only the rows where the values in the column specified in the by = statement match in both tables.\n\n\ninner_join(): gif by Garrick Aden-Buie\n\n\n\nleft_join() retains the complete first (left) table and adds values from the second (right) table that have matching values in the column specified in the by = statement. Rows in the left table with no match in the right table will have missing values (NA) in the new columns.\n\n\nleft_join(): gif by Garrick Aden-Buie\n\n\n\nright_join() retains the complete second (right) table and adds values from the first (left) table that have matching values in the column specified in the by = statement. Rows in the right table with no match in the left table will have missing values (NA) in the new columns.\n\n\nright_join(): gif by Garrick Aden-Buie\n\n\n\nfull_join() returns all rows and all columns from both tables. NA values fill unmatched rows.\n\n\nfull_join(): gif by Garrick Aden-Buie\n\n\n\n\n\n\nFrom our data_prp, we need to select demographics data and all summarised questionnaire data from time point 2. Next, we will join this with all other aggregated datasets from time point 1 which are currently stored in separate data objects in the Global Environment.\nYou have already encountered inner_join last year, but for now, we want to retain all data from all the data objects, so we will use full_join instead. Keep in mind, you can only join two data objects at a time, so there will be quite a bit of piping and joining in the upcoming code chunk.\nPS: Since I (Gaby) am a bit particular, I want my columns to be arranged in a meaningful pattern. Hence I’m using select to sort them in a better way.\n\ndata_prp_final &lt;- data_prp %&gt;% \n  select(Code:Plan_prereg, Other_OS_behav_2:Time2_Understanding_OS) %&gt;% \n  full_join(qrp_t1) %&gt;% \n  full_join(understanding_t1) %&gt;% \n  full_join(sats_t1) %&gt;% \n  full_join(super) %&gt;% \n  select(Code:Plan_prereg, Pre_reg_group, SATS28_Affect_Time1_mean, SATS28_CognitiveCompetence_Time1_mean, SATS28_Value_Time1_mean, SATS28_Difficulty_Time1_mean, QRPs_Acceptance_Time1_mean, Time1_Understanding_OS, Other_OS_behav_2:Time2_Understanding_OS, Mean_Supervisor_Support)\n\nAnd this is basically the dataset we need for Chapter 4 and Chapter 5."
  },
  {
    "objectID": "03-wrangling2.html#activity-7-knit-and-export",
    "href": "03-wrangling2.html#activity-7-knit-and-export",
    "title": "3  Data wrangling II",
    "section": "\n3.7 Activity 7: Knit and export",
    "text": "3.7 Activity 7: Knit and export\nKnit the .Rmd file to ensure everything runs as expected. Once it does, export the data object data_prp_final as a csv for use in the Chapter 4. Name it something meaningful, something like data_prp_for_ch4.csv.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwrite_csv(data_prp_final, \"data_prp_for_ch4.csv\")"
  },
  {
    "objectID": "03-wrangling2.html#pair-coding",
    "href": "03-wrangling2.html#pair-coding",
    "title": "3  Data wrangling II",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "03-wrangling2.html#test-your-knowledge-and-challenge-yourself",
    "href": "03-wrangling2.html#test-your-knowledge-and-challenge-yourself",
    "title": "3  Data wrangling II",
    "section": "Test your knowledge and challenge yourself",
    "text": "Test your knowledge and challenge yourself"
  },
  {
    "objectID": "04-dataviz.html#intended-learning-outcomes",
    "href": "04-dataviz.html#intended-learning-outcomes",
    "title": "4  Data viz I",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\nexplain the layered grammar of graphics\nchoose an appropriate plot for categorical variables\ncreate a basic version of an appropriate plot\napply extra layers to change the appearance of the plot\n\nIt is time to think about which plot is the appropriate plot for your data. Different types of variables require different types of plots, so this comes back to how many variables are you aiming to plot and what kind of data type are they. Today, we will focus on plots for categorical data. Next week, we’ll cover plots for continuous variables and discover which plots are appropriate if you have a mix of continuous and categorical variables."
  },
  {
    "objectID": "04-dataviz.html#individual-walkthrough",
    "href": "04-dataviz.html#individual-walkthrough",
    "title": "4  Data viz I",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "04-dataviz.html#building-plots",
    "href": "04-dataviz.html#building-plots",
    "title": "4  Data viz I",
    "section": "\n4.1 Building plots",
    "text": "4.1 Building plots\nWe are using the package ggplot2 to create data visualisations. It’s part of the tidyverse package. Actually, most people call th package ggplot but it’s official name is ggplot2.\n\n\nggplot2 uses a layered grammar of graphics, in which plots are built up in a series of layers. You would start with a base layer (opening ggplot), adding data and aesthetics, and selecting the geometries for plot.\nThese first 3 layers will give you the most simple version of a complete plot, but you could add other layers to make the plots more accessible (and pretty) by using scales, facets, coordinates, labels and themes.\n\n\n\n\ngg layers (Presentation by Ryan Safner)\n\n\n\nTo give you a brief overview of the layering system, let’s use the package palmerpenguins (https://allisonhorst.github.io/palmerpenguins/). It contains data about bill length and depth, flipper length, and body mass, etc.\n\nhead(penguins)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\nLet’s build a basic scatterplot to show the relationship between flipper_length and body_mass. We will customise plots further later on in the individual plots. This is just a quick overview of the different layers.\n\nLayer 1 creates a plot base to built up upon.\nLayer 2 adds the data and some aesthetics\n\ndata is first argument\naesthetics are added via the mapping argument. There you define your variables to be added (such as x, or x and y) and allows you specify overall properties like the colour of grouping variables etc.\n\n\nLayer 3 adds the geometries or geom_? for short. This tells ggplot in which style we want to plot the data points. Remember to add these layers with a + rather than a pipe %&gt;%. You can add multiple geoms if you wish, e.g., building a violin-boxplot\nLayer 4 adds the scale_? functions which can help you customise the aesthetics, such as changing colour. You can do much more with scales, but we’ll get to that later.\nLayer 5 introduces facets, such as facet_wrap() which allows you to add another dimension to the data output by showing the relationship you are interested in for each level of a categorical variable.\nLayer 6 - coordinates: coord_cartesian() controls the limits for the x- and y-axes (arguments xlim and ylim). Changing those allows you to zoom in or out of your plot.\nLayer 7 helps you to modify axes labels.\nLayer 8 controls the general style of a ggplot (e.g., background colour, size of text, borders, etc.). R comes with a few pre-defined ones (like theme_classic, theme_bw, theme_minimal, theme_light).\n\nClick on the tabs below to see how each layer contributes to refining the plot.\n\n\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\n\n\n\n\nggplot()\n\n\n\n\n\n\n\nWe don’t see much here. It’s basically an empty plot layer.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm))\n\n\n\n\n\n\n\nYou won’t see any data points yet, because we haven’t specified how we want to display the data points. But we mapped in the aesthetics, that we want to plot variable body mass on the x-axis and flipper length on the y-axis. This also adds the axes titles and the values and break points of the axes.\n\n\n\n\n\n\nTip\n\n\n\nYou won’t need to add data = or mapping = if you keep those arguments in exactly that order. Likewise, the first column name you enter within the aes() function will always be interpreted as x, and the second as y, so you could omit them if you wish.\n\nggplot(penguins, aes(body_mass_g, flipper_length_mm))\n\nwill give you the same output as the code above.\n\n\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHere we are telling ggplot that we want a scatterplot added. There is a warning displayed showing that rows were removed because of missing values.\nThe argument colour adds colour to the points according to a grouping variable (in this case sex). If you want all of the points to be black (i.e. only represent 2 rather than 3 dimensions of the data), leave the colour argument out.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  # changes colour palette\n  scale_colour_brewer(palette = \"Dark2\") + \n  # add breaks from 2500 to 6500 in increasing steps of 500\n  scale_x_continuous(breaks = seq(from = 2500, to = 6500, by = 500)) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nThe scale_? functions allow us to change the colour palette of the plot or the axes breaks etc. You could change the name of the axis in scale_x_continuous() as well or leave it for Layer 7.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  # split main plot up into different subplots by species \n  facet_wrap(~ species) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nHere we are faceting this plot out for the individual species.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  # limits the range of the y axis\n  coord_cartesian(ylim = c(0, 250)) \n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nChanging the limits of the y axis to zoom in or out of the plot. If you wanted to the same for the x axis, you would add an argument xlim to the coord_cartesian() function.\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  labs(x = \"Body Mass (in g)\", # labels the x axis\n       y = \"Flipper length (in mm)\", # labels the y axis\n       colour = \"Sex\") # labels the grouping variable in the legend\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou can change the axes labels via the labs() function or include that step when modifying the scales (i.e. in the scale_x_continuous() function).\n\n\n\nggplot(data = penguins, mapping = aes(x=body_mass_g, y=flipper_length_mm, colour=sex)) +\n  geom_point() +\n  scale_colour_brewer(palette = \"Dark2\") + \n  facet_wrap(~ species) +\n  labs(x = \"Body Mass (in g)\", \n       y = \"Flipper length (in mm)\",\n       colour = \"Sex\") +\n  # add a theme\n  theme_classic()\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\ntheme_classic() is applied to change the overall appearance of the plot.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to stick to the first 3 layers to get your base plot. Everything else is optional meaning you don’t have to use all 8 layers in a plot. And layers 4-8 can be added in a random order whereas layers 1-3 are fixed."
  },
  {
    "objectID": "04-dataviz.html#activity-1-set-up-and-data-for-today",
    "href": "04-dataviz.html#activity-1-set-up-and-data-for-today",
    "title": "4  Data viz I",
    "section": "\n4.2 Activity 1: Set-up and data for today",
    "text": "4.2 Activity 1: Set-up and data for today\n\nWe are still working with the data by Pownall et al. (2023), so open your project.\nHowever, we could do with a fresh Rmd: Create a new .Rmd file* and save it to your project folder. Name it something meaningful (e.g., “chapter_04”, “04_data_viz.Rmd”). See Section 1.3 if you need some guidance. Delete everything below line 12 (keep the set-up code chunk)\nWe aggregated the data in Chapter 2 and Chapter 3. If you want a fresh copy, download the data here: data_prp_for_ch4.csv. Make sure to place the csv file in the project folder.\nIf you need a reminder about the data and variables, have a look at the codebook and/or Section 1.4."
  },
  {
    "objectID": "04-dataviz.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "href": "04-dataviz.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "title": "4  Data viz I",
    "section": "\n4.3 Activity 2: Load in libraries, read in data, and adjust data types",
    "text": "4.3 Activity 2: Load in libraries, read in data, and adjust data types\nWe need the package tidyverse today, and the data data_prp_for_ch4.csv.\n\n## packages \n???\n\n## data\ndata_prp_viz &lt;- read_csv(???)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp_viz &lt;- read_csv(\"data_prp_for_ch4.csv\")\n\n\n\n\nAs we said in Section 1.6, it is always recommended to glimpse at the data to see how many variables and observations there are in the dataset and what kind of data type they are.\n\n\n\n\n\n\nglimpse output\n\n\n\n\n\n\nglimpse(data_prp_viz)\n\nRows: 89\nColumns: 28\n$ Code                                  &lt;chr&gt; \"Tr10\", \"Bi07\", \"SK03\", \"SM95\", …\n$ Gender                                &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2,…\n$ Age                                   &lt;dbl&gt; 22, 20, 22, 26, 22, 20, 21, 21, …\n$ Ethnicity                             &lt;chr&gt; \"White European\", \"White British…\n$ Secondyeargrade                       &lt;dbl&gt; 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1,…\n$ Opptional_mod                         &lt;dbl&gt; 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,…\n$ Opptional_mod_1_TEXT                  &lt;chr&gt; \"Research methods in first year\"…\n$ Research_exp                          &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Research_exp_1_TEXT                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Plan_prereg                           &lt;dbl&gt; 1, 3, 1, 2, 1, 1, 3, 3, 2, 2, 2,…\n$ Pre_reg_group                         &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 2,…\n$ SATS28_Affect_Time1_mean              &lt;dbl&gt; 4.000000, 3.833333, 5.000000, 5.…\n$ SATS28_CognitiveCompetence_Time1_mean &lt;dbl&gt; 5.166667, 5.166667, 5.666667, 4.…\n$ SATS28_Value_Time1_mean               &lt;dbl&gt; 6.000000, 6.666667, 5.222222, 5.…\n$ SATS28_Difficulty_Time1_mean          &lt;dbl&gt; 3.571429, 2.428571, 3.571429, 3.…\n$ QRPs_Acceptance_Time1_mean            &lt;dbl&gt; 5.909091, 6.090909, 6.545455, 5.…\n$ Time1_Understanding_OS                &lt;dbl&gt; 5.500000, 3.166667, 4.500000, 3.…\n$ Other_OS_behav_2                      &lt;dbl&gt; 1, NA, NA, NA, 1, NA, NA, 1, NA,…\n$ Other_OS_behav_4                      &lt;dbl&gt; 1, NA, NA, NA, NA, NA, NA, NA, N…\n$ Other_OS_behav_5                      &lt;dbl&gt; NA, NA, NA, NA, 1, 1, NA, NA, NA…\n$ Closely_follow                        &lt;dbl&gt; 2, 2, 2, NA, 3, 3, 3, NA, NA, 2,…\n$ SATS28_Affect_Time2_mean              &lt;dbl&gt; 3.500000, 3.166667, 4.833333, 4.…\n$ SATS28_CognitiveCompetence_Time2_mean &lt;dbl&gt; 4.166667, 4.666667, 6.166667, 5.…\n$ SATS28_Value_Time2_mean               &lt;dbl&gt; 3.000000, 6.222222, 6.000000, 4.…\n$ SATS28_Difficulty_Time2_mean          &lt;dbl&gt; 2.857143, 2.857143, 4.000000, 2.…\n$ QRPs_Acceptance_Time2_mean            &lt;dbl&gt; 5.636364, 5.454545, 6.272727, 5.…\n$ Time2_Understanding_OS                &lt;dbl&gt; 5.583333, 3.333333, 5.416667, 4.…\n$ Mean_Supervisor_Support               &lt;dbl&gt; 5.230769, 6.285714, 6.857143, 2.…\n\n\n\n\n\nWe can see that some of the categorical data in data_prp_viz was read in as numeric variables which makes them continuous. This is going to haunt us big time when building the plots. We would be better off addressing these changes in the dataset before we start plotting (and potentially getting frustrated with R and data viz in general)\nLet’s turn a few of the categorical variables into factors.\n\ndata_prp_viz &lt;- data_prp_viz %&gt;% \n  mutate(Gender = factor(Gender,\n                         levels = c(2, 1, 3),\n                         labels = c(\"females\", \"males\", \"non-binary\")),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         Plan_prereg = factor(Plan_prereg,\n                              levels = c(1, 3, 2),\n                              labels = c(\"Yes\", \"Unsure\", \"No\")),\n         Closely_follow = factor(Closely_follow,\n                                 levels = c(2, 3),\n                                 labels = c(\"Followed it somewhat\", \"Followed it exactly\")),\n         Research_exp = factor(Research_exp),\n         Pre_reg_group = factor(Pre_reg_group))"
  },
  {
    "objectID": "04-dataviz.html#activity-3-barchart-geom_bar",
    "href": "04-dataviz.html#activity-3-barchart-geom_bar",
    "title": "4  Data viz I",
    "section": "\n4.4 Activity 3: Barchart (geom_bar())",
    "text": "4.4 Activity 3: Barchart (geom_bar())\nA barchart is the way to go when you have a single categorical variable you want to plot.\nLet’s say we want to count some demographics. To keep it simple, we want to show gender counts. We would use a barplot for it. This is done with geom_bar() in your third layer, and because the counting is done in the background, the aes only requires an x value (i.e. the name of your variable).\n\nggplot(data_prp_viz, aes(x = Gender)) +\n  geom_bar() \n\n\n\nFigure 4.1: Default barchart\n\n\n\nThis is the base plot done. You can customise it by adding different layers. For example, the labels aren’t super clear or it could do with a dash of colour. Click on the tabs below to see examples of what could be added and practice on your base plot in your own Rmd.\n\n\nColour\nAxes labels & margins\nLegend\nThemes\n\n\n\nWe can change the colour by adding a fill argument in the aes(). If we want to modify these colours further, we would add a scale_fill_? argument. If you have specific colours in mind, you would use scale_fill_manual() or if you want to stick with pre-defined ones, like viridis, use scale_fill_viridis_d()\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  # customise colour\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\nThe x-axis label is fine, but the categories need to be relabeled. You can achieve that with the scale_x_discrete() function and the argument labels =. But make sure you order them in the order of the labels in the dataframe.\nThere is also this gap between bottom of the chart and the bars which seems a bit weird. We can remove that with an expansion() function.\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  # changing group labels on the breaks of the x axis\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) + \n  scale_y_continuous(\n    # changing name of the y axis\n    name = \"Count\",\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05))\n  )\n\n\n\n\n\n\n\n\n\nThe legend does not add any information because the labels are already provided on the x axis. We can remove the legend by adding the argument guide = \"none\" into the scale_fill function.\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d(\n    # remove the legend\n    guide = \"none\") +\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) +\n  scale_y_continuous(\n    name = \"Count\",\n    expand = expansion(mult = c(0, 0.05))\n  )\n\n\n\n\n\n\n\n\n\nLet’s experiment with the themes. For this plot we have chosen theme_minimal()\n\nggplot(data_prp_viz, aes(x = Gender, fill = Gender)) +\n  geom_bar() +\n  scale_fill_viridis_d(\n    guide = \"none\") +\n  scale_x_discrete(labels = c(\"Female\", \"Male\", \"Non-Binary\")) +\n  scale_y_continuous(\n    name = \"Count\",\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  # pick a theme\n  theme_minimal()"
  },
  {
    "objectID": "04-dataviz.html#activity-4-column-plot-geom_col",
    "href": "04-dataviz.html#activity-4-column-plot-geom_col",
    "title": "4  Data viz I",
    "section": "\n4.5 Activity 4: Column plot (geom_col())",
    "text": "4.5 Activity 4: Column plot (geom_col())\nIf someone had already summarised those counts for you, you would not be able to use geom_bar(). In that case, you would switch to geom_col().\n\ngender_count &lt;- data_prp_viz %&gt;% \n  count(Gender)\n\ngender_count\n\n\n\n\nGender\nn\n\n\n\nfemales\n69\n\n\nmales\n17\n\n\nnon-binary\n3\n\n\n\n\n\n\nThe mapping for geom_col() requires both an x and a y aesthetics. In our example, x would be our categorical variable (e.g., Gender), and y would be the column name that stored the values (n). Note how the base version has now n as an axis title (instead of count).\n\nggplot(gender_count, aes(x = Gender, y = n, fill = Gender)) +\n  geom_col()\n\n\n\nFigure 4.2: Column plot with different coloured bars\n\n\n\n\n\n\n\n\n\nYour Turn: Make the column plot pretty\n\n\n\nThe other layers to change the colour scheme, axes labels and margins, removing the legend and altering the theme require exactly the same functions as with the boxplot above. Test yourself to see if you could…\n\n\nchange the colour scheme (e.g., viridis or any other colour palettes)\n\nremove the legend\n\nchange the title of the x and y axes\n\nmake the bars start directly on the x axis\n\nadd a theme of your linking\n\n\n\n\n\n\n\nPossible solution code for the column plot (with a different colour palette and a different theme)\n\n\n\n\n\n\nggplot(gender_count, aes(x = Gender, y = n, fill = Gender)) +\n  geom_col() +\n  # replaced vidiris with the brewer palette\n  scale_fill_brewer(\n    palette = \"Set1\", # try \"Set2\" or \"Dark2\" for some variety\n    guide = \"none\") + # legend removed\n  # labels of the categories changed\n  scale_x_discrete(labels = c(\"Male\", \"Female\", \"Non-Binary\")) + \n  scale_y_continuous(\n    # change y axis label\n    name = \"Count\",\n    # starts bars on x axis without any gaps but leaves some space at the top (this time 10%)\n    expand = expansion(mult = c(0, 0.1)) \n  ) +\n  # different theme\n  theme_light()"
  },
  {
    "objectID": "04-dataviz.html#activity-5-stacked-percent-stacked-and-grouped-barchart",
    "href": "04-dataviz.html#activity-5-stacked-percent-stacked-and-grouped-barchart",
    "title": "4  Data viz I",
    "section": "\n4.6 Activity 5: Stacked, percent stacked, and grouped barchart",
    "text": "4.6 Activity 5: Stacked, percent stacked, and grouped barchart\nIf we are dealing with two categorical variables, we have the three options to display stacked barcharts - “normal” Stacked barchart (default option), a Percent stacked barchart, or a Grouped barchart.\nFor this activity, we can look at the variable Plan_prereg measuring whether students planned to pre-register their undergraduate dissertation at time point 1, and Pre_reg_group whether they actually ended up doing a pre-registration for their undergraduate dissertations.\nOne way to display that data is by creating a Stacked barchart (default option) or a Percent stacked barchart. For both options, the subgroups are displayed on top of each other. To show the two plots next to each other for better comparison, we moved the legend to the bottom of the chart.\n\n## Stacked barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar() + # no position argument added\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n## Percent stacked barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n\n\n\n\nFigure 4.3: Stacked barchart (left), and Percent stacked barchart (right)\n\n\n\nIn the stacked barchart (Figure 4.3, left plot), you are able to plot participant numbers. Here we can see that the highest student number were unsure whether they wanted to pre-register their dissertation or not, followed closely by participants who answered yes. We can also see that the number of people who did not end up with a pre-registered dissertation (blue category) is the same for students who had planned to pre-register and those who did not want to pre-register. However, because No has quite a lower number than the other two categories, we are having a tough time seeing whether the ratio is the same across all 3 groups.\nIf we wanted to show that, a Percent stacked barchart (Figure 4.3, right plot) would make a lot more sense. Now we would see that approximately 80% of the students who wanted to pre-register their dissertations, 50% of the students who were initially unsure, and only 33% of the students who had no pre-registration plans ended up with a ended up with a pre-registered dissertation. BUT! We would lose the information about the raw values in the sample.\nIt’s all a trade-off and the plot you choose depends on which “story” of the data you want to tell.\n\n\n\n\n\n\nNote\n\n\n\nThe position argument position = \"stack\" is the default. Adding this argument to the code for the left plot in Figure 4.3 would produce the same plot a leaving the argument out.\n\n\nThe other option is a Grouped barchart which displays the bars next to each other. We would achieve that by changing the position argument to “dogde”. You can see the default version of the plot in the Figure 4.4 on the left, and one with more layers on the right.\nInstead of using an existing colour palette, we changed the colours manually using hex codes. These are some of the colours Gaby used in her PhD thesis, but you can either …\n\ncreate your own colour hex codes; check out this website OR\nuse a pre-defined colour name like “green” or “purple instead. See a full list here\n\n\nFeel free to explore.\nSince the legend title for the second plot is a bit long, we displayed the legend content across 2 rows by adding the layer guides(fill = guide_legend(nrow = 2)) at the end.\n\n## Default grouped barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"dodge\") + # add position argument here\n  theme(legend.position = \"bottom\") # move legend to the bottom\n\n## Prettier grouped barchart\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Pre_reg_group)) +\n  geom_bar(position = \"dodge\") + # add position argument here\n  # changing labels for x, y, and fill category - alternative method\n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registered dissertation\") +\n  # manual colour change for values\n  scale_fill_manual(values = c('#648FFF', '#DC267F'),\n                    labels = c(\"Yes\", \"No\")) +\n  scale_y_continuous(\n    # remove the space below the bars, but keep a tiny bit (5%) above\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  # pick a theme\n  theme_classic() + \n  # need to move this following line to the end otherwise the `theme_*` overrides it\n  theme(legend.position = \"bottom\") + \n  # display across 2 rows\n  guides(fill = guide_legend(nrow = 2))\n\n\n\n\n\nFigure 4.4: Default grouped barchart (left) and one with a few more layers added (right)\n\n\n\n\n\n\n\n\n\nSpecial case: Categorical variables with missing values\n\n\n\n\n\nIf we had chosen a different categorical variable containing missing values, such as Closely_follow then our plots would have included those missing values by default. The code for changing the colour of the missing values columns would need to be specified slightly differently using an na.value = argument within the scale_fill function. Here is an example of a grouped barchart.\n\n# default grouped barchart with missing values\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\n## Prettier grouped barchart with missing values\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registration followed\") +\n  # manual colour change for values of the factor and the NA responses\n  scale_fill_manual(values = c('#648FFF', '#DC267F'), na.value = '#FFB000') +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_classic() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\n\n\n\n\nFigure 4.5: Default grouped barchart (left) and one with a few more layers added (right) for a variable with missing values\n\n\n\nIf you did not want those missing values to appear in the plot, you would have to do some data wrangling first to remove them. The function for that is drop_na(). Here we used drop_na() on Closely_follow only.\n\n# remove NA\nprereg_plan_follow &lt;- data_prp_viz %&gt;% \n  select(Code, Plan_prereg, Closely_follow) %&gt;% \n  drop_na(Closely_follow)\n\n\n\n\n\n\n\ncheck NAs have been removed\n\n\n\n\n\n\n# check NA have been removed\nprereg_plan_follow %&gt;% \n  distinct(Plan_prereg, Closely_follow) %&gt;% \n  arrange(Plan_prereg, Closely_follow)\n\n\n\n\nPlan_prereg\nClosely_follow\n\n\n\nYes\nFollowed it somewhat\n\n\nYes\nFollowed it exactly\n\n\nUnsure\nFollowed it somewhat\n\n\nUnsure\nFollowed it exactly\n\n\nNo\nFollowed it somewhat\n\n\nNo\nFollowed it exactly\n\n\n\n\n\n\n\n\n\nBut keep in mind that it could misrepresent the data, e.g., giving a wrong impression about proportions. As a comparison…\n\n# with NA\nggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") + # move legend to the bottom\n  guides(fill = guide_legend(nrow = 2)) # display across 2 rows\n\n# without NA\nggplot(prereg_plan_follow, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"fill\") + # add position argument here\n  theme(legend.position = \"bottom\") + # move legend to the bottom\n  guides(fill = guide_legend(nrow = 2)) # display across 2 rows\n\n\n\n\n\nFigure 4.6: Percent stacked barchart with (left) and without missing values (right)"
  },
  {
    "objectID": "04-dataviz.html#activity-6-save-your-plots",
    "href": "04-dataviz.html#activity-6-save-your-plots",
    "title": "4  Data viz I",
    "section": "\n4.7 Activity 6: Save your plots",
    "text": "4.7 Activity 6: Save your plots\nYou can save your figures with the function ggsave(). It will save to your project folder.\nThere are two ways you can use ggsave(). If you don’t tell ggsave() which plot you want to save, by default it will save the last plot you created. Our last plot was the one without NA from the special case scenario (Figure 4.6). However, if you did not follow along with the special case scenario, your last plot will be grouped bar chart on the right from Figure 4.4.\n\nggsave(filename = \"last_plot.png\")\n\n\n\n\n\n\n\nOur last plot saved\n\n\n\n\n\n\n\n\n\nThe second option is to save the plot as an object and refer to the object within ggsave(). As an example, let’s save the grouped barchart that contained missing values (Figure 4.4) as an object called grouped_bar.\n\ngrouped_bar &lt;- ggplot(data_prp_viz, aes(x = Plan_prereg, fill = Closely_follow)) +\n  geom_bar(position = \"dodge\") + \n  labs(x = \"Pre-registration planned\", y = \"Count\", fill = \"Pre-registration followed\") +\n  # manual colour change for values of the factor and the NA responses\n  scale_fill_manual(values = c('#648FFF', '#DC267F'), na.value = '#FFB000') +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.05))\n  ) +\n  theme_classic() + \n  theme(legend.position = \"bottom\") + \n  guides(fill = guide_legend(nrow = 3)) # display across 3 rows\n\nThen you run the line\n\nggsave(filename = \"grouped_bar.png\", \n       plot = grouped_bar)\n\n\n\nSaving 7 x 5 in image\n\n\nFilename is the name you want your png to be called, plot refers to the object name.\n\n\n\n\n\n\nOur saved grouped_bar.png would look like this:\n\n\n\n\n\n\n\n\n\nThat is with default settings. If you like it, keep it, but if you think it looks a bit “off”, you can specify the width, the height, and the units (e.g., “cm”, “mm”, “in”, “px” are possible). You might need to play about with the dimension before it feels right.\n\nggsave(filename = \"grouped_bar2.png\", \n       plot = grouped_bar, \n       width = 16, height = 9, units = \"cm\")\n\n\n\n\n\n\n\ngrouped_bar.png with different dimensions"
  },
  {
    "objectID": "04-dataviz.html#pair-coding",
    "href": "04-dataviz.html#pair-coding",
    "title": "4  Data viz I",
    "section": "Pair-coding",
    "text": "Pair-coding\nProvide a barchart, a violin-boxplot, and a scatterplot from the loneliness data and the students have to try and recreate one of those in the lab. If they are overly fast, they can do the other 2."
  },
  {
    "objectID": "04-dataviz.html#test-your-knowledge",
    "href": "04-dataviz.html#test-your-knowledge",
    "title": "4  Data viz I",
    "section": "Test your knowledge",
    "text": "Test your knowledge\nWhich plot would you choose for"
  },
  {
    "objectID": "05-dataviz2.html#intended-learning-outcomes",
    "href": "05-dataviz2.html#intended-learning-outcomes",
    "title": "5  Data viz II",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\nchoose an appropriate plot for continuous variables\nchoose an appropriate plot when you’ve got a mix of continuous/categorical variables\ncreate a basic version of an appropriate plot\napply extra layers to change the appearance of the plot\n\nIn this chapter, we are continuing our journey of appropriate plots. Last week, we looked at which plots are appropriate for categorical variables. Today, we’ll focus on continuous variables and which plots to choose with a mix of continuous and categorical variables."
  },
  {
    "objectID": "05-dataviz2.html#individual-walkthrough",
    "href": "05-dataviz2.html#individual-walkthrough",
    "title": "5  Data viz II",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "05-dataviz2.html#activity-1-set-up-and-data-for-today",
    "href": "05-dataviz2.html#activity-1-set-up-and-data-for-today",
    "title": "5  Data viz II",
    "section": "\n5.1 Activity 1: Set-up and data for today",
    "text": "5.1 Activity 1: Set-up and data for today\n\nWe are still working with the data by Pownall et al. (2023). Open the project.\nYou could use the same .Rmd file as last week if you want to keep all plotting in one document or create a new .Rmd to separate plots for categorical and continuous variables. Up to you.\nThe aggregated data is the same as last week. It should be in your project folder but in case it got lost, download it again and place it in your project folder: data_prp_for_ch4.csv.\nIf you need a reminder about the data and variables, have a look at the codebook and/or Section 1.4."
  },
  {
    "objectID": "05-dataviz2.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "href": "05-dataviz2.html#activity-2-load-in-libraries-read-in-data-and-adjust-data-types",
    "title": "5  Data viz II",
    "section": "\n5.2 Activity 2: Load in libraries, read in data, and adjust data types",
    "text": "5.2 Activity 2: Load in libraries, read in data, and adjust data types\nWe need the package tidyverse today, and the data data_prp_ch3.csv.\n\n## packages \n???\n\n## data\ndata_prp_viz &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\ndata_prp_viz &lt;- read_csv(\"data_prp_for_ch4.csv\")\n\n\n\n\nThis is the same as last week. We need to turn our categorical variables into factors to make plotting easier.\n\ndata_prp_viz &lt;- data_prp_viz %&gt;% \n  mutate(Gender = factor(Gender,\n                         levels = c(2, 1, 3),\n                         labels = c(\"females\", \"males\", \"non-binary\")),\n         Secondyeargrade = factor(Secondyeargrade,\n                                  levels = c(1, 2, 3, 4, 5),\n                                  labels = c(\"≥ 70% (1st class grade)\", \"60-69% (2:1 grade)\", \"50-59% (2:2 grade)\", \"40-49% (3rd class)\", \"&lt; 40%\")),\n         Plan_prereg = factor(Plan_prereg,\n                              levels = c(1, 3, 2),\n                              labels = c(\"Yes\", \"Unsure\", \"No\")),\n         Closely_follow = factor(Closely_follow,\n                                 levels = c(2, 3),\n                                 labels = c(\"Followed it somewhat\", \"Followed it exactly\")),\n         Research_exp = factor(Research_exp),\n         Pre_reg_group = factor(Pre_reg_group))\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are working within the same Rmd file as last week, you can skip these initial steps but you have to run the code you had already placed at the start of last-week’s Rmd file to load tidyverse into the library, read in the data, and convert some of the variables into factors."
  },
  {
    "objectID": "05-dataviz2.html#activity-3-histogram-geom_histogram",
    "href": "05-dataviz2.html#activity-3-histogram-geom_histogram",
    "title": "5  Data viz II",
    "section": "\n5.3 Activity 3: Histogram (geom_histogram())",
    "text": "5.3 Activity 3: Histogram (geom_histogram())\nIf you wanted to show the distribution of a continuous variable, you can use a histogram. As with every plot, you need at least 3 layers to create a base version of the plot. Similar to geom_bar(), geom_histogram() only requires an x variable as it does the counting “in the background”.\nA histogram splits the data into “bins” (i.e., groupings displayed in a single bar). These values are plotted along the x-axis and shows the count of how many observations are in each bin along the y-axis. It’s basically a bar chart for continuous variables.\nLet’s have a look at the age distribution in our dataset.\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nFigure 5.1: Default histogram\n\n\n\nThe default bin number is 30 (as displayed in Figure 5.1 above). Changing the number of bins (argument bins) can help to show more or less fine tuning in the data. Bigger numbers of bins means more finetuning.\nPerhaps it’s more intuitive to modify the width of each bin instead via the argument (binwidth). So for example binwidth = 1 for category age would mean 1 year of age per “age group”; binwidth = 5 would mean 5 years of age span per “age group”, etc. The plots below show modifications for both bin and binwidth.\n\n#less finetuning\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(bins = 10) \n\n# more fineturning\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1) \n\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range (`stat_bin()`).\nRemoved 2 rows containing non-finite outside the scale range (`stat_bin()`).\n\n\n\n\nFigure 5.2: Bins vs binwidth arguments\n\n\n\nThe warning message telling us 2 row of data were removed due to containing non-finite values outside the scale range. Have a look at the age column in hp_data to see if you can decipher the warning message.\nThey were removed because \nthey fall outside of the plot range\nthey contain missing values.\nColours are getting manipulated slightly differently to the barchart. Click through each tab to see how you can alter colour, axes labels, margins and breaks, and add a different theme.\n\n\nColour\nAxes labels, margins, and breaks\nThemes\n\n\n\nWe can change the plot colours by adding a fill argument and a colour argument. The fill argument manipulates the colour of the bars, and the colour argument changes the outline of the bars. Pay attention that they are added directly to the geom_histogram arguments, not the overall aes() like we did with the boxplot.\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou could use\n\nhex codes for fill and color, like we used here, geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\"). If you want create your own colours, check out this website. OR\na pre-defined colour name geom_histogram(binwidth = 1, fill = \"purple\", colour = \"green\"). See a full list here OR\n\n\n\n\n\nHere we removed the label for the y axes Count (to show you some variety) and modified the breaks. The y axis is now displayed in increasing steps of 5 (rather than 10), and the x axis has 1-year increments rather than 5.\nNotice how the breaks = argument manipulates the labels of the break ticks but not the limit of the scale. You can manipulate the limits of the scale with the limits = argument. To exaggerate, we set the limits to 15 and 50. See how the values for 15 to 19, and 44 to 50 do not have a label. You would need to adjust that in breaks = argument.\nThe expansion() function removes the gap between x axis and bars. It is exactly the same code as we used in Chapter 4 .\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\") +\n  labs(x = \"Age (in years)\", # renaming x axis label\n       y = \"\") + # removing the y axis label\n  scale_y_continuous(\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05)),\n    # changing break points on y axis\n    breaks = seq(from = 0, to = 30, by = 5)\n    ) +\n  scale_x_continuous(\n    # changing break points on x axis\n    breaks = seq(from = 20, to = 43, by = 1),\n    # Experimenting with\n    limits = c(15, 50)\n    )\n\n\n\n\n\n\n\n\n\nLet’s experiment with the themes. For this plot we have chosen theme_bw()\n\nggplot(data_prp_viz, aes(x = Age)) +\n  geom_histogram(binwidth = 1, fill = \"#586cfd\", colour = \"#FC58BE\") +\n  labs(x = \"Age (in years)\", # renaming x axis label\n       y = \"\") + # removing the y axis label\n  scale_y_continuous(\n    # remove the space below the bars (first number), but keep a tiny bit (5%) above (second number)\n    expand = expansion(mult = c(0, 0.05)),\n    # changing break points on y axis\n    breaks = seq(from = 0, to = 30, by = 5)\n    ) +\n  scale_x_continuous(\n    # changing break points on x axis\n    breaks = seq(from = 19, to = 44, by = 1)\n    ) +\n  # pick a theme\n  theme_bw()"
  },
  {
    "objectID": "05-dataviz2.html#activity-4-scatterplot-geom_point",
    "href": "05-dataviz2.html#activity-4-scatterplot-geom_point",
    "title": "5  Data viz II",
    "section": "\n5.4 Activity 4: Scatterplot (geom_point())",
    "text": "5.4 Activity 4: Scatterplot (geom_point())\nScatterplots are appropriate when you want to plot two continuous variables. Here, we want to display the relationship between Acceptance of QRPs at Time point 1 and 2. The default scatterplot would be created with geom_point().\nWe could also add a trendline by adding geom_smooth(). The default trendline is loess. If you want a linear trendline, you would need to add method = lm into geom_smooth() function.\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean)) +\n  geom_point() +\n  geom_smooth()\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nFigure 5.3: Default Scatterplot with added trendline - loess (left) and linear (right)\n\n\n\nCustomising the colour of plot is slightly different to the other plots we’ve encountered so far. Technically, the point is not a “filled-in black area”, but an extremely wide outline of a circle. Therefore, we cannot use the usual fill argument, but have to switch to the colour argument (like we did for the outline of the histogram). See the tabs below how to change the colour for all points or if you want to change the colour according to groupings.\n\n\nColour for all points\nColour with grouping\nLegend title and labels\n\n\n\nIf we want to change the colour of all the points, we would add the colour argument to the geom_point() function. Likewise, changing the colour of the trendline would also require a colour argument. Here we went with pre-defined colour names, but HEX codes would work too\n\n# colour of all points and the trendline\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean)) +\n  geom_point(colour = 'magenta') +\n  geom_smooth(method = lm, colour = 'turquoise')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIf we wanted the points to change colour due to another grouping variable, the colour argument would go into the aes() If you don’t want to define the colours manually, use a colour palette like brewer (scale_colour_brewer()) or viridis (scale_colour_viridis_d()).\n\n## adding grouping variable Pre_reg_group and changing the colour values manually\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean, colour = Pre_reg_group)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  scale_colour_manual(values = c('mediumvioletred', 'steelblue1'))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nYou can tidy the legend title and group labels via the scale_colour_? function\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean, colour = Pre_reg_group)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  scale_colour_manual(values = c('mediumvioletred', 'steelblue1'),\n                      name = \"Pre-registered Dissertation\",\n                      labels = c(\"Yes\", \"No\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAll other layers would be exactly the same as in other plots. Try to add layers to the make the plot above prettier:\n\n\n1. relabel axes x and y\n\n2. set the x and y axis range from 1 to 7\n\n3. move the legend to a different position (either top, left, or bottom)\n\n4. add a theme\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\ncan be done in 2 different ways - labs() or scale_x_?\nwe did that for the histogram\nWe did that for the bar charts\npick a theme you like\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(data_prp_viz, aes(x = QRPs_Acceptance_Time1_mean, y = QRPs_Acceptance_Time2_mean, colour = Pre_reg_group)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  scale_colour_manual(values = c('mediumvioletred', 'steelblue1'),\n                      name = \"Pre-registered Dissertation\",\n                      labels = c(\"Yes\", \"No\")) +\n  labs (x = \"Acceptance of Questionable Research Practices (Time 1)\", \n        y = \"Acceptance of Questionable Research Practices (Time 2)\") +\n  theme_light() + # place before moving the legend position\n  theme(legend.position = \"top\") # move legend to the top\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "05-dataviz2.html#activity-5-boxplot-geom_boxplot",
    "href": "05-dataviz2.html#activity-5-boxplot-geom_boxplot",
    "title": "5  Data viz II",
    "section": "\n5.5 Activity 5: Boxplot (geom_boxplot())",
    "text": "5.5 Activity 5: Boxplot (geom_boxplot())\nA boxplot is one of the options to display a continuous variable with categorical grouping variable. Here, we want to create a boxplot to investigate whether their understanding of open science depends on whether or not students had research experience. Our default boxplot would look like this:\n\n# default boxplot\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nTada. As usual, we can make the plot pretty by adding various layers. Click on the tabs to see how\n\n\nColour\nAxes labels\nLegend and Theme\n\n\n\nWe can change the colour by adding a fill argument in the aes(). If we want to modify these colours further, we would add a scale_fill_? layer If you have specific colours in mind, you would use scale_fill_manual() or if you want to stick with pre-defined ones, like brewer scale_fill_brewer().\nBtw, this is exactly the same code we used for the barcharts.\n\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS, fill = Research_exp)) +\n  geom_boxplot() +\n  # customise colour\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\nWe need to relabel the axes labels for x and y with scale_x_discrete() and scale_y_continuous(). We can also tidy the labels for the groups and the breaks on the y-axis (in steps of 1 rather than 2) within the same functions\n\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS, fill = Research_exp)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_x_discrete(\n    # changing the label of x\n    name = \"Research Experience\",\n    # changing the group labels of the 2 groups\n    labels = c(\"Yes\", \"No\")) + \n  scale_y_continuous(\n    # changing name of the y axis\n    name = \"Confidence in Understanding Open Science (Time 1)\",\n    # changing break labels\n    breaks = c(seq(from = 1, to = 7, by = 1))\n  )\n\n\n\n\n\n\n\n\n\nThe legend is superfluous; best to take it off. As before, we can remove the legend by adding the argument guide = \"none\" into the scale_fill function.\nLet’s pick a theme we haven’t used yet: theme_dark()\n\nggplot(data_prp_viz, aes(x = Research_exp, y = Time1_Understanding_OS, fill = Research_exp)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette = \"Dark2\",\n                    # removing the legend\n                    guide = \"none\") +\n  scale_x_discrete(\n    name = \"Research Experience\",\n    labels = c(\"Yes\", \"No\")) + \n  scale_y_continuous(\n    name = \"Confidence in Understanding Open Science (Time 1)\",\n    breaks = c(seq(from = 1, to = 7, by = 1))\n  ) +\n  # pick a theme\n  theme_dark()"
  },
  {
    "objectID": "05-dataviz2.html#activity-6-violin-plot-geom_violin",
    "href": "05-dataviz2.html#activity-6-violin-plot-geom_violin",
    "title": "5  Data viz II",
    "section": "\n5.6 Activity 6: Violin plot (geom_violin())",
    "text": "5.6 Activity 6: Violin plot (geom_violin())\nAn alternative to display a continuous variable with categorical grouping variable is a violin plot. Here, we want to create a violin plot to investigate whether the perception of supervisor support depended on planning to pre-register the dissertation. Our default violin plot would look like this:\n\n# default boxplot\nggplot(data_prp_viz, aes(x = Plan_prereg, y = Mean_Supervisor_Support)) +\n  geom_violin()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nAdjusting the violin plot would be exactly the same as the boxplot. Try to add layers to the base plot above to\n\n\nchange the colours either manually or using a pre-defined colour palette\n\ntidy the axes labels and group names\n\nin case a legend appears, take it off\n\nadd a theme\n\n\n\n\n\n\n\nOne possible Solution\n\n\n\n\n\n\nggplot(data_prp_viz, aes(x = Plan_prereg, y = Mean_Supervisor_Support, fill = Plan_prereg)) +\n  geom_violin() +\n  scale_fill_manual(values = c('mediumspringgreen', 'orangered', 'slateblue'),\n                    # removing the legend\n                    guide = \"none\") +\n  scale_x_discrete(name = \"Plan to pre-register the dissertation\") + \n  scale_y_continuous(\n    name = \"Perceived Supervisory Support\",\n    breaks = c(seq(from = 1, to = 7, by = 1))\n  ) +\n  # pick a theme\n  theme_minimal()\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_ydensity()`)."
  },
  {
    "objectID": "05-dataviz2.html#activity-7-violin-boxplots",
    "href": "05-dataviz2.html#activity-7-violin-boxplots",
    "title": "5  Data viz II",
    "section": "\n5.7 Activity 7: Violin-boxplots",
    "text": "5.7 Activity 7: Violin-boxplots\nSo far, we’ve only added one geom_ to our plots. Due to the layered system, we could add multiple geoms, for example, when creating a violin-boxplot.\nRemember, the order of the layers makes a difference sometimes. We’ve seen already, with themes we added at the very end that could override the argument for a legend position earlier. Here, ggplot + violinplot + boxplot looks different to ggplot + boxplot + violinplot.\nLet’s take the example of QRPs at timepoint 2 and a grouping variable of Second-year Grade.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean)) +\n  geom_violin() +\n  geom_boxplot()\n\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean)) +\n  geom_boxplot() +\n  geom_violin()\n\n\n\n\n\nFigure 5.4: Default violin-boxplot: Order of the layer matters\n\n\n\nCustomising a few elements such as width of the boxes and the colour.\n\n\nWidth of the boxes\nColour\n\n\n\nIf we want to get any information from the boxplot, we need to use order 1. But still, the boxplot is pretty wide and covers up important information from the violin. We could adjust the width of the boxes to make information more visible. This takes a bit of trial and error which width looks appropriate.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean)) +\n  geom_violin() +\n  geom_boxplot(width = 0.2)\n\n\n\nFigure 5.5: Default violin-boxplot: adjusting width of the box\n\n\n\n\n\nAdding colour should be pretty straight forward by now. This code is no different to the one we used in the boxplot or violin plot. We need to add the fill argument within the aes(), and a scale_fill_?layer.\nHowever, we can modify this further by adding an opacity argument alpha to the violin plot geom.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean, fill = Secondyeargrade)) +\n  geom_violin(alpha = 0.4) + # alpha for opacity\n  geom_boxplot(width = 0.2) + # change width of the boxes\n  scale_fill_brewer(palette = \"RdPu\") # customise colour\n\n\n\nFigure 5.6: Violin-boxplot with a different colour palette\n\n\n\n\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\n\n\nChange the x- and y-axis labels\n\nRemove the legend\n\nadd a theme\n\n\n\n\n\n\n\nOne possible Solution\n\n\n\n\n\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean, fill = Secondyeargrade)) +\n  geom_violin(alpha = 0.4) +\n  geom_boxplot(width = 0.2) +\n  scale_fill_brewer(palette = \"RdPu\",\n                    guide = \"none\") + # removes the legend\n  # change labels of x and y\n  labs (x = \"Second-year Grade\", y = \"Acceptance of Questionable Research Practices (Time 2)\") +\n  theme_classic()"
  },
  {
    "objectID": "05-dataviz2.html#activity-8-faceting---adding-another-grouping-variable",
    "href": "05-dataviz2.html#activity-8-faceting---adding-another-grouping-variable",
    "title": "5  Data viz II",
    "section": "\n5.8 Activity 8: Faceting - adding another grouping variable",
    "text": "5.8 Activity 8: Faceting - adding another grouping variable\nFaceting is really useful when you have subsets in the data. We will use this here on the violin-boxplot from above, but you could add this to pretty much any plot. The function to split up the plots into facets is called facet_wrap().\nLet’s add another grouping variable, Pre_reg_group, so we can see separate plots for the yes and no groups.\nSince the group labels on the x-axis are quite long, we need to adjust them. Adding guide = guide_axis(n.dodge = 2) to the scale_x_discrete() function helps to display labels across multiple rows.\n\nggplot(data_prp_viz, aes(x = Secondyeargrade, y = QRPs_Acceptance_Time2_mean, fill = Secondyeargrade)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.2) +\n  scale_fill_brewer(palette = \"RdPu\",\n                    guide = \"none\") + \n  labs (x = \"Second-year Grade\", y = \"Acceptance of Questionable Research Practices (Time 2)\") +\n  theme_classic() +\n  facet_wrap(~Pre_reg_group) + # faceting to split into subplots for yes and no\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) # want display labels in 2 rows\n\n\n\nFigure 5.7: Pretty violin-boxplot split into pre-registration groups (yes and no)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe labels of Pre_reg_group are displayed as numbers 1 and 2. If this bugs you, fix the labels in the dataset. This would be less hassle than trying to fix it in the plot.\n\n\n\n\n\n\n\n\nSpecial case: Variables with subscales\n\n\n\n\n\nFor example, we want to show the relationship between SATs scores at timepoints 1 and 2, separately for all 4 subscales of the SATs questionnaires, we would need to wrangle the data first. The dataframe we need should look like this:\n\nhead(data_facet, n=5)\n\n\n\n\nCode\nSubscale\nTime1\nTime2\n\n\n\nTr10\nAffect\n4.000000\n3.500000\n\n\nTr10\nCognitiveCompetence\n5.166667\n4.166667\n\n\nTr10\nValue\n6.000000\n3.000000\n\n\nTr10\nDifficulty\n3.571429\n2.857143\n\n\nBi07\nAffect\n3.833333\n3.166667\n\n\n\n\n\n\nTry wrangling the data so that it looks like data_facet above\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nstep 1: select variables of interest\nstep 2: pivot\nstep 3: try to access information on subscales and timepoints from the variable names\nstep 4: pivot in the other direction\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndata_facet &lt;- data_prp_viz %&gt;% \n  select(Code, starts_with(\"SATS\")) %&gt;% \n  pivot_longer(cols = starts_with(\"SATS\"), names_to = \"Variable\", values_to = \"Mean_Scores\") %&gt;% \n  separate(Variable, into = c(NA, \"Subscale\", \"Timepoint\", NA), sep = \"_\") %&gt;% \n  pivot_wider(names_from = Timepoint, values_from = Mean_Scores)\n\n\n\n\n\n\n\nThen we can build a scatterplot with facets for the subscales\n\nggplot(data_facet, aes(x = Time1, y = Time2)) +\n  geom_point() +\n  facet_wrap(~Subscale)"
  },
  {
    "objectID": "05-dataviz2.html#pair-coding",
    "href": "05-dataviz2.html#pair-coding",
    "title": "5  Data viz II",
    "section": "Pair-coding",
    "text": "Pair-coding\nProvide a barchart, a violin-boxplot, and a scatterplot from the loneliness data and the students have to try and recreate one of those in the lab. If they are overly fast, they can do the other 2."
  },
  {
    "objectID": "05-dataviz2.html#test-your-knowledge-on-chapters-3-and-4",
    "href": "05-dataviz2.html#test-your-knowledge-on-chapters-3-and-4",
    "title": "5  Data viz II",
    "section": "Test your knowledge on Chapters 3 and 4",
    "text": "Test your knowledge on Chapters 3 and 4\nWhich plot would you choose for\n\n5.8.1 Activity 5: Your Turn\nPick any single or two categorical variables from the dataset and choose one of the appropriate plot choices. Start with a base plot and add other layers if you please.\nSave your plot and share it with us on Teams."
  },
  {
    "objectID": "06-chi-square-one-sample.html#intended-learning-outcomes",
    "href": "06-chi-square-one-sample.html#intended-learning-outcomes",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\ncompute a Cross-tabulation Chi-square test and report the results\ncompute a one-sample t-test and report the results\nunderstand when to use a non-parametric equivalent for the one-sample t-test, compute it, and report the results"
  },
  {
    "objectID": "06-chi-square-one-sample.html#individual-walkthrough",
    "href": "06-chi-square-one-sample.html#individual-walkthrough",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "06-chi-square-one-sample.html#overview",
    "href": "06-chi-square-one-sample.html#overview",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.1 Overview",
    "text": "6.1 Overview\nWhich test to use depends on what type of variable(s) you have and what your research question is about. You can see them in the flowchart below.\n\n\nSimplified flowchart to help select the most appropriate test (created with drawio). View larger version\n\nYou can see below in which chapter they will be discussed:\n\nCross-tabulation chi-Square test (this chapter, Section 6.5)\nOne-sample t-test (this chapter, Section 6.6)\nTwo-sample or independent t-test (between-subjects design, Chapter 7)\nPaired t-test (within-subjects design, Chapter 8)\nCorrelation (Chapter 9)\nSimple regression (Chapter 10)\nMultiple regression (Chapter 11)\nOne-way ANOVA (Chapter 12)\nFactorial ANOVA (Chapter 13)"
  },
  {
    "objectID": "06-chi-square-one-sample.html#activity-1-setup-download-the-data",
    "href": "06-chi-square-one-sample.html#activity-1-setup-download-the-data",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.2 Activity 1: Setup & download the data",
    "text": "6.2 Activity 1: Setup & download the data\n\ncreate a new project and name it something meaningful (e.g., “2A_chapter6”, or “06_chi_square_one_sample_t”). See Section 1.2 if you need some guidance.\ncreate a new Rmd file and save it to your project folder. See Section 1.3 if you get stuck.\ndelete everything after the setup code chunk (e.g., line 12 and below)\ndownload a reduced dataset here: data_ch6.zip. You’ll see one csv file with demographic information and questionnaire data as well as the codebook.\nExtract the data files from the zip folder and place them in your project folder. If you need help, see Section 1.4.\n\nCitation\n\nBallou, N., Vuorre, M., Hakman, T., Magnusson, K., & Przybylski, A. K. (2024, July 12). Perceived value of video games, but not hours played, predicts mental well-being in adult Nintendo players. https://doi.org/10.31234/osf.io/3srcw\n\nAs you can see, the study is a pre-print published on PsyArXiv Preprints. The data and supplementary materials are available on OSF: https://osf.io/6xkdg/\nAbstract\n\nStudies on video games and well-being often rely on self-report measures or data from a single game. Here, we study how 703 US adults’ time spent playing for over 140,000 hours across 150 Nintendo Switch games relates to their life satisfaction, affect, depressive symptoms, and general mental well-being. We replicate previous findings that playtime over the past two weeks does not predict well-being, and extend these findings to a wider range of timescales (one hour to one year). Results suggest that relationships, if present, dissipate within two hours of gameplay. Our non-causal findings suggest substantial confounding would be needed to shift a meaningful true effect to the observed null. Although playtime was not related to well-being, players’ assessments of the value of game time—so called gaming life fit—was. Results emphasise the importance of defining the gaming population of interest, collecting data from more than one game, and focusing on how players integrate gaming into their lives rather than the amount of time spent.\n\nChanges made to the dataset\n\nWe selected demographic variables, such as age, gender, ethnicity, employment, education level, and the Warwick-Edinburgh Mental Wellbeing Scale from the rich dataset.\nWe removed rows with missing values and categorical groupings for which observed frequencies were considered too small for the purpose of this chapter.\nWe won’t be looking into any game-related associations, but feel free to download the full dataset to explore further.\nThe original authors had quite strict inclusion criteria for their analysis. We did not, hence we ended up with more participants than the original study"
  },
  {
    "objectID": "06-chi-square-one-sample.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "06-chi-square-one-sample.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.3 Activity 2: Load in the library, read in the data, and familiarise yourself with the data",
    "text": "6.3 Activity 2: Load in the library, read in the data, and familiarise yourself with the data\nToday, we’ll need the following packages tidyverse, lsr, scales, qqplotr, car, pwr, and rcompanion as well as the data data_ballou_reduced.\n\n# load in the packages\n???\n\n# read in the data\ndata_ballou &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# load in the packages\nlibrary(tidyverse)\nlibrary(lsr)\nlibrary(scales)\nlibrary(qqplotr)\nlibrary(car)\nlibrary(pwr)\nlibrary(rcompanion)\n\n# read in the data\ndata_ballou &lt;- read_csv(\"data_ballou_reduced.csv\")"
  },
  {
    "objectID": "06-chi-square-one-sample.html#activity-3-data-wrangling",
    "href": "06-chi-square-one-sample.html#activity-3-data-wrangling",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.4 Activity 3: Data wrangling",
    "text": "6.4 Activity 3: Data wrangling\nThe categorical variables look quite tidy, but we need to convert gender and education level into factors. Our statistical test requires factors and categories might be sorted more easily when plotting.\nWe also have to calculate the overall score for the Warwick-Edinburgh Mental Wellbeing Scale. According to the official WEMWBS website, the scores of the individual items are summed up.\n\ncreate a new data object data_wemwbs to calculate the summed up scores for the wemwbs.\nconvert gender and education level into factors in the original data_ballou object. Feel free to sort them in a meaningful order.\njoin this data_ballou with data_wembs.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWe converted categorical variables into factors in Chapter 4 if you need a refresher\nCheck the QRPs questionnaire in Chapter 2 to see how we approached aggregating scores\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndata_wemwbs &lt;- data_ballou %&gt;% \n  pivot_longer(cols = wemwbs_1:wemwbs_14, names_to = \"Questions\", values_to = \"Scores\") %&gt;% \n  group_by(pid) %&gt;% \n  summarise(wemwbs_sum = sum(Scores))\n\ndata_ballou &lt;- data_ballou %&gt;% \n  mutate(gender = factor(gender,\n                         levels = c(\"Woman\", \"Man\", \"Non-binary\")),\n         eduLevel = factor(eduLevel,\n                           levels = c(\"Completed Secondary School\", \"Some University but no degree\", \"University Bachelors Degree\", \"Vocational or Similar\", \"Graduate or professional degree (MA, MS, MBA, PhD, etc)\"))) %&gt;% \n  left_join(data_wemwbs)\n\nJoining with `by = join_by(pid)`"
  },
  {
    "objectID": "06-chi-square-one-sample.html#sec-chi_square",
    "href": "06-chi-square-one-sample.html#sec-chi_square",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.5 Activity 4: Cross-tabulation Chi-square test",
    "text": "6.5 Activity 4: Cross-tabulation Chi-square test\nA Cross-Tabulation Chi-Square Test, also known as a Chi-square test of association/independence, tests how one variable is associated with the distribution of outcomes in another variable.\nWe will be performing a Chi-Square test using the categorical variables gender and eduLevel:\n\nPotential research question: “Is there an association between gender and level of education in the population?”\nNull Hypothesis (H0): “Gender and student status are independent; there is no association between gender and level of education.”\nAlternative Hypothesis (H1): “Gender and student status are not independent; there is an association between gender and level of education.”\n\n\n6.5.1 Task 1: Preparing the dataframe\nWe need to select your variables of interest. We don’t have missing values in this dataset, but if your future dataframe might contain some, use drop_na() before you turn categorical variables into factors.\n\nchi_square &lt;- data_ballou %&gt;% \n  select(pid, gender, eduLevel)\n\n\n6.5.2 Task 2: Compute descriptives\nWe need to calculate counts for each combination of the variables. This is best done in a frequency table. This also allows us to check that we don’t have missing values in the cells. The function we are using does not work with missing values.\n\nchi_square_frequency &lt;- chi_square %&gt;% \n  count(gender, eduLevel) %&gt;% \n  pivot_wider(names_from = eduLevel, values_from = n)\n\n\nchi_square_frequency\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nCompleted Secondary School\nSome University but no degree\nUniversity Bachelors Degree\nVocational or Similar\nGraduate or professional degree (MA, MS, MBA, PhD, etc)\n\n\n\nWoman\n63\n118\n169\n42\n65\n\n\nMan\n70\n125\n250\n34\n81\n\n\nNon-binary\n9\n23\n20\n4\n10\n\n\n\n\n\n\nWe should be ok here, even though the count for non-binary/vocational is quite low.\n\n6.5.3 Task 3: Check assumptions\nAssumption 1: categorical data\nThe two variables should be categorical data measured either at an ordinal or nominal level.\nWe can confirm that for our dataset. Gender is \nordinal\nnominal, and level of education is \nordinal\nnominal.\nAssumption 2: Independent observartions\nThe value of one observation in the dataset does not affect the value of any other observation.\nAnd we assume as much for our data.\nAssumption 3: Cells in the contingency table are mutually exclusive\nIndividuals can only belong to one cell in the contingency table.\nWe can confirm that by looking at the data and the contingency table.\nAssumption 4: Expected frequencies are sufficiently large\nNot an assumption that is listed consistently across various sources. When it is, it suggests that expected frequencies are larger than 5 or at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, Danielle Navarro points out that this seems to be a “somewhat conservative” criterion and should be taken as “rough guidelines” only (see https://learningstatisticswithr.com/book/chisquare.html#chisqassumptions.\nThis is information, we can either compute manually (see lecture slides) or wait till we get the output from the inferential statistics later.\n\n6.5.4 Task 4: Create an appropriate plot\nNow we can create the appropriate plot. Which plot would you choose when building one from object chi_square? A \nBarchart\nHistogram\nScatterplot\nViolin-Boxplot with geom layer \ngeom_col\ngeom_bar\ngeom_histogram\ngeom_point\ngeom_boxplot and geom_violin\nTry first before looking at the solution. Feel free to practice adding other layers to make the plot pretty.\n\n\n\n\n\n\nOne possible solution\n\n\n\n\n\n… is a grouped bar chart.\nI played about with the labels of the x-axis categories since the graduate label is super long. Google was my friend in this instance and showed me a nifty function called label_wrap() from the scales package which automatically adds line breaks after X characters. Here we chose 12; looked best. (See other options for long labels at https://www.andrewheiss.com/blog/2022/06/23/long-labels-ggplot/).\n\nggplot(chi_square, aes(x = eduLevel, fill = gender)) +\n  geom_bar(position = \"dodge\") + \n  scale_fill_viridis_d(name = \"Gender\") +\n  scale_x_discrete(name = \"Level of Education\",\n                   labels = label_wrap(12)) +\n  scale_y_continuous(name = \"Count\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n6.5.5 Task 5: Compute a chi-square test\nBefore we can do that, we need to turn our tibble into a dataframe - the associationTest() function we are using to compute the Chi-square test does not like tibbles. [you have nooooo clue how long that took to figure out - let’s say the error message was not entirely useful]\n\nchi_square_df &lt;- as.data.frame(chi_square)\n\nNow we can run the associationTest() function from the lsr package. The first argument is a formula. It starts with a ~ and then expects the 2 variables we want to associate connected with a +. The second argument is the dataframe.\n\nassociationTest(formula = ~ eduLevel + gender, data = chi_square_df)\n\nWarning in associationTest(formula = ~eduLevel + gender, data = chi_square_df):\nExpected frequencies too small: chi-squared approximation may be incorrect\n\n\n\n     Chi-square test of categorical association\n\nVariables:   eduLevel, gender \n\nHypotheses: \n   null:        variables are independent of one another\n   alternative: some contingency exists between variables\n\nObserved contingency table:\n                                                         gender\neduLevel                                                  Woman Man Non-binary\n  Completed Secondary School                                 63  70          9\n  Some University but no degree                             118 125         23\n  University Bachelors Degree                               169 250         20\n  Vocational or Similar                                      42  34          4\n  Graduate or professional degree (MA, MS, MBA, PhD, etc)    65  81         10\n\nExpected contingency table under the null hypothesis:\n                                                         gender\neduLevel                                                  Woman   Man\n  Completed Secondary School                               59.9  73.4\n  Some University but no degree                           112.2 137.5\n  University Bachelors Degree                             185.2 227.0\n  Vocational or Similar                                    33.8  41.4\n  Graduate or professional degree (MA, MS, MBA, PhD, etc)  65.8  80.7\n                                                         gender\neduLevel                                                  Non-binary\n  Completed Secondary School                                    8.65\n  Some University but no degree                                16.21\n  University Bachelors Degree                                  26.75\n  Vocational or Similar                                         4.88\n  Graduate or professional degree (MA, MS, MBA, PhD, etc)       9.51\n\nTest results: \n   X-squared statistic:  13.594 \n   degrees of freedom:  8 \n   p-value:  0.093 \n\nOther information: \n   estimated effect size (Cramer's v):  0.079 \n   warning: expected frequencies too small, results may be inaccurate\n\n\nThe output is quite informative. It gives information about:\n\nthe variables that were tested,\nthe null and alternative hypotheses,\na table with the observed frequencies (which matches what we calculated in chi_square_assumptions without the rows/columns of the missing values we removed),\nan output of the frequencies you’d expect if the null hypothesis were true,\nthe result of the hypothesis test, and\nthe effect size Cramer’s v.\n\nAnd it gives us a warning message saying that expected frequencies are too small and that the chi-squared approximation may be incorrect. This ties in with Assumption 4. Depending on your stance on Assumption 4, you may or may not want to ignore the warning.\nThe p-value tells us that the null hypothesis is not being rejected, since the p-value is larger than 0.05.\n\n6.5.6 Task 6: The write-up\nThe Chi-Square test revealed that there is no statistically significant association between Gender and Student Status, \\(\\chi^2(8) = 13.59, p = .093, V = .079\\). The strength of the association between the variables is considered small. We therefore fail to reject the null hypothesis."
  },
  {
    "objectID": "06-chi-square-one-sample.html#sec-onesample",
    "href": "06-chi-square-one-sample.html#sec-onesample",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.6 Activity 5: One-sample t-test",
    "text": "6.6 Activity 5: One-sample t-test\nThe one-sample t-test is used to determine whether a sample comes from a population with a specific mean. This population mean is not always known, but is sometimes hypothesized.\nWe will be performing a one-sample t-test using the continuous variable wemwbs_sum. The official website for the Warwick-Edinburgh Mental Wellbeing Scales states that the “WEMWBS has a mean score of 51.0 in general population samples in the UK with a standard deviation of 7 (Tennant et al., 2007)”.\n\nPotential research question: “Is the average mental well-being of gamers different from the general population’s average well-being?”\nNull Hypothesis (H0): “The summed-up WEMWBS score of gamers is not different to 51.0.”\nAlternative Hypothesis (H1): “The summed-up WEMWBS score of gamers is different from 51.0.”\n\n\n6.6.1 Task 1: Preparing the dataframe\nWe need to select your variables of interest.\n\none_sample &lt;- data_ballou %&gt;% \n  select(pid, wemwbs_sum)\n\n\n6.6.2 Task 2: Compute descriptives\nWe want to compute means and standard deviations for our variable of interest. This should be straight forward. Try it yourself and then compare your result with the solution below.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndescriptives &lt;- one_sample %&gt;% \n  summarise(mean_wemwbs = mean(wemwbs_sum),\n            sd = sd(wemwbs_sum))\n\ndescriptives\n\n\n\n\nmean_wemwbs\nsd\n\n\n45.42013\n10.88615\n\n\n\n\n\n\n\n\n\n6.6.3 Task 3: Create an appropriate plot\nThis is the one you want to include in your report, so make sure everything is clearly labelled. Which plot would you choose? Start creating one first before comparing with the solution below.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(one_sample, aes(x = \"\", y = wemwbs_sum)) +\n  geom_violin(fill = \"#FB8D61\", alpha = 0.4) + # alpha for opacity, fill for adding colour\n  geom_boxplot(fill = \"#FB8D61\", width = 0.5) + # change width of the boxes\n  theme_classic() +\n  labs(x = \"\",\n       y = \"Total WEMWBS Scores\")\n\n\n\n\n\n\n\n\n\n\n\n6.6.4 Task 4: Check assumptions\nAssumption 1: Continuous DV\nThe dependent variable needs to be measured at interval or ratio level. We can confirm that by looking at one_sample.\nAssumption 2: Data are independent\nThere is no relationship between the observations. Whilst this is an important assumption, it’s not one we can really test for. It has more to do with study design. Anyway, we assume this assumption holds for our data.\nAssumption 3: No significant outliers\nWe can check for that visually, for example in the violin-boxplot above.\nIt appears that there is one outlier on the lower tail, however, when inspecting the data in one_sample, we can see that it’s one person who got a score of 14 which is a possible value. Furthermore, when sample sizes are sufficiently large, like ours with 1083 participants, removing a single outlier makes not much sense. So we have checked this assumption, consider this outlier not significant, and therefore keep this observation in the dataset.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are taking any of the outliers out, you need to recalculated the descriptive stats.\n\n\nAssumption 4: DV should be approximately normally distributed\nWe can already check normality from the violin-boxplot above but you could plot a histogram, a density plot, or a qqplot as an alternative to assess normality visually.\nAll of these options show that the data is normally distributed. That means, we will conduct a parametric test, i.e. a one-sample t-test.\n\n\n\n\n\n\nAlternatives to visually assess normality\n\n\n\n\n\n\n\nHistogram\nDensity plot\nQ-Q plot\n\n\n\nWe’ve already covered histograms in Chapter 5.\n\nggplot(one_sample, aes(x = wemwbs_sum)) +\n  geom_histogram(binwidth = 1, fill = \"magenta\")\n\n\n\n\n\n\n\n\n\nA density plot shows a smooth distribution curve of the data. The curve represents the proportion of the data in each range rather than the frequency. This means that the height of the curve doesn’t show how many times a value appears but rather the proportion of the data that falls into that range.\n\nggplot(one_sample, aes(x = wemwbs_sum)) +\n  geom_density(fill = \"magenta\")\n\n\n\n\n\n\n\n\n\nQ-Q plot stands for Quantile-Quantile Plot and compare two distributions by matching a common set of quantiles. It basically means that it’s comparing the distribution you have in your data with a normal distribution and plots this along a 45 degree line.\nIf the dots in the Q-Q plot fall roughly along that line, you can assume normality of the data. If they stray away from the line (and worse in some sort of pattern), we might not assume normality and conduct a non-parametric test instead. For the non-parametric equivalent, see Section 6.7.\nWe can either use the package car or qqplotr to build the qqplot.\n\nThe function qqPlot() is a single line but uses BaseR coding (i.e., the $ symbol) to access the column in the data object.\n\n\n# Version 1 with the car package\nqqPlot(one_sample$wemwbs_sum)\n\n[1] 295 394\n\n\n\n\nFigure 6.1: Q-Q plot created with the car package\n\n\n\n\nIf you have gotten used to ggplot by now, and prefer avoiding BaseR, you can use the package qqplotr. The downside is that you have to add the points, the line, and the confidence envelope yourself. On the plus, it has layers like ggplot, and is more customisable (just in case you wanted to look at something more colourful in the 2 seconds it’ll take you to assess normality).\n\n\n# Version 2 with package qqplotr\nggplot(one_sample, aes(sample = wemwbs_sum)) +\n  stat_qq_band(fill = \"#FB8D61\", alpha = 0.4) +\n  stat_qq_line(colour = \"#FB8D61\") +\n  stat_qq_point()\n\n\n\nFigure 6.2: Q-Q plot created with the qqplotr package\n\n\n\n\n\n\n\n\n\nYou can also assess normality with the Shapiro-Wilk’s test. The null hypothesis is that the population is distributed normally. Therefore, if the p-value of the Shapiro-Wilk’s test smaller than .05, normality is rejected.\n\n\n\n\n\n\nImportant\n\n\n\nShapiro-Wilk is a good method for small sample sizes (e.g., smaller than 50 samples). If you have large sample sizes, Shapiro-Wilk will definitely produce a significant p-value regardless of what the distribution looks like. So don’t rely on its output when you have large sample sizes.\n\n\nThe function in R is shapiro.test() and it’s built into BaseR, meaning, we need our data object, the $ and the column we want to address.\n\nshapiro.test(one_sample$wemwbs_sum)\n\n\n    Shapiro-Wilk normality test\n\ndata:  one_sample$wemwbs_sum\nW = 0.99404, p-value = 0.0002619\n\n\nNo surprise here, the test shows p &lt; .001 because we have more than 1000 participants. Nevertheless, if you had a smaller sample and you needed to write this result up in your report, you would report it in APA style: \\(W = .99, p &lt; .001\\).\n\n\n\n\n\n\nReport-writing Tip\n\n\n\nEither choose visual or computational inspection for normality tests. NO NEED TO DO BOTH!!!\n\nState what method you used and your reasons for choosing the method (visual/computational and what plot/test you used)\nState the outcome of the test - for visual inspection just say whether normality assumption held or not (no need to include that extra plot in the results section). For computational methods, report the test result in APA style\nState the conclusions you draw from it - parametric or non-parametric test\n\n\n\n\n6.6.5 Task 5: Compute a One-sample t-test and effect size\nWe can use the t.test() function to compute a one-sample t-test. The t.test() function is part of BaseR, and yes, you guessed it, our first argument has to follow the pattern data$column. The second argument (mu) lists the population mean we are testing our sample against (here 51.0). The alternative is “two.sided” by default, so you could leave the argument out.\n\nt.test(one_sample$wemwbs_sum, mu = 51.0, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  one_sample$wemwbs_sum\nt = -16.868, df = 1082, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 51\n95 percent confidence interval:\n 44.77106 46.06920\nsample estimates:\nmean of x \n 45.42013 \n\n\nThe output is quite informative. It gives us information about:\n\nthe variable column that was tested,\nthe t value, degrees of freedom, and p,\nthe alternative hypothesis,\na 95% confidence interval,\nand the mean of the column (which matches the one we computed in the descriptive - yay)\n\nWhat it doesn’t give us is an effect size. Meh. So we have to compute one ourselves.\nWe will calculate Cohen’s d using the function cohensD() from the lsr package. Similar to the t-test we just conducted, the first argument is data$column, the second argument is mu.\n\ncohensD(one_sample$wemwbs_sum, mu = 51.0)\n\n[1] 0.5125662\n\n\n\n6.6.6 Task 6: Sensitivity power analysis\nA sensitivity power analysis allows you to determine the minimum effect size that the study could reliably detect given the number of participants you have in the sample, the alpha level at 0.05, and an assumed power of 0.8.\nThe function we need to compute this is pwr.t.test() which is part of the pwr package. There are 4 factors - the APES (alpha, power, effect size, and sample size) - if you have 3, you can calculate the 4th. As stated above, we have 3. We also need to specify the type, i.e., that we are using it for a one-sample t-test, and that alternative is “two.sided” because we have a non-directional hypothesis.\n\npwr.t.test(n = 1083, sig.level = 0.05, power = 0.8, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 1083\n              d = 0.08520677\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\nSo the smallest effect size we can detect with a sample size of 1083, an alpha level of 0.05, and power of 0.8 is 0.09. This is a smaller value than the actual effect size we calculated with our CohensD function above (i.e., 0.51) which means our analysis is sufficiently powered.\n\n6.6.7 Task 7: The write-up\nA one-sample t-test was computed to determine whether the average mental well-being of gamers as measured by the WEMWBS was different to the population well-being mean. The average WEMWBS of the gamers \\((N = 1083, M = 45.42, SD = 10.89)\\) was significantly lower than the population mean well-being score of 51.0, \\(t(1082) = 16.87, p &lt; .001, d = .51\\). The strength of the effect is considered medium and the study was sufficiently powered. We therefore reject the null hypothesis in favour of H1."
  },
  {
    "objectID": "06-chi-square-one-sample.html#sec-alternative_one_sample",
    "href": "06-chi-square-one-sample.html#sec-alternative_one_sample",
    "title": "6  Chi-square and one-sample t-test",
    "section": "\n6.7 Activity 6: Non-parametric alternative",
    "text": "6.7 Activity 6: Non-parametric alternative\nIf any of the assumptions are violated, you need to switch to the non-parametric alternative. For the one-sample t-test, this is a One-sample Wilcoxon signed-rank test. Instead of the mean, it compares the median of a sample against a single value (i.e., the population median).\nThat means we need to find the population median, and calculate some summary stats for our sample:\n\nThe population median is listed in a supporting document on the official WEMWBS website as 53.0.\nWe can easily calculate the summary statistics using the function summary.\n\n\nsummary(one_sample)\n\n     pid              wemwbs_sum   \n Length:1083        Min.   :14.00  \n Class :character   1st Qu.:38.00  \n Mode  :character   Median :46.00  \n                    Mean   :45.42  \n                    3rd Qu.:53.00  \n                    Max.   :70.00  \n\n\nThe function to compute a one-sample Wilcoxon test test is wilcox.test(). It’s part of BaseR and code-wise, it works very similar to the one-sample t-test.\n\nwilcox.test(one_sample$wemwbs_sum, mu = 53.0)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  one_sample$wemwbs_sum\nV = 87218, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 53\n\n\nAs we can see, the output gives us a V value, but we need to report a Z value in the final write-up. Unfortunately, we have to calculate Z manually. According to Andy Field (2012, p. 665), we need to use the qnorm function on the halved p-value from our Wilcoxon test above.\n\n# storing the p-value\np_wilcoxon &lt;- wilcox.test(one_sample$wemwbs_sum, mu = 53.0)$p.value\n\n# calculate the z value from half the p-value\nz = qnorm(p_wilcoxon/2)\nz\n\n[1] -19.12264\n\n\nWe also need to calculate the effect size r, which we can do via the wilcoxonOneSampleR from the rcompanion package. The default value will be 3 decimal places, but you can change that with the digits argument.\n\nwilcoxonOneSampleR(one_sample$wemwbs_sum, mu = 53.0, digits = 3)\n\n    r \n-0.59 \n\n\nNow we have all the numbers we need to write up the results:\nA One-sample Wilcoxon signed-rank test was used to compare Gamers’ mental-wellbeing scores (Mdn = 46.0) to the population median of 53.0. The test showed a significant difference, \\(Z = -19.12, p &lt; .001, r = .590\\). The strength of the effect is considered medium. We therefore reject the null hypothesis in favour of H1."
  },
  {
    "objectID": "06-chi-square-one-sample.html#pair-coding",
    "href": "06-chi-square-one-sample.html#pair-coding",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "06-chi-square-one-sample.html#test-your-knowledge-on-chapters-3-and-4",
    "href": "06-chi-square-one-sample.html#test-your-knowledge-on-chapters-3-and-4",
    "title": "6  Chi-square and one-sample t-test",
    "section": "Test your knowledge on Chapters 3 and 4",
    "text": "Test your knowledge on Chapters 3 and 4"
  },
  {
    "objectID": "07-independent.html#intended-learning-outcomes",
    "href": "07-independent.html#intended-learning-outcomes",
    "title": "7  Two-sample t-test",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\na\nb\nc"
  },
  {
    "objectID": "07-independent.html#individual-walkthrough",
    "href": "07-independent.html#individual-walkthrough",
    "title": "7  Two-sample t-test",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "07-independent.html#activity-1-setup-download-the-data",
    "href": "07-independent.html#activity-1-setup-download-the-data",
    "title": "7  Two-sample t-test",
    "section": "\n7.1 Activity 1: Setup & download the data",
    "text": "7.1 Activity 1: Setup & download the data\n\ncreate a new project and name it something meaningful (e.g., “2A_chapter7”, or “07_independent_ttest”). See Section 1.2 if you need some guidance.\ncreate a new Rmd file and save it to your project folder. See Section 1.3 if you get stuck.\ndelete everything after the setup code chunk (e.g., line 12 and below)\ndownload a reduced dataset here: data_ch7.zip. You’ll see the Codebook, a demographics file, a file containing the mean response times, and a docx file of Supplementary Materials with extra information about the Simon Task, the results, etc. We also provided the raw data file for you to see what experimental data looks like when it hasn’t been pre-processed yet.\nExtract the data files from the zip folder and place them in your project folder. If you need help, see Section 1.4.\n\nCitation\n\nZwaan, R. A., Pecher, D., Paolacci, G., Bouwmeester, S., Verkoeijen, P., Dijkstra, K., & Zeelenberg, R. (2018). Participant nonnaiveté and the reproducibility of cognitive psychology. Psychonomic Bulletin & Review, 25, 1968-1972. https://doi.org/10.3758/s13423-017-1348-y\n\nThe data and supplementary materials are available on OSF: https://osf.io/ghv6m/\nAbstract\n\nMany argue that there is a reproducibility crisis in psychology. We investigated nine well-known effects from the cognitive psychology literature—three each from the domains of perception/action, memory, and language, respectively—and found that they are highly reproducible. Not only can they be reproduced in online environments, but they also can be reproduced with nonnaïve participants with no reduction of effect size. Apparently, some cognitive tasks are so constraining that they encapsulate behavior from external influences, such as testing situation and prior recent experience with the experiment to yield highly robust effects.\n\nChanges made to the dataset\n\nWe reduced the dataset, demographic information, and Supplementary Materials to only include information about the Simon Task. The full dataset including the other 8 tasks can be found on OSF.\nNo other changes were made."
  },
  {
    "objectID": "07-independent.html#activity-2-library-and-data-for-today",
    "href": "07-independent.html#activity-2-library-and-data-for-today",
    "title": "7  Two-sample t-test",
    "section": "\n7.2 Activity 2: Library and data for today",
    "text": "7.2 Activity 2: Library and data for today\nToday, we’ll need the following packages rstatix, tidyverse, car, lsr, pwr. Make sure the rstatix package is read in before tidyverse, otherwise, it will mask the filter function in dplyr. ETC. We also need to read in the data from MeansSimonTask.csv and the demographics information from DemoSimonTask.csv.\n\n# load in the packages\n???\n\n# read in the data\nzwaan_data &lt;- ???\nzwaan_demo &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# load in the packages\nlibrary(rstatix)\nlibrary(tidyverse)\nlibrary(car)\nlibrary(lsr)\nlibrary(pwr)\n\n# read in the data\nzwaan_data &lt;- read_csv(\"MeansSimonTask.csv\")\nzwaan_demo &lt;- read_csv(\"DemoSimonTask.csv\")"
  },
  {
    "objectID": "07-independent.html#activity-3-familiarise-yourself-with-the-data",
    "href": "07-independent.html#activity-3-familiarise-yourself-with-the-data",
    "title": "7  Two-sample t-test",
    "section": "\n7.3 Activity 3: Familiarise yourself with the data",
    "text": "7.3 Activity 3: Familiarise yourself with the data\nAs usual, familiarise yourself with the data before starting on the between-subjects t-test. Also, more importantly, have a look at the Supplementary Materials in which the Simon effect is explained in more depth.\nIn general, the Simon effect refers to the observation that participants have shorter response times when the stimulus appears on the same side of the screen as the button they need to press (i.e., a congruent condition). In contrast, when the stimulus appears on the opposite side of the screen from the button they are supposed to press (i.e., an incongruent condition), their response times are longer.\nIn this experiment, all participants completed 2 sessions of trials. Half of the participants received the same stimulus set across sessions 1 and 2, whereas for the other half the stimulus set they received in session 2 differed from the one already encountered in session 1.\n\nPotential research question: “Is there a significant difference in the Simon effect between participants who received the same stimuli in both sessions compared to those who received different stimuli?”\nNull Hypothesis (H0): “There is no significant difference in the Simon effect between participants who received the same stimuli in both sessions and those who received different stimuli.”\nAlternative Hypothesis (H1): “There is a significant difference in the Simon effect between participants who received the same stimuli in both sessions and those who received different stimuli.”"
  },
  {
    "objectID": "07-independent.html#activity-4-preparing-the-dataframe",
    "href": "07-independent.html#activity-4-preparing-the-dataframe",
    "title": "7  Two-sample t-test",
    "section": "\n7.4 Activity 4: Preparing the dataframe",
    "text": "7.4 Activity 4: Preparing the dataframe\nThe data is already in a very good shape, however, we do have some wrangling to do to compute this Simon effect.\nTo calculate the Simon effect, we need\n\none mean response time (RT) value for congruent and one for incongruent trials per participant, and then\nsubtract the mean RT of congruent trials from the mean RT of incongruent trials.\n\nAnd to have all data in one place, we should join this output with the demographics.\nBasically, we want to create a tibble that has the following content. [Note that I re-arranged the columns and re-labelled some of them in a final step, so your column names and/or order might be slightly different, but content should match.]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparticipant\ngender\nage\neducation\nsimilarity\ncongruent\nincongruent\nsimon_effect\n\n\n\nT1\nFemale\n50\nHigh school\nsame\n475.0032\n508.2835\n33.28029\n\n\nT10\nMale\n45\nAssociate’s degree\nsame\n420.1515\n401.5800\n-18.57148\n\n\nT109\nMale\n33\nBachelor’s degree\nsame\n339.5343\n375.7152\n36.18085\n\n\nT11\nFemale\n71\nHigh school\nsame\n516.9722\n542.3111\n25.33889\n\n\nT111\nFemale\n34\nHigh school\nsame\n373.5778\n394.0665\n20.48874\n\n\n\n\n\n\nObviously, there are various ways of doing this, so feel free to come up with your own way. However, we will provide step-by-step instructions for one of those ways that will get you the output:\n\n\n\n\n\n\nHints\n\n\n\n\n\n\n\nStep 1: the data is currently in wide format but would be better if it were in long format (so that all RT values are in 1 column; each participant has now 4 rows)\n\nStep 2: there should be a column now that contained the previous column headings with information on session number and congruency. It would be best if that was separated into 2 columns instead.\n\nStep 3: now we can calculate the mean RT for each participant, similarity, and congruency\n\n\nStep 4: pivot the data again, this time into wide format so that congruent and incongruent values are in 2 columns\n\nStep 5: create a new column called the simon_effect that subtracts congruent from incongruent values\n\nStep 6: join this together with the demographic information\n\nStep 7: feel free to rearrange the order of columns and/or rename them to match your output with ours (not strictly necessary tbh)\n\n\n\n\n\n\n\nSolution to the steps outlined above\n\n\n\n\n\n\nsimon_effect &lt;- zwaan_data %&gt;% \n  pivot_longer(cols = session1_congruent:session2_incongruent, names_to = \"col_headings\", values_to = \"RT\") %&gt;% \n  separate(col_headings, into = c(\"Session_number\", \"congruency\"), sep = \"_\") %&gt;% \n  group_by(participant, similarity, congruency) %&gt;% \n  summarise(mean_RT = mean(RT)) %&gt;% \n  ungroup() %&gt;% \n  pivot_wider(names_from = congruency, values_from = mean_RT) %&gt;% \n  mutate(simon_effect = incongruent - congruent) %&gt;% \n  full_join(zwaan_demo, by = join_by(participant == twosubjectnumber)) %&gt;% \n  select(participant, gender = gender_response, age = age_response, education = education_response, similarity:simon_effect)"
  },
  {
    "objectID": "07-independent.html#activity-4-compute-descriptives",
    "href": "07-independent.html#activity-4-compute-descriptives",
    "title": "7  Two-sample t-test",
    "section": "\n7.5 Activity 4: Compute descriptives",
    "text": "7.5 Activity 4: Compute descriptives\nWe want to compute means and standard deviations for each group of our variable of interest, i.e., the mean RT and sd for our variable simon_effect for the same and the different group.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndescriptives &lt;- simon_effect %&gt;% \n  group_by(similarity) %&gt;% \n  summarise(mean_RT = mean(simon_effect),\n            sd_RT = sd(simon_effect))\n\ndescriptives\n\n\n\n\nsimilarity\nmean_RT\nsd_RT\n\n\n\ndifferent\n32.85726\n20.79313\n\n\nsame\n35.99415\n22.39601"
  },
  {
    "objectID": "07-independent.html#activity-5-create-an-appropriate-plot",
    "href": "07-independent.html#activity-5-create-an-appropriate-plot",
    "title": "7  Two-sample t-test",
    "section": "\n7.6 Activity 5: Create an appropriate plot",
    "text": "7.6 Activity 5: Create an appropriate plot\nWhich plot would you choose to represent the data appropriately? Create an appropriate plot, then compare with the solution below.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(simon_effect, aes(x = similarity, y = simon_effect, fill = similarity)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.4, alpha = 0.8) +\n  scale_fill_viridis_d(guide = \"none\") +\n  theme_classic() +\n  labs(x = \"Similarity\", y = \"Simon effect\")"
  },
  {
    "objectID": "07-independent.html#activity-6-check-assumptions",
    "href": "07-independent.html#activity-6-check-assumptions",
    "title": "7  Two-sample t-test",
    "section": "\n7.7 Activity 6: Check assumptions",
    "text": "7.7 Activity 6: Check assumptions\nAssumption 1: Continuous DV\nThe dependent variable needs to be measured at interval or ratio level. We can confirm that by looking at simon_effect.\nAssumption 2: Data are independent\nThere is no relationship between the observations. The scores in one condition/observation can’t influence the scores in another. We assume this assumption holds for our data.\nAssumption 3: Homoskedasticity (homogeneity of variance)\nIf the variances between the 2 groups are similar/equal, we have homoskedasticity. If the variances between the 2 groups are dissimilar/unequal, we have heteroskedasticity.\nWe can test this with a Levene’s Test for Equality of Variance. The Levene’s test is part of the package car. The first argument is a formula, and it’s structured as DV ~ IV. In our data, the DV would be our continuous simon_effect variable, and the IV is the grouping variable similarity. Separate those 2 variables with a tilde. The second argument is the data.\n\nleveneTest(simon_effect ~ similarity, data = simon_effect)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\n\n\n\n\nDf\nF value\nPr(&gt;F)\n\n\n\ngroup\n1\n0.7263221\n0.3953679\n\n\n\n158\nNA\nNA\n\n\n\n\n\n\nThe warning message tells us that the grouping variable was converted into a factor. Oops, I guess we forgot to turn the variables into a factor during data wrangling.\nFrom the above result, we see that p-value is greater than .05. That means, we do have not enough evidence to reject the null hypothesis. So the variance across the 2 groups are assumed equal.\nYou would report this in APA style: A Levene’s test of homogeneity of variances was used to compare the variances of the same and the different groups. It indicated that the variances were homogenous, \\(F(1,158) = 0.73, p = .395\\).\n\n\n\n\n\n\nImportant\n\n\n\nOne other thing to note is the t-test we are conducting is a Welch t-test by default. Welch gives similar results to a Student’s t-test when variances are equal, but is to be favoured when variances are not equal.\nSo even if Levene’s returns a significant p-value indicating groups have unequal variances, we can still use the Welch t-test.\n\n\nAssumption 4: DV should be approximately normally distributed\nHere, we need to pay attention that this means normally distributed in each group.\nWe can either use our eyeballs again on the violin-boxplot we created earlier (or use a qqplot, density plot, or histogram instead), OR compute a statistic like the Shapiro-Wilk’s test we already mentioned for the one-sample t-test. However, might still have the issue of large sample sizes (i.e. ca 80 participants in each group).\nVisual inspection would tell us that both groups look pretty normally distributed - the “same” group slightly more so than the “different” group because of the peak in the lower tail. But it still looks pretty normal for real-life data.\n\n\n\n\n\n\nTip\n\n\n\nIf you wanted to use a histogram, density plot or qqplot (the ones created with the ggplot2 and qqplotr packages), you could just add a facet_wrap() function to display the figures separate for each group.\nIf you used the Q-Q plot function from the car package, you would need to create different data objects with the filtered data for each group first before you can create the Q-Q plots for both groups separately.\n\n\nYou can still use computational methods, like the Shapiro-Wilk’s test we mentioned in the last chapter. The function does not allow for a formula, which means we would have to use different objects for the 2 different groups first. I guess this is a good way of practicing the filter function.\nTask: Create separate data object for the same and different group and then run the Shapiro-Wilk test on them. What do you conclude from the results?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## same group\nsame &lt;- simon_effect %&gt;% \n  filter(similarity == \"same\")\n\nshapiro.test(same$simon_effect)\n\n## different group\ndifferent &lt;- simon_effect %&gt;% \n  filter(similarity == \"different\")\n\nshapiro.test(different$simon_effect)\n\n\n    Shapiro-Wilk normality test\n\ndata:  same$simon_effect\nW = 0.98921, p-value = 0.7447\n\n\n    Shapiro-Wilk normality test\n\ndata:  different$simon_effect\nW = 0.96949, p-value = 0.05262\n\n\nShapiro-Wilk’s test also showed the data for both groups, “same” and “different”, are normally distributed as all p-values are above .05. Again, if you used this method in your report, you would have to write up the results in APA style (see one-sample t-test).\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you have read the Delacre et al. (2017) paper (https://rips-irsp.com/articles/10.5334/irsp.82), you might be aware that the normality assumption is not overly important for the Welch t-test.\nSo whether you judge both groups as “normally distributed” or interpret one as deviating slightly from normality, the Welch t-test should still be ok to use for this dataset.\n\n\nAfter checking all of these assumptions, we decided that all of them held, and we will compute a Welch two-sample t-test."
  },
  {
    "objectID": "07-independent.html#activity-7-compute-a-two-sample-t-test-and-effect-size",
    "href": "07-independent.html#activity-7-compute-a-two-sample-t-test-and-effect-size",
    "title": "7  Two-sample t-test",
    "section": "\n7.8 Activity 7: Compute a Two-sample t-test and effect size",
    "text": "7.8 Activity 7: Compute a Two-sample t-test and effect size\nThe t.test() function we used for the one-sample t-test can be used here, however, we can use it in a slightly different way. It does allow for a formula option. So instead of having to wrangle the data again and having to use $ to access columns, we can use the formula DV ~ IV. The t.test() function expects the following arguments:\n\nThe first argument in the formula with the pattern DV ~ IV\n\nThe second argument is the data\nThe third argument is specifying whether variances are equal between the groups. By default is is var.equal = FALSE which means a Welch t-test is getting conducted. If you were to set var.equal to TRUE, you would conduct a Student t-test.\nThe 4th argument alternative is “two.sided” by default, meaning we are checking the alternative hypothesis in both directions (i.e., a non-directional hypothesis)\n\n\nt.test(simon_effect ~ similarity, data = simon_effect, var.equal = FALSE, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  simon_effect by similarity\nt = -0.91809, df = 157.14, p-value = 0.36\nalternative hypothesis: true difference in means between group different and group same is not equal to 0\n95 percent confidence interval:\n -9.885574  3.611799\nsample estimates:\nmean in group different      mean in group same \n               32.85726                35.99415 \n\n\nThe output tells us:\n\nthe test that was conducted (here Welch)\nthe variables that were tested (here simon_effect by similarity),\nthe t value, degrees of freedom, and p,\nthe alternative hypothesis,\na 95% confidence interval,\nand the mean of both groups (which again match with our descriptives)\n\nThe t.test() function does not give us an effect size, so we have to compute it once again. We can use the CohensD() function from the lsr package as we did for the one-sample t-test. We can use the formula approach here as well.\n\ncohensD(simon_effect ~ similarity, data = simon_effect)\n\n[1] 0.1451628"
  },
  {
    "objectID": "07-independent.html#activity-8-sensitivity-power-analysis",
    "href": "07-independent.html#activity-8-sensitivity-power-analysis",
    "title": "7  Two-sample t-test",
    "section": "\n7.9 Activity 8: Sensitivity power analysis",
    "text": "7.9 Activity 8: Sensitivity power analysis\nNext up is the sensitivity power analysis to determine the minimum effect size that we could have reliably detected with the number of participants that took part, the alpha level at 0.05, and an assumed power of 0.8.\nThe function we need to compute this is pwr.t.test() which is part of the pwr package. The arguments in the formula are the same as for the one-sample t-test; we just need to adjust the number of participants (which is the number of observations per sample) and setting the type to “two.sample”.\n\npwr.t.test(n = 80, sig.level = 0.05, power = 0.8, type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 80\n              d = 0.445672\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nSo the smallest effect size we can detect with a sample size of 80 participants in each group, an alpha level of 0.05, and power of 0.8 is 0.45. This is a larger value than the actual effect size we calculated with the CohensD function above (i.e., 0.14) which means our analysis is underpowered to detect this extremely small effect.\nJust out of curiosity, if we were to replicate this study, and we wanted to be able to detect an effect size that small, how many participants would we need to recruit? We run the pwr.t.test() function again, but replacing n with the effect size d. Ooft; we would need ca 1500 participants in total. To be honest, 0.145 would not be a meaningful effect size.\n\npwr.t.test(d = 0.145, sig.level = 0.05, power = 0.8, type = \"two.sample\", alternative = \"two.sided\")\n\n\n     Two-sample t test power calculation \n\n              n = 747.5833\n              d = 0.145\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group"
  },
  {
    "objectID": "07-independent.html#but-my-two-groups-have-unequal-sample-sizes-and-there-is-only-one-n-in-pwr.t.test.-what-do-i-do",
    "href": "07-independent.html#but-my-two-groups-have-unequal-sample-sizes-and-there-is-only-one-n-in-pwr.t.test.-what-do-i-do",
    "title": "7  Two-sample t-test",
    "section": "\n7.10 But my two groups have unequal sample sizes, and there is only one n in pwr.t.test. What do I do?",
    "text": "7.10 But my two groups have unequal sample sizes, and there is only one n in pwr.t.test. What do I do?\nNo problem. You can use the function pwr.t2n.test() that allows you to specify two different sample sizes n1 and n2. The rest stays pretty much the same, even though there is no need to specify the type anymore.\n\npwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL, alternative = c(\"two.sided\", \"less\",\"greater\"))\n\nLet’s try it for our example. We should get the same result though.\n\npwr.t2n.test(n1 = 80, n2= 80, sig.level = 0.05, power = 0.8, alternative = \"two.sided\")\n\n\n     t test power calculation \n\n             n1 = 80\n             n2 = 80\n              d = 0.445672\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided"
  },
  {
    "objectID": "07-independent.html#activity-9-the-write-up",
    "href": "07-independent.html#activity-9-the-write-up",
    "title": "7  Two-sample t-test",
    "section": "\n7.11 Activity 9: The write-up",
    "text": "7.11 Activity 9: The write-up\nWe hypothesised that there would be a significant difference in the Simon effect between participants who received the same stimuli in both sessions \\((N = 80, M = 35.99 msec, SD = 22.40 msec)\\) and those who received different stimuli \\((N = 80, M = 32.86 msec, SD = 20.79 msec)\\). Using a two-sample Welch t-test, the effect was found to be non-significant and of a small magnitude, \\(t(157.14) = 0.92, p = .360, d = 0.15\\). The overall mean difference between groups was small \\((M_{diff} = 3.14 msec)\\). Therefore, we fail to reject the null hypothesis."
  },
  {
    "objectID": "07-independent.html#sec-alternative_two_sample",
    "href": "07-independent.html#sec-alternative_two_sample",
    "title": "7  Two-sample t-test",
    "section": "\n7.12 Activity 10: Non-parametric alternative",
    "text": "7.12 Activity 10: Non-parametric alternative\nThe Mann-Whitney U-test is the non-parametric equivalent to the independent two-sample t-test. The test can be used for any situation requiring a test to compare the median of two samples.\nAccording to the paper by Delacre et al. (2017), the Mann-Whitney U-test can cope with normality issues, but it remains sensitive to heteroscedasticity. Here, we won’t have a problem, since the variances in the two groups were equal, but perhaps be mindful in other datasets when assessing assumptions and drawing conclusions from them.\nFirst, let’s start of by computing the median for each group\n\nsimon_effect %&gt;% group_by(similarity) %&gt;% \n  summarise(n = n(), \n            median = median(simon_effect))\n\n\n\n\nsimilarity\nn\nmedian\n\n\n\ndifferent\n80\n34.44134\n\n\nsame\n80\n35.68470\n\n\n\n\n\n\nTo conduct a Mann-Whitney U-test, use the function wilcox.test(). This time, use the formula approach DV ~ IV - again, this is the same code structure we just used for the independent t-test.\n\nwilcox.test(simon_effect ~ similarity, data = simon_effect)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  simon_effect by similarity\nW = 3001, p-value = 0.4981\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe effect size for the Mann-Whitney U-test is r. To compute r, we’d need the standardised test statistic z and divide that the square-root of the number of pairs n: \\(r = \\frac{|z|}{\\sqrt n}\\). Or we could just use the wilcox_effsize() function from the rstatix package.\nThe arguments are in a slightly different order, but exactly the same as in the Wilcox test we used above.\n\nwilcox_effsize(data = simon_effect, formula = simon_effect ~ similarity)\n\n\n\n\n.y.\ngroup1\ngroup2\neffsize\nn1\nn2\nmagnitude\n\n\nsimon_effect\ndifferent\nsame\n0.0536884\n80\n80\nsmall\n\n\n\n\n\nThis is once again considered a small effect. Anyway, we do have all the numbers now to write up the results:\nA Mann-Whitney U-test was conducted to determine whether there was a significant difference in the Simon effect between participants who received the same stimuli in both sessions \\((N = 80, MdnRT = 35.68 msec)\\) and those who received different stimuli \\((N = 80, Mdn RT = 34.44 msec)\\). The results indicate that the median difference was non-significant and of a small magnitude, \\(W = 3001, p = .498, r = .054\\). Therefore, we fail to reject the null hypothesis."
  },
  {
    "objectID": "07-independent.html#pair-coding",
    "href": "07-independent.html#pair-coding",
    "title": "7  Two-sample t-test",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "07-independent.html#test-your-knowledge",
    "href": "07-independent.html#test-your-knowledge",
    "title": "7  Two-sample t-test",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "08-paired.html#intended-learning-outcomes",
    "href": "08-paired.html#intended-learning-outcomes",
    "title": "8  Paired t-test",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:\n\na\nb\nc"
  },
  {
    "objectID": "08-paired.html#individual-walkthrough",
    "href": "08-paired.html#individual-walkthrough",
    "title": "8  Paired t-test",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "08-paired.html#activity-1-setup",
    "href": "08-paired.html#activity-1-setup",
    "title": "8  Paired t-test",
    "section": "\n8.1 Activity 1: Setup",
    "text": "8.1 Activity 1: Setup\nWe will still be working with the dataset from the study by Zwaan et al. (2018) in this chapter. Have a look at Chapter 7 or the SupMats document if you need a refresher about the Simon Task data.\n\nOpen last week’s project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "08-paired.html#activity-2-library-and-data-for-today",
    "href": "08-paired.html#activity-2-library-and-data-for-today",
    "title": "8  Paired t-test",
    "section": "\n8.2 Activity 2: Library and data for today",
    "text": "8.2 Activity 2: Library and data for today\nToday, we’ll need the following packages tidyverse, ETC. Again, we also need to read in the data from MeansSimonTask.csv and the demographics information from DemoSimonTask.csv.\n\n# load in the packages\n???\n\n# read in the data\nzwaan_data &lt;- ???\nzwaan_demo &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# load in the packages\nlibrary(tidyverse)\n\n# read in the data\nzwaan_data &lt;- read_csv(\"MeansSimonTask.csv\")\nzwaan_demo &lt;- read_csv(\"DemoSimonTask.csv\")\n\n\n\n\nAs usual, familiarise yourself with the data before starting on the between-subjects t-test."
  },
  {
    "objectID": "08-paired.html#activity-3-preparing-the-dataframe",
    "href": "08-paired.html#activity-3-preparing-the-dataframe",
    "title": "8  Paired t-test",
    "section": "\n8.3 Activity 3: Preparing the dataframe",
    "text": "8.3 Activity 3: Preparing the dataframe\njoin the files together"
  },
  {
    "objectID": "08-paired.html#activity-4-compute-descriptives",
    "href": "08-paired.html#activity-4-compute-descriptives",
    "title": "8  Paired t-test",
    "section": "\n8.4 Activity 4: Compute descriptives",
    "text": "8.4 Activity 4: Compute descriptives\ncompute means and sd"
  },
  {
    "objectID": "08-paired.html#activity-5-create-an-appropriate-plot",
    "href": "08-paired.html#activity-5-create-an-appropriate-plot",
    "title": "8  Paired t-test",
    "section": "\n8.5 Activity 5: Create an appropriate plot",
    "text": "8.5 Activity 5: Create an appropriate plot\nmake a plot"
  },
  {
    "objectID": "08-paired.html#activity-6-check-assumptions",
    "href": "08-paired.html#activity-6-check-assumptions",
    "title": "8  Paired t-test",
    "section": "\n8.6 Activity 6: Check assumptions",
    "text": "8.6 Activity 6: Check assumptions\ntest assumptions and say what it means for your test\nAssumes interval/ratio data continuous measurement scale Assumes scores are independent from each other one score should not have any relationship to another (e.g. each pair of values is from a separate participant) Assumes difference scores are approximately normally distributed\nIf any of the assumptions are violated, use the non-parametric equivalent to the between-subjects t-test, see Section 8.11."
  },
  {
    "objectID": "08-paired.html#activity-7-compute-a-two-sample-t-test-and-effect-size",
    "href": "08-paired.html#activity-7-compute-a-two-sample-t-test-and-effect-size",
    "title": "8  Paired t-test",
    "section": "\n8.7 Activity 7: Compute a Two-sample t-test and effect size",
    "text": "8.7 Activity 7: Compute a Two-sample t-test and effect size\nt-test with effect size"
  },
  {
    "objectID": "08-paired.html#activity-7-compute-a-two-sample-t-test-and-effect-size-1",
    "href": "08-paired.html#activity-7-compute-a-two-sample-t-test-and-effect-size-1",
    "title": "8  Paired t-test",
    "section": "\n8.8 Activity 7: Compute a Two-sample t-test and effect size",
    "text": "8.8 Activity 7: Compute a Two-sample t-test and effect size\n\nlibrary(rstatix)\n\n\nAttaching package: 'rstatix'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe t.test() function we used for the one-sample t-test can be used here, however, it has been a bit buggy recently not taking in certain arguments (more so for the paired sample t-test we talk about in the next chapter). Therefore, we are switching over to the t_test() function from the rstatix package.\nThe output is organised slightly different to what we’ve seen in the t.test() function, and whilst it looks neater, it does provide less information at first glimpse.\nThe Welch t-test is the default option because the var.equalargument is set to FALSE. You won’t be able to see it in the table output unless you know what you are looking for. Hence we would suggest adding all the default arguments into the function to be able to trace what you are doing.\n\nThe first argument is the data\nThe second argument in the formula with the pattern DV ~ IV\n\nNext we want to specify paired = FALSE as we don’t want a paired t-test (paired = FALSE is the default)\nThen, we want to specify var.equal = FALSE for a Welch t-test (var.equal = FALSE is the default)\nThe alternative is “two.sided” by default\nYou can set the argument detailed to TRUE to get some more information (default is detailed = FALSE).\n\n\nt_test(data = simon_effect, \n       formula = simon_effect ~ similarity, # DV ~ IV\n       paired = FALSE, # for an independent t-test (default)\n       var.equal = FALSE, # for a Welch t-test (default)\n       alternative = \"two.sided\", # default - the alternative hypothesis is non-directional\n       detailed = FALSE) # set this to true for more detail (FALSE is default)\n\nSo, what can we see in the output:\n\nThe output gives you the .y. which is the DV and group1 and group2 will list the 2 levels of the IV that the test compared (in our case “different” and “same”).\nIt also lists n1 and n2, which are the sample sizes for groups 1 and 2 respectively\nstatistic is the t-value\ndf is the degrees of freedom, and\np is the p-value\n\nIf you set the detailed argument to TRUE, you get slightly more information. Let’s have a look\n\nt_test(data = simon_effect, \n       formula = simon_effect ~ similarity, # DV ~ IV\n       paired = FALSE, # for an independent t-test (default)\n       var.equal = FALSE, # for a Welch t-test (default)\n       alternative = \"two.sided\", # default - the alternative hypothesis is tested in both directions\n       detailed = TRUE) # set this to true for more detail (FALSE is default)\n\n\n\nAdditional information that is useful:\n\nestimate which is the difference score between the groups (here 3.14 msec = 35.99-32.86)\nestimate1 which is the average value from group 1 (here 32.86 ms - and it matches with the value we calculated in the descriptives for the “different” group)\nestimate2 which is the average value from group 2 (here 35.99 ms - and it matches with the value we calculated in the descriptives for the “same” group)\nlower and higher confidence intervals might also be useful, and\nthe alternative is listed as “two.sided”\n\nNow on to the information that is a bit of a let-down: The method is listed as T-test. This will always be the output whether use the formula for a one-sample t-test, independent t-test, or a paired t-test. Therefore it’s really not great. This is the reason why we tell you to put all the arguments in, even the default ones. If you don’t you may have a tough time identifying what you just did.\n\n\n\n\n\n\nTip\n\n\n\nYou can see what test you conducted by looking at the df in the table.\n\nIf the df is \\(n1 + n2 - 2\\) it’s a Student t-test and equal variance are assumed (i.e., var.equal = TRUE).\nIf the df is approximately \\(n1 + n2 -2\\) but some weird number with decimal places, you know var.equal was set to FALSE and it is actually a Welch t-test.\nIf the df is \\(n1 - 1\\) or \\(n2 - 1\\) it’s a paired t-test (even though it would be more useful to only have one n displayed in that case - but ah well)\n\n\n\nThis t_test() function does not give us an effect size either, so we have to compute it once again. We can use the CohensD() function from the lsr package as we did for the one-sample t-test. We can use the formula approach here as well.\n\ncohensD(simon_effect ~ similarity, data = simon_effect)"
  },
  {
    "objectID": "08-paired.html#activity-8-sensitivity-power-analysis",
    "href": "08-paired.html#activity-8-sensitivity-power-analysis",
    "title": "8  Paired t-test",
    "section": "\n8.9 Activity 8: Sensitivity power analysis",
    "text": "8.9 Activity 8: Sensitivity power analysis\npower calculations"
  },
  {
    "objectID": "08-paired.html#activity-9-the-write-up",
    "href": "08-paired.html#activity-9-the-write-up",
    "title": "8  Paired t-test",
    "section": "\n8.10 Activity 9: The write-up",
    "text": "8.10 Activity 9: The write-up"
  },
  {
    "objectID": "08-paired.html#sec-alternative_two_sample",
    "href": "08-paired.html#sec-alternative_two_sample",
    "title": "8  Paired t-test",
    "section": "\n8.11 Activity 10: Non-parametric alternative",
    "text": "8.11 Activity 10: Non-parametric alternative\n\nwilcox.test(x ~ group, data = xxx, paired = TRUE)\n\nhttps://tuos-bio-data-skills.github.io/intro-stats-book/non-parametric-tests.html Wilcoxon signed-rank test: This test is equivalent to a one-sample and paired-sample t-test. This test can be used to:\ncompare a sample to a single value, or test for differences between paired samples."
  },
  {
    "objectID": "08-paired.html#pair-coding",
    "href": "08-paired.html#pair-coding",
    "title": "8  Paired t-test",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "08-paired.html#test-your-knowledge",
    "href": "08-paired.html#test-your-knowledge",
    "title": "8  Paired t-test",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "09-correlation.html#intended-learning-outcomes",
    "href": "09-correlation.html#intended-learning-outcomes",
    "title": "9  Correlations",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "09-correlation.html#individual-walkthrough",
    "href": "09-correlation.html#individual-walkthrough",
    "title": "9  Correlations",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "09-correlation.html#activity-1-setup",
    "href": "09-correlation.html#activity-1-setup",
    "title": "9  Correlations",
    "section": "9.1 Activity 1: Setup",
    "text": "9.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "09-correlation.html#activity-2-download-the-data",
    "href": "09-correlation.html#activity-2-download-the-data",
    "title": "9  Correlations",
    "section": "9.2 Activity 2: Download the data",
    "text": "9.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "09-correlation.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "09-correlation.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "9  Correlations",
    "section": "9.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data",
    "text": "9.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data"
  },
  {
    "objectID": "09-correlation.html#assumption-tests",
    "href": "09-correlation.html#assumption-tests",
    "title": "9  Correlations",
    "section": "9.4 Assumption tests",
    "text": "9.4 Assumption tests\nhttps://cran.r-project.org/web/packages/ggExtra/vignettes/ggExtra.html\nSpearman’s Rank correlation: This is equivalent to Pearson’s correlation—it tests for an association between two variables. https://tuos-bio-data-skills.github.io/intro-stats-book/non-parametric-tests.html"
  },
  {
    "objectID": "09-correlation.html#pair-coding",
    "href": "09-correlation.html#pair-coding",
    "title": "9  Correlations",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "09-correlation.html#test-your-knowledge",
    "href": "09-correlation.html#test-your-knowledge",
    "title": "9  Correlations",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "10-regression.html#intended-learning-outcomes",
    "href": "10-regression.html#intended-learning-outcomes",
    "title": "10  Simple regression",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "10-regression.html#individual-walkthrough",
    "href": "10-regression.html#individual-walkthrough",
    "title": "10  Simple regression",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "10-regression.html#activity-1-setup",
    "href": "10-regression.html#activity-1-setup",
    "title": "10  Simple regression",
    "section": "\n10.1 Activity 1: Setup",
    "text": "10.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "10-regression.html#activity-2-download-the-data",
    "href": "10-regression.html#activity-2-download-the-data",
    "title": "10  Simple regression",
    "section": "\n10.2 Activity 2: Download the data",
    "text": "10.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "10-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "10-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "10  Simple regression",
    "section": "\n10.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data",
    "text": "10.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data\nleave pretty much as is but change the data to one of the STARS datasets and keep that for today and next week"
  },
  {
    "objectID": "10-regression.html#activity-1-setup-download-the-data",
    "href": "10-regression.html#activity-1-setup-download-the-data",
    "title": "10  Simple regression",
    "section": "\n10.4 Activity 1: Setup & download the data",
    "text": "10.4 Activity 1: Setup & download the data\n\ncreate a new project and name it something meaningful (e.g., “2A_chapter5”, or “05_chi_square_one_sample_t”). See Section 1.2 if you need some guidance.\ncreate a new Rmd file and save it to your project folder. See Section 1.3 if you get stuck.\ndelete everything after the setup code chunk (e.g., line 12 and below)\ndownload the data here: data_ch5.zip.\nExtract the data files from the zip folder and place them in your project folder. If you need help, see Section 1.4.\n\nCitation\n\nAlter, U., Dang, C., Kunicki, Z. J., & Counsell, A. (2024). The VSSL scale: A brief instructor tool for assessing students’ perceived value of software to learning statistics. Teaching Statistics, 46(3), 152-163. https://doi.org/10.1111/test.12374\n\nAbstract\n\nThe biggest difference in statistical training from previous decades is the increased use of software. However, little research examines how software impacts learning statistics. Assessing the value of software to statistical learning demands appropriate, valid, and reliable measures. The present study expands the arsenal of tools by reporting on the psychometric properties of the Value of Software to Statistical Learning (VSSL) scale in an undergraduate student sample. We propose a brief measure with strong psychometric support to assess students’ perceived value of software in an educational setting. We provide data from a course using SPSS, given its wide use and popularity in the social sciences. However, the VSSL is adaptable to any statistical software, and we provide instructions for customizing it to suit alternative packages. Recommendations for administering, scoring, and interpreting the VSSL are provided to aid statistics instructors and education researchers understand how software influences students’ statistical learning.\n\nThe data is available on OSF: https://osf.io/bk7vw/\nChanges made to the dataset\n\nWe turned the excel file into a csv\nWe aggregated the main scales by reverse-scoring reverse-coded items (as listed in the codebook) and averaging.\nHowever, the responses to the individual items of the questionnaires are the raw data, not the reverse-coded scores! If you want to practice your data wrangling skills, feel free to do so.\nWe have tidied up the columns RaceEthE, GradesE, and MajorE, but we’ve left Gender and Student Status for you to tidy."
  },
  {
    "objectID": "10-regression.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "10-regression.html#activity-2-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "10  Simple regression",
    "section": "\n10.5 Activity 2: Load in the library, read in the data, and familiarise yourself with the data",
    "text": "10.5 Activity 2: Load in the library, read in the data, and familiarise yourself with the data\nToday, we’ll need the following packages tidyverse, lsr ETC as well as the data Alter_2024_data.csv.\n\n???\n\ndata_alter &lt;- ???\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lsr)\ndata_alter &lt;- read_csv(\"Alter_2024_data.csv\")"
  },
  {
    "objectID": "10-regression.html#activity-3-data-wrangling",
    "href": "10-regression.html#activity-3-data-wrangling",
    "title": "10  Simple regression",
    "section": "\n10.6 Activity 3: Data Wrangling",
    "text": "10.6 Activity 3: Data Wrangling\nTo have more informative categories within the demographic data, we would recommend relabeling the remaining two columns Gender and Student status according to the information in the codebook. Add Gender_tidy and StuSta_tidy to the data_alter object.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nGender would be a case of recoding one value as another (we did that for the Understanding_OS questionnaire in Chapter 2)\nStudent Status would be slightly more intricate having multiple entries that would be recoded as the same category (we did that for the SATs questionnaire in Chapter 2)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndata_alter &lt;- data_alter %&gt;% \n  mutate(Gender_tidy = case_match(GenderE,\n                                  1 ~ \"Female\",\n                                  2 ~ \"Male\",\n                                  3 ~ \"Non-Binary\",\n                                  .default = NA),\n         StuSta_tidy = case_when(\n           StuStaE %in% c(\"1\", \"Freshman\") ~ \"Freshman\",\n           StuStaE %in% c(\"2\", \"Sophomore\") ~ \"Sophomore\",\n           StuStaE %in% c(\"3\", \"Junior\") ~ \"Junior\",\n           StuStaE %in% c(\"4\", \"senior\", \"Senior\", \"post-bac\") ~ \"Senior or Higher\",\n           .default = StuStaE))\n\n\n\n\n\n\n\n\nggplot(data_alter, aes(sample = Mean_MA)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QANX)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QINFL)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QSF)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(x = Mean_QSF)) +\n  geom_histogram()\n\n\nggplot(data_alter, aes(sample = Mean_QHIND)) +\n  stat_qq() +\n  stat_qq_line()\n\nggplot(data_alter, aes(x = Mean_QHIND)) +\n  geom_histogram(binwidth = 0.1)\n\n\nggplot(data_alter, aes(sample = Mean_QSC)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(sample = Mean_QSE)) +\n  stat_qq() +\n  stat_qq_line()\n\n\nggplot(data_alter, aes(x = Mean_QSE)) +\n  geom_histogram(binwidth = 0.1)\n\n\nggplot(data_alter, aes(sample = Mean_SPSS)) +\n  stat_qq() +\n  stat_qq_line()"
  },
  {
    "objectID": "10-regression.html#pair-coding",
    "href": "10-regression.html#pair-coding",
    "title": "10  Simple regression",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "10-regression.html#test-your-knowledge",
    "href": "10-regression.html#test-your-knowledge",
    "title": "10  Simple regression",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "11-multiple-regression.html#intended-learning-outcomes",
    "href": "11-multiple-regression.html#intended-learning-outcomes",
    "title": "11  Multiple regression",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "11-multiple-regression.html#individual-walkthrough",
    "href": "11-multiple-regression.html#individual-walkthrough",
    "title": "11  Multiple regression",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "11-multiple-regression.html#activity-1-setup",
    "href": "11-multiple-regression.html#activity-1-setup",
    "title": "11  Multiple regression",
    "section": "11.1 Activity 1: Setup",
    "text": "11.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "11-multiple-regression.html#activity-2-download-the-data",
    "href": "11-multiple-regression.html#activity-2-download-the-data",
    "title": "11  Multiple regression",
    "section": "11.2 Activity 2: Download the data",
    "text": "11.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "11-multiple-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "href": "11-multiple-regression.html#activity-3-load-in-the-library-read-in-the-data-and-familiarise-yourself-with-the-data",
    "title": "11  Multiple regression",
    "section": "11.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data",
    "text": "11.3 Activity 3: Load in the library, read in the data and familiarise yourself with the data"
  },
  {
    "objectID": "11-multiple-regression.html#pair-coding",
    "href": "11-multiple-regression.html#pair-coding",
    "title": "11  Multiple regression",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "11-multiple-regression.html#test-your-knowledge",
    "href": "11-multiple-regression.html#test-your-knowledge",
    "title": "11  Multiple regression",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "12-one-way-anova.html#intended-learning-outcomes",
    "href": "12-one-way-anova.html#intended-learning-outcomes",
    "title": "12  One-way ANOVA",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "12-one-way-anova.html#individual-walkthrough",
    "href": "12-one-way-anova.html#individual-walkthrough",
    "title": "12  One-way ANOVA",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "12-one-way-anova.html#activity-1-setup",
    "href": "12-one-way-anova.html#activity-1-setup",
    "title": "12  One-way ANOVA",
    "section": "12.1 Activity 1: Setup",
    "text": "12.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "12-one-way-anova.html#activity-2-download-the-data",
    "href": "12-one-way-anova.html#activity-2-download-the-data",
    "title": "12  One-way ANOVA",
    "section": "12.2 Activity 2: Download the data",
    "text": "12.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables\n\nKruskal-Wallis test: This is equivalent to the global significance test in a one-way ANOVA—it tests for differences between the central tendency of several samples."
  },
  {
    "objectID": "12-one-way-anova.html#pair-coding",
    "href": "12-one-way-anova.html#pair-coding",
    "title": "12  One-way ANOVA",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "12-one-way-anova.html#test-your-knowledge",
    "href": "12-one-way-anova.html#test-your-knowledge",
    "title": "12  One-way ANOVA",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "13-factorial-anova.html#intended-learning-outcomes",
    "href": "13-factorial-anova.html#intended-learning-outcomes",
    "title": "13  Factorial ANOVA",
    "section": "Intended Learning Outcomes",
    "text": "Intended Learning Outcomes\nBy the end of this chapter you should be able to:"
  },
  {
    "objectID": "13-factorial-anova.html#individual-walkthrough",
    "href": "13-factorial-anova.html#individual-walkthrough",
    "title": "13  Factorial ANOVA",
    "section": "Individual Walkthrough",
    "text": "Individual Walkthrough"
  },
  {
    "objectID": "13-factorial-anova.html#activity-1-setup",
    "href": "13-factorial-anova.html#activity-1-setup",
    "title": "13  Factorial ANOVA",
    "section": "13.1 Activity 1: Setup",
    "text": "13.1 Activity 1: Setup\n\ncreate a new project\ncreate a new Rmd file and save it to your project folder\ndelete everything after the setup code chunk"
  },
  {
    "objectID": "13-factorial-anova.html#activity-2-download-the-data",
    "href": "13-factorial-anova.html#activity-2-download-the-data",
    "title": "13  Factorial ANOVA",
    "section": "13.2 Activity 2: Download the data",
    "text": "13.2 Activity 2: Download the data\n\nDownload the data: link\nTalk about the files\ninsert citation and abstract of the data\nLook at the codebook and the variables"
  },
  {
    "objectID": "13-factorial-anova.html#pair-coding",
    "href": "13-factorial-anova.html#pair-coding",
    "title": "13  Factorial ANOVA",
    "section": "Pair-coding",
    "text": "Pair-coding"
  },
  {
    "objectID": "13-factorial-anova.html#test-your-knowledge",
    "href": "13-factorial-anova.html#test-your-knowledge",
    "title": "13  Factorial ANOVA",
    "section": "Test your knowledge",
    "text": "Test your knowledge"
  },
  {
    "objectID": "appendix-a-installing-r.html#installing-base-r",
    "href": "appendix-a-installing-r.html#installing-base-r",
    "title": "Appendix A — Installing R",
    "section": "\nA.1 Installing Base R",
    "text": "A.1 Installing Base R\nInstall base R. Choose the download link for your operating system (Linux, Mac OS X, or Windows).\nIf you have a Mac, install the latest release from the newest R-x.x.x.pkg link (or a legacy version if you have an older operating system). After you install R, you should also install XQuartz to be able to use some visualisation packages.\nIf you are installing the Windows version, choose the “base” subdirectory and click on the download link at the top of the page. After you install R, you should also install RTools; use the “recommended” version highlighted near the top of the list.\nIf you are using Linux, choose your specific operating system and follow the installation instructions."
  },
  {
    "objectID": "appendix-a-installing-r.html#installing-rstudio",
    "href": "appendix-a-installing-r.html#installing-rstudio",
    "title": "Appendix A — Installing R",
    "section": "\nA.2 Installing RStudio",
    "text": "A.2 Installing RStudio\nGo to posit.co and download the RStudio Desktop (Open Source License) version for your operating system under the list titled Installers for Supported Platforms."
  },
  {
    "objectID": "appendix-a-installing-r.html#rstudio-settings",
    "href": "appendix-a-installing-r.html#rstudio-settings",
    "title": "Appendix A — Installing R",
    "section": "\nA.3 RStudio Settings",
    "text": "A.3 RStudio Settings\nThere are a few settings you should fix immediately after updating RStudio. Go to Global Options... under the Tools menu (⌘,), and in the General tab, uncheck the box that says Restore .RData into workspace at startup. If you keep things around in your workspace, things will get messy, and unexpected things will happen. You should always start with a clear workspace. This also means that you never want to save your workspace when you exit, so set this to Never. The only thing you want to save are your scripts.\nYou may also want to change the appearance of your code. Different fonts and themes can sometimes help with visual difficulties or dyslexia.\n\n\n\n\nRStudio General and Appearance settings\n\n\n\nYou may also want to change the settings in the Code tab. For example, Lisa prefers two spaces instead of tabs for my code and likes to be able to see the whitespace characters. But these are all a matter of personal preference.\n\n\n\n\nRStudio Code settings"
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-rstudio",
    "href": "appendix-b-updating-packages.html#updating-rstudio",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.1 Updating RStudio",
    "text": "B.1 Updating RStudio\nRStudio is the easiest component to update. Typically, updates to RStudio won’t affect your code, instead they add in new features, like spell-check or upgrades to what RStudio can do. There’s usually very little downside to updating RStudio and it’s easy to do.\nClick Help &gt; Check for updates\n\n\n\n\nUpdating RStudio\n\n\n\nIf an update is available, it will prompt you to download it and you can install it as usual."
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-r",
    "href": "appendix-b-updating-packages.html#updating-r",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.2 Updating R",
    "text": "B.2 Updating R\nFinally, you may also wish to update R itself. The key thing to be aware of is that when you update R, if you just download the latest version from the website, you will lose all your packages.\n\nB.2.1 Windows\nThe easiest way to update R on Windows and not cause yourself a huge headache is to use the installr package. When you use the updateR() function, a series of dialogue boxes will appear. These should be fairly self-explanatory but there is a full step-by-step guide available for how to use installr, the important bit is to select “Yes” when it asked if you would like to copy your packages from the older version of R.\n\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Run the update function\ninstallR::updateR()\n\n\nB.2.2 Mac\nFor a Mac, you can use the updateR package. You’ll need to install this from GitHub. You will be asked to type your system password (that you use to log into your computer) in the console pane. If relevant, it will ask you if you want to restore your packages for a new major version.\n\n# install from github\ndevtools::install_github(\"AndreaCirilloAC/updateR\")\n\n# update your R version, you will need your system password\nupdateR::updateR()"
  },
  {
    "objectID": "appendix-b-updating-packages.html#updating-packages",
    "href": "appendix-b-updating-packages.html#updating-packages",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.3 Updating packages",
    "text": "B.3 Updating packages\nPackage developers will occasionally release updates to their packages. This is typically to add in new functions to the package, or to fix or amend existing functions. Be aware that some package updates may cause your previous code to stop working. This does not tend to happen with minor updates to packages, but occasionally with major updates, you can have serious issues if the developer has made fundamental changes to how the code works. For this reason, we recommend updating all your packages once at the beginning of each academic year (or semester) - don’t do it before an assessment or deadline just in case!\nTo update an individual package, the easiest way is to use the install.packages() function, as this always installs the most recent version of the package.\n\ninstall.packages(\"tidyverse\")\n\nTo update multiple packages, or indeed all packages, RStudio provides helpful tools. Click Tools &gt; Check for Package Updates. A dialogue box will appear and you can select the packages you wish to update. Be aware that if you select all packages, this may take some time and you will be unable to use R whilst the process completes.\n\n\n\n\nUpdating packages with RStudio"
  },
  {
    "objectID": "appendix-b-updating-packages.html#sec-package-install-troubleshooting",
    "href": "appendix-b-updating-packages.html#sec-package-install-troubleshooting",
    "title": "Appendix B — Updating R, RStudio, and packages",
    "section": "\nB.4 Troubleshooting",
    "text": "B.4 Troubleshooting\nOccasionally, you might have a few problem packages that seemingly refuse to update. For me, rlang and vctrs cause me no end of trouble. These aren’t packages that you will likely every explicitly load, but they’re required beneath the surface for R to do things like knit your Markdown files etc.\n\nB.4.1 Non-zero exit status\nIf you try to update a package and get an error message that says something like Warning in install.packages : installation of package ‘vctrs’ had non-zero exit status or perhaps Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :  namespace 'rlang' 0.4.9 is being loaded, but &gt;= 0.4.10 is required one solution I have found is to manually uninstall the package, restart R, and then install the package new, rather than trying to update an existing version. The installr package also has a useful function for uninstalling packages.\n\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")\n\n\nB.4.2 Cannot open file\nYou may get the following error after trying to install any packages at all:\n\nError in install packages : Cannot open file ‘C:/…..’: Permission denied\n\nThis usually indicates a permissions problem with writing to the default library (the folder that packages are kept in). Sometimes this means that you need to install R and RStudio as administrator or run it as administrator.\nOne other fix may be to change the library location using the following code (check in “C:/Program Files/R” for what version you should have instead of “R-3.5.2”):\n\n# change the library path\n.libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nIf that works and you can install packages, set this library path permanently:\n\nInstall the usethis package\nRun usethis::edit_r_profile() in the console; it will open up a blank file\nPaste into the file (your version of): .libPaths(c(\"C:/Program Files/R/R-3.5.2/library\"))\n\nSave and close the file\nRestart R for changes to take effect\n\nThe code in your .Rprofile will now run every time you start up R.\nAs always, if you’re having issues, please ask on Teams or come to office hours."
  },
  {
    "objectID": "appendix-c-exporting-server.html",
    "href": "appendix-c-exporting-server.html",
    "title": "Appendix C — Exporting files from the server",
    "section": "",
    "text": "If you are using the R server, you may need to export files to share them with other people or submit them for your assignments.\n\nFirst, make sure you have saved any changes you have made to the file. Do this by clicking “File - Save”, Ctrl + S, or clicking the save icon. If all your changes have been saved, the save icon will be greyed out. If there are new unsaved changes, you will be able to click the icon.\nSelect the file you and to download in the files pane (bottom right) by ticking the box next to it, then click “More - Export” and save the file to your computer.\nIf you do not have R installed, DO NOT try to open it on your computer. If you do, it will open in Word, Endnote or similar, and it may corrupt your code. Only open the file if you have R and R Studio installed.\nIf you want to double check that this file is definitely the right one to submit for an assignment, you can re-upload it to the server and open it again to make sure it has the answers in it."
  },
  {
    "objectID": "appendix-d-symbols.html",
    "href": "appendix-d-symbols.html",
    "title": "Appendix D — Symbols",
    "section": "",
    "text": "Symbol\npsyTeachR Term\nAlso Known As\n\n\n\n()\n(round) brackets\nparentheses\n\n\n[]\nsquare brackets\nbrackets\n\n\n{}\ncurly brackets\nsquiggly brackets\n\n\n&lt;&gt;\nchevrons\nangled brackets / guillemets\n\n\n&lt;\nless than\n\n\n\n&gt;\ngreater than\n\n\n\n&\nampersand\n“and” symbol\n\n\n#\nhash\npound / octothorpe\n\n\n/\nslash\nforward slash\n\n\n\\\nbackslash\n\n\n\n-\ndash\nhyphen / minus\n\n\n_\nunderscore\n\n\n\n*\nasterisk\nstar\n\n\n^\ncaret\npower symbol\n\n\n~\ntilde\ntwiddle / squiggle\n\n\n=\nequal sign\n\n\n\n==\ndouble equal sign\n\n\n\n.\nfull stop\nperiod / point\n\n\n!\nexclamation mark\nbang / not\n\n\n?\nquestion mark\n\n\n\n’\nsingle quote\nquote / apostrophe\n\n\n”\ndouble quote\nquote\n\n\n%&gt;%\npipe\nmagrittr pipe\n\n\n|\nvertical bar\npipe\n\n\n,\ncomma\n\n\n\n;\nsemi-colon\n\n\n\n:\ncolon\n\n\n\n@\n“at” symbol\nvarious hilarious regional terms\n\n\n…\nglossary(\"ellipsis\")\ndots\n\n\n\n\n\n\n\nImage by James Chapman/Soundimals"
  },
  {
    "objectID": "appendix-x-How-to-cite-R.html",
    "href": "appendix-x-How-to-cite-R.html",
    "title": "Appendix E — Citing R and RStudio",
    "section": "",
    "text": "How to cite R and RStudio\nYou may be some way off writing a scientific report where you have to cite and reference R, however, when the time comes it is important to do so to give the people who built it (most of them for free!) credit. You should provide separate citations for R, RStudio, and the packages you use.\nTo get the citation for the version of R you are using, simply run the citation() function which will always provide you with the most recent citation.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nTo generate the citation for any packages you are using, you can also use the citation() function with the name of the package you wish to cite.\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }\n\n\nTo generate the citation for the version of RStudio you are using, you can use the RStudio.Vesion() function:\n\nRStudio.Version()\n\nFinally, here’s an example of how that might look in the write-up of your method section:\n\nAnalysis was conducted using R (R Core Team, 2020), RStudio (Rstudio Team, 2020), and the tidyverse package (Wickham, 2017).\n\nAs noted, you may not have to do this for a while, but come back to this when you do because it’s important to give the open-source community credit for their work."
  },
  {
    "objectID": "appendix-y-license.html",
    "href": "appendix-y-license.html",
    "title": "License",
    "section": "",
    "text": "This book is licensed under Creative Commons Attribution-ShareAlike 4.0 International License (CC-BY-SA 4.0). You are free to share and adapt this book. You must give appropriate credit, provide a link to the license, and indicate if changes were made. If you adapt the material, you must distribute your contributions under the same license as the original."
  }
]